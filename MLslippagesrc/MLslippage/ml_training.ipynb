{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Code source by:                Gaël Varoquaux\n",
    "#                                        Andreas Müller\n",
    "# Modified for documentation by:         Jaques Grobler\n",
    "# Mainly Modified for private usage by:  Ioannis Agriomallos\n",
    "#                                        Ioanna Mitsioni\n",
    "# License: BSD 3 clause\n",
    "####################################################################################################################\n",
    "######==================== CURRENT CODE USAGE ======================================================================\n",
    "# -> Current code trains several ML Classifiers, to classify force input samples as stable (0) or slip (1)\n",
    "######---- Input\n",
    "# -> Input samples originate from optoforce sensors and are 3D (fx,fy,fz) and come from 3 different datasets, \n",
    "# containing several surfaces as well as slip-stable occurrences\n",
    "######---- Input transformation\n",
    "# -> Several pre-features are taken from these inputs.\n",
    "# -> Several time and frequency domain features are extracted from pre-feature windows. \n",
    "# (implemented in 'newfeatext.py') These windows have size w and are shifted by s on each sample\n",
    "# -> Then a feature selection-ranking is performed using MutualVariableInformation\n",
    "# -> Finally PCA is performed to keep the desired among the best selected features\n",
    "######---- Training of ML Classifiers\n",
    "# -> Several ML Classifiers are trained for all combinations of features-datasets\n",
    "######---- Stats report\n",
    "# -> Several stats are reported on the classification results of the trained models, \n",
    "# based on their accuracy (fscore and auc as well) on test data as well as previously unseen data\n",
    "######===================== POTENTIAL CODE EXTENSION FOR FURTHER MORE GENERIC USAGE ================================\n",
    "# -> As binary classification problem it may apply to all such problems given that:\n",
    "######----- Input\n",
    "# -> Input is loaded on datasets, which expect (n,1) input with (k,4) dimensions each,\n",
    "# where n data samples, k measurement in each sample, 3 force readings and 1 label for each measurement.\n",
    "######----- Input transformation\n",
    "# -> Pre-features must be changed or ommitted to fit your needs. \n",
    "# Here they served to transform the 3 (fx,fy,fz) force readings into one between (ft,fn,ft/fn,sf)\n",
    "# -> Features are performed on the desired pre-features. They take a (n,o,m) input matrix and return a (n,o,l),\n",
    "# where n data samples, o windows, m prefeatures, l features.\n",
    "# -> Feature selection gives a ranking on each l, so we keep b from the best\n",
    "# -> PCA is performed on the b selected features resulting in p dimensions (n,o,p) => (n*o,p)\n",
    "######----- Training of ML Classifiers\n",
    "# -> Each Classifier is trained on a subset of (n*o,p)\n",
    "######----- Stats\n",
    "# -> Change accordingly!\n",
    "####################################################################################################################\n",
    "print(__doc__)\n",
    "import time\n",
    "start_time = time.time()\n",
    "from copy import deepcopy, copy\n",
    "import math\n",
    "import scipy.io as sio\n",
    "import shutil\n",
    "import os, errno\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "from pylab import *\n",
    "from featext2 import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "# %matplotlib qt\n",
    "# inline (suitable for ipython only, shown inside browser!) or qt (suitable in general, shown in external window!)\n",
    "from matplotlib.colors import ListedColormap\n",
    "from mpl_toolkits.mplot3d import Axes3D #, axes3d\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold, ParameterGrid, KFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, normalize\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.decomposition import PCA, KernelPCA, TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, LassoCV, RandomizedLasso\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest, chi2, f_classif, mutual_info_classif, SelectFdr\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "import datetime\n",
    "import urllib\n",
    "import tarfile\n",
    "import joblib\n",
    "from joblib import Parallel, delayed, Memory\n",
    "from tempfile import mkdtemp\n",
    "import copy_reg\n",
    "import types\n",
    "import itertools\n",
    "import pywt\n",
    "from scipy import signal\n",
    "from scipy.signal import lfilter, butter\n",
    "from numpy.fft import fft, fftfreq, rfft\n",
    "\n",
    "#import multiprocessing\n",
    "def _pickle_method(m):\n",
    "    if m.im_self is None:\n",
    "        return getattr, (m.im_class, m.im_func.func_name)\n",
    "    else:\n",
    "        return getattr, (m.im_self, m.im_func.func_name)\n",
    "copy_reg.pickle(types.MethodType, _pickle_method)\n",
    "\n",
    "def ensure_dir(directory):\n",
    "    try:\n",
    "        os.makedirs(directory)\n",
    "    except OSError as e:\n",
    "        if e.errno != errno.EEXIST:\n",
    "            raise\n",
    "datapath = 'tmp/'\n",
    "featpath = datapath+'features/'\n",
    "ensure_dir(datapath)\n",
    "ensure_dir(featpath)\n",
    "            \n",
    "h = .2  # step size in the mesh\n",
    "\n",
    "names = [\"NearNb\", \"RBFSVM1\", \"MLP1\", \"RandFor\"]\n",
    "classifiers = [KNeighborsClassifier(5),\n",
    "               SVC(gamma='auto', C=1),\n",
    "               MLPClassifier(solver='lbfgs',alpha=1e-4,hidden_layer_sizes=(10,10),random_state=1,verbose=True),\n",
    "               RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)]\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, random_state=42)\n",
    "\n",
    "parameters_clf = [{'n_neighbors':[4,7,9,12]},\n",
    "                  {'kernel':['rbf'], 'C':[0.1,10,100,1000]},\n",
    "                  {'solver':['lbfgs'], 'alpha':[1e-5,1e-2], 'hidden_layer_sizes':[(10,10),(50,50)]},\n",
    "                  {'max_depth':[4,7,10,20],'n_estimators':[5,10,20],'max_features':[20,35,50]}]\n",
    "makepipe_parameters_clf = [{'classifier__'+key:p[key] for key in p} for p in parameters_clf]\n",
    "makepipe_parameters_clf += [{'feature_selection__k': (750,500,100), 'feature_selection__score_func': [mutual_info_classif]},\n",
    "                            {'decomp__n_components': (100,50)}]\n",
    "\n",
    "metric = ['accuracy','f1']\n",
    "dataset = 0 # all datasets (0), dataset 1-2 (1), dataset 3 (2), dataset4 (3)\n",
    "download = 1 # Download pre-computed (1) data or compute them anew (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################################## INITIALISATION PARAMETERS #################################################\n",
    "window, shift, keepfromshift = 1024, 20, 10\n",
    "samplesperdataset = 10000\n",
    "havelabel = 1\n",
    "returntime = 0\n",
    "featlabel = 0         # 0: all features, 1: temporal, 2: frequency, 3: FFT only\n",
    "magnFFT = 0           # 0: FFT in magnitude format, 1: FFT in real and imag format, \n",
    "featall = 0           # 0: all, 1: feat1 (phinyomark's), 2: feat2 (golz's)\n",
    "featparam = [havelabel,featlabel,magnFFT,featall,returntime]\n",
    "metr = metric[0]      # 0: accuracy, 1: f1score\n",
    "CV = 5                # cross validation checks\n",
    "datapath = 'tmp/'\n",
    "featpath = datapath+'features/'+str(window)+'_'+str(shift)+'/'\n",
    "ensure_dir(featpath)\n",
    "numfeat = 10 # number of features to show\n",
    "nfeat = 1000 # number of features to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################## Download necessary files ################################################\n",
    "# if download==1:\n",
    "#     downpath = featpath\n",
    "#     downfile = downpath+\"features.tar.gz\"\n",
    "#     downlink = \"https://www.dropbox.com/s/lx3ggezzq2wl2km/features.tar.gz?dl=1\"\n",
    "#     if not os.path.isfile(downfile):\n",
    "#         u = urllib.urlopen(downlink)\n",
    "#         data = u.read()\n",
    "#         print 'Completed downloading ',len(data)*1./(1024**3),'GB of ',downfile,'!'\n",
    "#         u.close()\n",
    "#         with open(downfile, \"wb\") as f :\n",
    "#             f.write(data)\n",
    "#     else:\n",
    "#         print 'Necessary ',downfile,'  already here!'\n",
    "#     print 'Extracting files...'\n",
    "#     directory = featpath\n",
    "#     def extract_nonexisting(archive):\n",
    "#         for name in archive.getnames():\n",
    "#             if os.path.exists(os.path.join(directory, name)):\n",
    "#                 print name, \"already exists\"\n",
    "#             else:\n",
    "#                 archive.extract(name, path=directory)\n",
    "#     archives = [name for name in os.listdir(directory) if name.endswith(\"tar.gz\")]\n",
    "#     for archive_name in archives:\n",
    "#         with tarfile.open(featpath+archive_name) as archive:\n",
    "#             extract_nonexisting(archive)\n",
    "# #     if (downfile.endswith(\"tar.gz\")):\n",
    "# #         tar = tarfile.open(downfile, \"r:gz\")\n",
    "# #         tar.extractall(path=downpath)\n",
    "# #         tar.close()\n",
    "#     print 'Completed extracting files!'\n",
    "########################################## Download necessary dataset #############################################\n",
    "datafile = datapath+'dataset.npz'\n",
    "if not os.path.isfile(datafile):\n",
    "#     downdata = \"https://www.dropbox.com/s/95znajbu6sga8iz/slipdataset3_C.mat?dl=1\"\n",
    "#     u = urllib.urlopen(downdata)\n",
    "#     data = u.read()\n",
    "#     print 'Completed downloading ',len(data)*1./(1024**2),'MB of ',datafile,'!'\n",
    "#     u.close()\n",
    "#     with open(datafile, \"wb\") as f :\n",
    "#         f.write(data)\n",
    "    print 'Necessary ',datafile,' not here! DO SOMETHING ABOUT IT!'\n",
    "else:\n",
    "    print 'Necessary ',datafile,' already here!'\n",
    "\n",
    "\n",
    "########################################## Initialize Feature names and paths #####################################\n",
    "\n",
    "featname = 'features'+'_'+str(window)+'_'+str(shift)+'_'+str(keepfromshift)+'_'+str(samplesperdataset)\n",
    "featfile = featpath+featname+'.npz'\n",
    "XYfile = featpath+featname+'_XY.npz'\n",
    "XYsplitfile = featpath+featname+'_XYsplit.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "########################################## READ THE DATASET #######################################################\n",
    "# ################ DATASET, from each of the 2 fingers for all 6 surfaces (see fd for details) #############\n",
    "inp = np.load(datafile)\n",
    "f = inp['f']\n",
    "l = inp['l']\n",
    "fd = inp['fd']\n",
    "print 1,f.shape,l.shape,fd.shape\n",
    "# membership of each sample, representing its portion in the dataset (first half finger1 and second half finger2)\n",
    "member = np.zeros(len(f))\n",
    "m1,m2 = len(f)/2, len(f)/2\n",
    "member[:m1] = np.ones(m1)*1./m1\n",
    "member[-m2:] = np.ones(m2)*1./m2\n",
    "print 2,m1,m2,sum(member[:m1]),sum(member[-m2:])\n",
    "########################################### MERGE f and l ##########################################################\n",
    "f = np.array([np.concatenate((f[i],l[i][:,np.newaxis]),axis=1) for i in range(len(f))])\n",
    "print 3,f.shape,\":\",[fi.shape for fi in f]\n",
    "########################################### SUBSAMPLING ############################################################\n",
    "step = 1 # NO SAMPLING\n",
    "f = np.array([fi[::step,:] for fi in f])\n",
    "print 4,f.shape,\":\",[fi.shape for fi in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################################################################################\n",
    "def make_pipe_clf(scaler,feature_selection,decomp,clf):\n",
    "    # first normalize, then perform feature selection, followed by PCA and finally the desired classifier.\n",
    "    pipeline = Pipeline([('scaler', scaler),\n",
    "                         ('feature_selection', feature_selection),\n",
    "                         ('decomp', decomp),         \n",
    "                         ('classifier', clf) ])\n",
    "    return pipeline\n",
    "###########################################################################################\n",
    "def make_pipe(scaler,feature_selection,decomp,order):\n",
    "    # first normalize, then perform feature selection, followed by PCA. \n",
    "    pipeline = Pipeline([('scaler', scaler),\n",
    "                         ('feature_selection', feature_selection),\n",
    "                         ('decomp', decomp) ])\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gs_fun(clf,params,cv,x,y):\n",
    "    grid_search = GridSearchCV(estimator=clf,param_grid= params, cv = cv, n_jobs=-1, verbose = 0)\n",
    "    grid_search.fit(x,y)\n",
    "    print(\"------ Grid search cv results for %0.8s ------\" %clf)\n",
    "    print(\"Best score: %0.4f\" %grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(params.keys()):\n",
    "         print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    return best_parameters, grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "########################################### PRE-FEATURES ###########################################################\n",
    "############# DEFINITION\n",
    "# featnum 0 : sf    = (fx^2+fy^2+fz^2)^0.5\n",
    "#         1 : ft    = (fx^2+fy^2)^0.5\n",
    "#         2 : fn    = |fz|\n",
    "#         3 : ft/fn = (fx^2+fy^2)^0.5/|fz|\n",
    "# input (nxm) -> keep (nx3) -> compute pre-feature and return (nx1)\n",
    "\n",
    "def sf(f):\n",
    "    return np.power(np.sum(np.power(f[:,:3],2),axis=1),0.5)\n",
    "def ft(f):\n",
    "    return np.power(np.sum(np.power(f[:,:2],2),axis=1),0.5)\n",
    "def fn(f):\n",
    "    return np.abs(f[:,2])\n",
    "def ftn(f):\n",
    "    retft = ft(f)\n",
    "    retfn = fn(f)\n",
    "    retft[retfn<=1e-2] = 0\n",
    "    return np.divide(retft,retfn+np.finfo(float).eps)\n",
    "def lab(f):\n",
    "    return np.abs(f[:,-1])\n",
    "prefeatfn = np.array([sf,ft,fn,ftn,lab]) # convert to np.array to be easily indexed by a list\n",
    "prefeatid = [0,4]   # only the prefeatures with corresponding ids will be computed\n",
    "\n",
    "############# COMPUTATION\n",
    "prefeatname = 'prefeatures'+'_'+str(window)+'_'+str(shift)+'_'+str(keepfromshift)+'_'+str(samplesperdataset)\n",
    "prefeatfile = featpath+prefeatname+'.npz'\n",
    "\n",
    "prefeat = np.array([np.array([prfn(f[i]) for prfn in prefeatfn[prefeatid]]).transpose() for i in range(len(f))])\n",
    "print prefeat.shape,\":\",[p.shape for p in prefeat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############# PLOT one force sample from dataset and the corresponding Prefeature normalized\n",
    "# i = -1 # take the laste sample set\n",
    "# print prefeat[i].shape\n",
    "# plt.subplot(2,1,1)\n",
    "# plt.plot(prefeat[i]/np.max(prefeat[i],axis=0))\n",
    "# plt.subplot(2,1,2)\n",
    "# plt.plot((f[i]-np.min(f[i],axis=0))/(np.max(f[i],axis=0)-np.min(f[i],axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# AVG Computation time of ALL features in secs ########################################\n",
    "t1 = time.time()\n",
    "tmpfeat = [feat(prefeat[k][i:i+window,:2],*featparam) for k in range(20) for i in range(100)] # avg over 20*100 times\n",
    "print 'Avg feature computation time (millisec): ', (time.time()-t1)/(100*20)*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################################# FEATURE COMPUTATION ##################################################\n",
    "def tmpfeatfilename(p,mode='all'):\n",
    "    allfeatpath = featpath+'AllFeatures/'\n",
    "    ensure_dir(allfeatpath)\n",
    "    if mode == 'all':\n",
    "        return allfeatpath+'feat_'+str(p)+'.pkl.z'\n",
    "    elif mode == 'red':\n",
    "        return allfeatpath+'feat_'+str(p)+'_red'+str(samplesperdataset)+'.pkl.z'\n",
    "if os.path.isfile(XYfile):\n",
    "    print \"XYfile found, so Features must be already computed. Continuing to XY values...\"\n",
    "else:\n",
    "    if os.path.isfile(featfile):\n",
    "        start_time = time.time()\n",
    "        features = np.load(featfile)['features']\n",
    "        labels = np.load(featfile)['labels']\n",
    "        print(\"Features FOUND PRECOMPUTED! Feature Loading DONE in: %s seconds \" % (time.time() - start_time))\n",
    "    else:\n",
    "        start_time = time.time()\n",
    "        features = []\n",
    "        labels = []\n",
    "        for ixp in range(len(prefeat)):\n",
    "            p = prefeat[ixp]\n",
    "            now = time.time()\n",
    "            tmpfn = tmpfeatfilename(ixp)\n",
    "            tmpfnred = tmpfeatfilename(ixp,'red')\n",
    "            if not os.path.isfile(tmpfnred):\n",
    "                if not os.path.isfile(tmpfn):\n",
    "                    # Computation of all features in PARALLEL by ALL cores\n",
    "                    tmp = np.array([Parallel(n_jobs=-1)([delayed(feat) (p[k:k+window],*featparam) for k in range(0,len(p)-window,shift)])])\n",
    "                    with open(tmpfn,'wb') as fo:\n",
    "                        joblib.dump(tmp,fo)\n",
    "                    print 'sample:',ixp, ', time(sec):', '{:.2f}'.format(time.time()-now), tmpfn, ' computing... ', tmp.shape\n",
    "                else:\n",
    "                    with open(tmpfn,'rb') as fo:\n",
    "                        tmp = joblib.load(fo)\n",
    "                    print 'sample:',ixp, ', time(sec):', '{:.2f}'.format(time.time()-now), tmpfn, ' already here!', tmp.shape\n",
    "                # keep less from each feature vector but keep number of samples for each dataset almost equal\n",
    "                tmpskip = int(round(tmp.shape[1]/(member[ixp]*samplesperdataset)))\n",
    "                if tmpskip == 0: \n",
    "                    tmpskip = 1\n",
    "                # Save reduced size features\n",
    "                tmp = tmp[0,::tmpskip,:,:]\n",
    "                with open(tmpfnred,'wb') as fo:\n",
    "                    joblib.dump(tmp,fo)\n",
    "                print 'sample:',ixp, ', time(sec):', '{:.2f}'.format(time.time()-now), tmpfnred, tmp.shape\n",
    "        for ixp in range(len(prefeat)):  \n",
    "            tmpfnred = tmpfeatfilename(ixp,'red')\n",
    "            with open(tmpfnred,'rb') as fo:\n",
    "                tmp = joblib.load(fo)\n",
    "            print 'sample:',ixp, ', time(sec):', '{:.2f}'.format(time.time()-now), tmpfnred, 'already here!', tmp.shape\n",
    "            features.append(tmp[:,:,:-1])\n",
    "            labels.append(tmp[:,0,-1])\n",
    "        print(\"Features NOT FOUND PRECOMPUTED! Feature Computation DONE in: %s seconds \" % (time.time() - start_time))\n",
    "        features = np.array(features)\n",
    "        labels = np.array(labels)\n",
    "        print 'features: ',features.shape,[ftmp.shape for ftmp in features]\n",
    "        print 'labels: ', labels.shape,[l.shape for l in labels]\n",
    "        np.savez(featfile,features=features,labels=labels)\n",
    "    print features.shape, labels.shape\n",
    "#     for i in range(features.shape[0]):\n",
    "#         print i, np.array(features[i]).shape, np.array(labels[i]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if os.path.isfile(featfile):\n",
    "    start_time = time.time()\n",
    "    features = np.load(featfile)['features']\n",
    "    labels = np.load(featfile)['labels']\n",
    "    print(\"Features FOUND PRECOMPUTED! Feature Loading DONE in: %s seconds \" % (time.time() - start_time))\n",
    "else:\n",
    "    print(\"Features NOT FOUND BUT SHOULD BE precomputed! DO something about it!\")\n",
    "print 'features: ', features.shape, ', labels: ', labels.shape\n",
    "# for i in range(features.shape[0]):\n",
    "#     print i, np.array(features[i]).shape, np.array(labels[i]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###### Keeping the purely stable and slip parts of label, thus omitting some samples around sign change points \n",
    "lbl_approx = []\n",
    "history = 500\n",
    "for i in range(len(prefeat)):\n",
    "    tmpd = np.abs(np.diff(prefeat[i][:,-1].astype(int),n=1,axis=0))\n",
    "    if np.sum(tmpd) > 0:\n",
    "        tmpind = np.array(range(len(tmpd)))[tmpd > 0]   # find the sign change points\n",
    "        tmpindrng = []\n",
    "        for j in range(len(tmpind)):\n",
    "            length = history                # keep/throw a portion of the signal's length around change points\n",
    "            tmprng = np.array(range(tmpind[j]-length,tmpind[j]+length))\n",
    "            tmprng = tmprng[tmprng>=0]      # make sure inside singal's x-range\n",
    "            tmprng = tmprng[tmprng<prefeat[i].shape[0]]\n",
    "            tmpindrng += tmprng.tolist()\n",
    "        tmpindrng = np.array(tmpindrng).flatten()\n",
    "        tmp_lbl = deepcopy(prefeat[i][:,-1])\n",
    "        tmp_lbl[tmpindrng] = -1\n",
    "        lbl_approx.append(tmp_lbl)\n",
    "    else:\n",
    "        lbl_approx.append(prefeat[i][:,-1])\n",
    "# for i in range(len(lbl_approx)):\n",
    "#     print i, lbl_approx[i].shape, prefeat[i].shape\n",
    "\n",
    "new_labels = deepcopy(labels)\n",
    "for ixp in range(len(lbl_approx)):\n",
    "    p = lbl_approx[ixp]\n",
    "    tmp = np.array([p[k+window] for k in range(0,len(p)-window,shift)])\n",
    "    tmpskip = int(round(tmp.shape[0]/(member[ixp]*samplesperdataset)))\n",
    "    if tmpskip == 0: \n",
    "        tmpskip = 1\n",
    "    # Sampling appropriately\n",
    "    tmp = tmp[::tmpskip]\n",
    "    if len(tmp) > len(labels[ixp]):\n",
    "        tmp = tmp[:-1]\n",
    "    new_labels[ixp] = tmp\n",
    "print 'new_labels: ', new_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## GATHERING into complete arrays ready for FITTING #######################################\n",
    "if os.path.isfile(XYfile) and os.path.isfile(XYsplitfile):\n",
    "    X = np.load(XYfile)['X']\n",
    "    Y = np.load(XYfile)['Y']\n",
    "    Yn = np.load(XYfile)['Yn']\n",
    "    Xsp = np.load(XYsplitfile)['X']\n",
    "    Ysp = np.load(XYsplitfile)['Y']\n",
    "else:\n",
    "    # gathering features X,Xsp and labels Y,Ysp,Yn into one array each\n",
    "    ind,X,Xsp,Y,Ysp,Yn = {},{},{},{},{},{}\n",
    "    ind[2] = range(features.shape[0])                                      # indeces for both fingers\n",
    "    ind[0] = range(features.shape[0])[:m1]                                 # indeces for finger1\n",
    "    ind[1] = range(features.shape[0])[-m2:]                                # indeces for finger2\n",
    "    ind = np.array([i for _,i in ind.items()])                             # convert to array\n",
    "    for k in range(len(ind)):\n",
    "        X[k] = features[ind[k]]                                            # input feature matrix\n",
    "        Y[k] = labels[ind[k]]                                              # output label vector\n",
    "        Yn[k] = new_labels[ind[k]]                                         # output new_label vector\n",
    "        print 'Before -> X[',k,']: ',X[k].shape,', Y[',k,']: ',Y[k].shape,', Yn[',k,']: ',Yn[k].shape\n",
    "        X[k] = np.concatenate(X[k],axis=0)\n",
    "        Y[k] = np.concatenate(Y[k],axis=0)\n",
    "        Yn[k] = np.concatenate(Yn[k],axis=0)\n",
    "        print 'Gathered -> X[',k,']: ',X[k].shape,', Y[',k,']: ',Y[k].shape,', Yn[',k,']: ',Yn[k].shape\n",
    "        X[k] = np.array([X[k][:,:,i] for i in range(X[k].shape[2])])\n",
    "        tmp_sampling = int(round(X[k].shape[1]*1./samplesperdataset))\n",
    "        X[k] = X[k][0,::tmp_sampling,:]\n",
    "        Y[k] = Y[k][::tmp_sampling]\n",
    "        Yn[k] = Yn[k][::tmp_sampling]\n",
    "        print 'Gathered, sampled to max ',samplesperdataset,' -> X[',k,']: ',X[k].shape,', Y[',k,']: ',Y[k].shape,', Yn[',k,']: ',Yn[k].shape\n",
    "        keepind = Yn[k]>=0\n",
    "        Xsp[k] = X[k][keepind,:]\n",
    "        Ysp[k] = Yn[k][keepind]\n",
    "        print 'Split -> Xsp[',k,']: ',Xsp[k].shape,', Ysp[',k,']: ',Ysp[k].shape\n",
    "    X = np.array([i for _,i in X.items()])\n",
    "    Xsp = np.array([i for _,i in Xsp.items()])\n",
    "    Y = np.array([i for _,i in Y.items()])\n",
    "    Ysp = np.array([i for _,i in Ysp.items()])\n",
    "    Yn = np.array([i for _,i in Yn.items()])\n",
    "    np.savez(XYfile,X=X,Y=Y,Yn=Yn)\n",
    "    np.savez(XYsplitfile, X=Xsp, Y=Ysp)\n",
    "print 'X,Y [0,1,2]: ', X[0].shape, Y[0].shape, X[1].shape, Y[1].shape, X[2].shape, Y[2].shape\n",
    "print 'Xsp,Ysp [0,1,2]: ', Xsp[0].shape, Ysp[0].shape, Xsp[1].shape, Ysp[1].shape, Xsp[2].shape, Ysp[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################## Feature Names ###########################################################\n",
    "####################################################################################################################\n",
    "##  features:                                                                                  ||      if         ##\n",
    "##  |----------> time domain      :                                                            || samples = 1024  ##\n",
    "##  |------------|---> phinyomark : 11+3{shist} -----------------------------> = 14+0.0samples ||             14  ##\n",
    "##  |------------|---> golz       : 10+samples{acrol} -----------------------> = 10+1.0samples ||           1034  ##\n",
    "##  |----------> frequency domain :                                                                               ##\n",
    "##  |------------|---> phinyomark : 3{arco}+4{mf}+2(samples/2+1){RF,IF} -----> =  9+1.0samples ||           1033  ##\n",
    "##  |------------|---> golz       : 2(samples/2+1){AF,PF} -------------------> =  2+1.0samples ||           1026  ##\n",
    "##  |------------|--------|-------alltogether--------------------------------> = 35+3.0samples || numfeat = 3107  ##\n",
    "####################################################################################################################\n",
    "## Time Domain Phinyomark feats\n",
    "featnames = ['intsgnl', 'meanabs', 'meanabsslp', 'ssi', 'var', 'rms', 'rng', 'wavl', 'zerox', 'ssc', 'wamp', \n",
    "             'shist1', 'shist2', 'shist3']                                                   # 11+3{shist}\n",
    "## Frequency Domain Phinyomark feats\n",
    "featnames += ['arco1', 'arco2', 'arco3', 'mnf', 'mdf', 'mmnf', 'mmdf']                       # 3{arco}+4{mf}\n",
    "featnames += ['reFFT{:03d}'.format(i) for i in range(window/2+1)]                            # samples/2+1{RF}\n",
    "featnames += ['imFFT{:03d}'.format(i) for i in range(window/2+1)]                            # samples/2+1{IF}\n",
    "## Time Domain Golz feats\n",
    "featnames += ['meanv', 'stdr', 'mx', 'rngx', 'rngy', 'med', 'hjorth', 'sentr', 'se', 'ssk']  # 10\n",
    "featnames += ['acrol{:04d}'.format(i) for i in range(window)]                                # samples{acrol}\n",
    "## Frequency Domain Golz feats\n",
    "featnames += ['amFFT{:03d}'.format(i) for i in range(window/2+1)]                            # samples/2+1{AF}\n",
    "featnames += ['phFFT{:03d}'.format(i) for i in range(window/2+1)]                            # samples/2+1{PF}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feat_id(feat_ind, printit=0, sample_window=window): \n",
    "    # get the feat inds wrt their source : 3rd level\n",
    "    norm_time_phin = range(0,14)\n",
    "    norm_freq_phin = range(norm_time_phin[-1] + 1, norm_time_phin[-1] + 9 + sample_window + 1)\n",
    "    norm_time_golz = range(norm_freq_phin[-1] + 1, norm_freq_phin[-1] + 10 + sample_window + 1)\n",
    "    norm_freq_golz = range(norm_time_golz[-1] + 1, norm_time_golz[-1] + 2 + sample_window + 1)\n",
    "    # get the feat inds wrt their domain : 2nd level \n",
    "    norm_time_feats = norm_time_phin + norm_time_golz\n",
    "    norm_freq_feats = norm_freq_phin + norm_freq_golz\n",
    "    # get the feat inds wrt their prefeat: 1st level \n",
    "    norm_feats = norm_time_feats + norm_freq_feats\n",
    "\n",
    "    # get the feat inds wrt their source : 3rd level\n",
    "    disp = norm_feats[-1]+1\n",
    "    ftfn_time_phin = range(disp ,disp + 14)\n",
    "    ftfn_freq_phin = range(ftfn_time_phin[-1] + 1, ftfn_time_phin[-1] + 9 + sample_window + 1)\n",
    "    ftfn_time_golz = range(ftfn_freq_phin[-1] + 1, ftfn_freq_phin[-1] + 10 + sample_window + 1)\n",
    "    ftfn_freq_golz = range(ftfn_time_golz[-1] + 1, ftfn_time_golz[-1] + 2 + sample_window + 1)\n",
    "    # get the feat inds wrt their domain : 2nd level \n",
    "    ftfn_time_feats = ftfn_time_phin + ftfn_time_golz\n",
    "    ftfn_freq_feats = ftfn_freq_phin + ftfn_freq_golz\n",
    "    # get the feat inds wrt their prefeat: 1st level \n",
    "    ftfn_feats = ftfn_time_feats + ftfn_freq_feats\n",
    "\n",
    "    # create the final \"reference dictionary\"\n",
    "    id_list = [np.zeros((len(ftfn_feats + norm_feats),1)) for i in range(3)] #3 np.arrays, id_list[0] = level 1 etc\n",
    "    id_list[0][:norm_feats[-1]+1] = 0 # 0 signifies norm / 1 signifies ft/fn\n",
    "    id_list[0][norm_feats[-1]+1:] = 1\n",
    "\n",
    "    id_list[1][:norm_time_phin[-1]+1] = 0 #0 signifies time / 1 signifies freq\n",
    "    id_list[1][norm_time_phin[-1]+1:norm_freq_phin[-1]+1] = 1\n",
    "    id_list[1][norm_freq_phin[-1]+1:norm_time_golz[-1]+1] = 0\n",
    "    id_list[1][norm_time_golz[-1]+1:norm_freq_golz[-1]+1] = 1\n",
    "    id_list[1][norm_freq_golz[-1]+1:ftfn_time_phin[-1]+1] = 0\n",
    "    id_list[1][ftfn_time_phin[-1]+1:ftfn_freq_phin[-1]+1] = 1\n",
    "    id_list[1][ftfn_freq_phin[-1]+1:ftfn_time_golz[-1]+1] = 0\n",
    "    id_list[1][ftfn_time_golz[-1]+1:] = 1\n",
    "\n",
    "    id_list[2][:norm_freq_phin[-1]+1] = 0 #0 signifies phinyomark / 1 signifies golz\n",
    "    id_list[2][norm_freq_phin[-1]+1:norm_freq_golz[-1]+1] = 1\n",
    "    id_list[2][norm_freq_golz[-1]+1:ftfn_freq_phin[-1]+1] = 0\n",
    "    id_list[2][ftfn_freq_phin[-1]+1:] = 1 \n",
    "    \n",
    "    full_path_id = [np.zeros((len(feat_ind),5)) for i in range(len(feat_ind))]\n",
    "   \n",
    "    for ind, val in enumerate(feat_ind):\n",
    "        full_path_id[ind] = [val, id_list[2][val], id_list[1][val], id_list[0][val]]\n",
    "        if (printit==1):\n",
    "            if(full_path_id[ind][1]==0):\n",
    "                lvl3 = 'Phin'\n",
    "            else:\n",
    "                lvl3 = 'Golz'\n",
    "            if(full_path_id[ind][2]==0):\n",
    "                lvl2 = 'Time'\n",
    "            else:\n",
    "                lvl2 = 'Freq'\n",
    "            if(full_path_id[ind][3]==0):\n",
    "                lvl1 = 'Norm'\n",
    "            else:\n",
    "                lvl1 = 'Ft/Fn'\n",
    "            print(feat_ind[ind],featnames[val%(norm_feats[-1]+1)],lvl3,lvl2,lvl1)\n",
    "    \n",
    "    return(full_path_id,norm_time_feats,norm_freq_feats)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feat_occ(feat_masks):\n",
    "    #get the number of occurences for each feature after SelectKbest\n",
    "    feat_occ = Counter(feat_masks)\n",
    "    return feat_occ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def surface_split(data_X, data_Y, n=6, k=2):\n",
    "    # n different surfaces, with the convention that data_X contains k*n almost equally sized data\n",
    "    # where the n first are acquired from finger1 ... and the n last from fingerk. \n",
    "    # step of n: 0 upto (k-1)*n, 1 upto (k-1)*n+1, 2 upto (k-1)*n+2, ... correspond to the same surface (finger1 upto fingerk)\n",
    "    # assuming k=2, namely 2 fingers case, unless stated differently\n",
    "    keep = data_X.shape[0]-np.mod(data_X.shape[0],k*n)\n",
    "    surfaces_pre = np.array(np.split(data_X[:keep,:],k*n))\n",
    "    surf_labels_pre = np.array(np.split(data_Y[:keep],k*n))\n",
    "    surfaces, surf_labels = {},{}\n",
    "    for i in range(n):\n",
    "        inds = range(i,k*n,n)\n",
    "        surfaces[inds[0]] = np.concatenate((surfaces_pre[inds[0]], surfaces_pre[inds[1]]), axis = 0)\n",
    "        surf_labels[inds[0]] = np.concatenate((surf_labels_pre[inds[0]], surf_labels_pre[inds[1]]), axis = 0)\n",
    "    surfaces = np.array([i for _,i in surfaces.items()])\n",
    "    surf_labels = np.array([i for _,i in surf_labels.items()])\n",
    "    return surfaces, surf_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feat_subsets(data,fs_ind,ofs=len(featnames)):\n",
    "    _,tf,ff = get_feat_id(range(ofs))\n",
    "    amfft_inds = []\n",
    "    temp1 = deepcopy(data)\n",
    "    \n",
    "    for i in range(len(featnames)):\n",
    "        if (featnames[i].startswith('amFFT')):\n",
    "            amfft_inds.append(i)\n",
    "\n",
    "    if (fs_ind == 2):\n",
    "        ff2 = [ff[i]+ofs for i in range(len(ff))]\n",
    "        tf2 = [tf[i]+ofs for i in range(len(tf))]\n",
    "        amfft2 = [amfft_inds[i]+ofs for i in range(len(amfft_inds))]\n",
    "        freqf = ff2 + ff\n",
    "        timef = tf2 + tf\n",
    "        amfft = amfft_inds + amfft2\n",
    "    else:\n",
    "        freqf = ff\n",
    "        timef = tf\n",
    "        amfft = amfft_inds\n",
    "\n",
    "    X_amfft = temp1[:,amfft]\n",
    "    X_time = np.delete(temp1,freqf,axis=1)\n",
    "    X_freq_all = np.delete(temp1,timef,axis=1)\n",
    "    X_both = data\n",
    "    return X_amfft, X_freq_all, X_time, X_both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## Prepare the dataset split for each surface ######################################\n",
    "surffile = datapath+'2_fingers_6_surfaces.npz'\n",
    "if os.path.isfile(surffile):\n",
    "    surf = np.load(surffile)['surf']\n",
    "    surfla = np.load(surffile)['surfla']\n",
    "else:\n",
    "    surf, surfla = [], []\n",
    "    for i in range(len(prefeatid)-1): # for each featureset (corresponding to each prefeature, here only |f|)\n",
    "        surf1, surfla1 = surface_split(Xsp[2], Ysp[2])\n",
    "        tmpsurf = deepcopy(surf1)\n",
    "        tmpsurfla = deepcopy(surfla1)\n",
    "        tmpsurfsubfeat = []\n",
    "        for j in range(tmpsurf.shape[0]+1): # for each surface\n",
    "            print i,j,surf1.shape\n",
    "            if j == tmpsurf.shape[0]:\n",
    "                tmpsurfsubfeat.append(feat_subsets(tmpsurf[j-1,:-1,:],i)) # ommit a sample for converting to array\n",
    "            else:\n",
    "                tmpsurfsubfeat.append(feat_subsets(tmpsurf[j],i)) # keep all subfeaturesets\n",
    "        surf.append(tmpsurfsubfeat)\n",
    "        surfla.append(surfla1)\n",
    "    surf = np.array(surf).transpose()[:,:-1,:] # dims: (subfeaturesets, surfaces, featuresets)\n",
    "    surfla = np.array(surfla).transpose() # dims: (,surfaces, featuresets)\n",
    "    np.savez(surffile,surf=surf,surfla=surfla)\n",
    "print surf.shape, surfla.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## TRAINING DEFAULTS\n",
    "cv = KFold(n_splits=5,random_state=42)\n",
    "scaler = StandardScaler() ;\n",
    "decomp = PCA(n_components=20)\n",
    "i = len(prefeatid)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######## Cross surface validation, TRAINING with 1 surface each time, out of 6 surfaces in total\n",
    "def filename(i,j,k,l):\n",
    "    return 'fs_'+str(i)+'_subfs_'+str(j)+'_tr_'+str(k)+'_ts_'+str(l)\n",
    "\n",
    "def cross_fit(i,j,k,l,data,labels,data2,labels2,pipe):\n",
    "    filepath = datapath+'/results1/'\n",
    "    ensure_dir(filepath)\n",
    "    fileid = filepath+filename(i,j,k,l)+'.npz'\n",
    "    if not os.path.isfile(fileid):\n",
    "        print i,j,k,l\n",
    "        if k==l: # perform K-fold             \n",
    "            folds = cv.split(data, labels)\n",
    "            cm_all = np.zeros((2,2))\n",
    "            for fold, (train_ind, test_ind) in enumerate(folds):\n",
    "                x_train, x_test = data[train_ind], data[test_ind]\n",
    "                y_train, y_test = labels[train_ind], labels[test_ind]\n",
    "                model = pipe.fit(x_train,y_train)\n",
    "                y_pred = model.predict(x_test)\n",
    "                cm = confusion_matrix(y_pred=y_pred, y_true=y_test)\n",
    "                cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "                cm_all += cm/5.\n",
    "            np.savez(fileid,cm=cm_all,model=np.array([model]))\n",
    "        else: # perform cross-check\n",
    "            tr_data = data\n",
    "            tr_labels = labels\n",
    "            ts_data = data2\n",
    "            ts_labels = labels2\n",
    "            print 'Fitting on '+str(k)+', testing on '+str(l)+'...'\n",
    "            model = pipe.fit(tr_data,tr_labels)\n",
    "            y_pred = model.predict(ts_data)\n",
    "            cm = confusion_matrix(y_pred=y_pred, y_true=ts_labels)\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            np.savez(fileid,cm=cm,model=np.array([model]))\n",
    "\n",
    "def init_steps(i,j,jmax,surf,surfla):\n",
    "    if j==jmax:\n",
    "        featsel = SelectKBest(k=1000,score_func= mutual_info_classif)\n",
    "    else:\n",
    "        featsel = SelectKBest(k='all',score_func= mutual_info_classif)\n",
    "    pipe = make_pipe_clf(scaler, featsel, decomp, classifiers[2])\n",
    "    for k in range(surf.shape[0]): # for every training surface\n",
    "        for l in range(surf.shape[0]): # for every testing surface\n",
    "            cross_fit(i,j,k,l,surf[k],surfla[:,k],surf[l],surfla[:,l],pipe)\n",
    "\n",
    "##### Parallelized -on surface level- training of models with 1 surface\n",
    "[Parallel(n_jobs=-1)([delayed(init_steps) (i,j,surf.shape[0]-1,surf[j,:,i],surfla[:,:,i]) for j in range(surf.shape[0])])]            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Cross surface validation, training with 2 surfaces each time, out of 6 surfaces in total\n",
    "def filename(i,j,k1,k2,l):\n",
    "    return 'fs_'+str(i)+'_subfs_'+str(j)+'_tr1_'+str(k1)+'_tr2_'+str(k2)+'_ts_'+str(l)\n",
    "\n",
    "def cross_fit(i,j,k1,k2,l,data,labels,data2,labels2,pipe):\n",
    "    filepath = datapath+'/results2/'\n",
    "    ensure_dir(filepath)\n",
    "    fileid = filepath+filename(i,j,k1,k2,l)+'.npz'\n",
    "    if not os.path.isfile(fileid):\n",
    "        print i,j,k1,k2,l\n",
    "        if k1==l or k2==l: # perform K-fold      \n",
    "            print 'Fitting on '+str(k1)+\"-\"+str(k2)+', cross-validating on '+str(l)+'...'\n",
    "            if l == k1: # copy if existent from the other sibling file\n",
    "                tmpcopyfileid = filepath+filename(i,j,k1,k2,k2)+'.npz'\n",
    "            else:   # same as above\n",
    "                tmpcopyfileid = filepath+filename(i,j,k1,k2,k1)+'.npz'                \n",
    "            if not os.path.isfile(tmpcopyfileid):\n",
    "                folds = cv.split(data, labels)\n",
    "                cm_all = np.zeros((2,2))\n",
    "                for fold, (train_ind, test_ind) in enumerate(folds):\n",
    "                    x_train, x_test = data[train_ind], data[test_ind]\n",
    "                    y_train, y_test = labels[train_ind], labels[test_ind]\n",
    "                    model = pipe.fit(x_train,y_train)\n",
    "                    y_pred = model.predict(x_test)\n",
    "                    cm = confusion_matrix(y_pred=y_pred, y_true=y_test)\n",
    "                    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "                    cm_all += cm/5.\n",
    "            else:\n",
    "                cm_all = np.load(tmpcopyfileid)['cm']\n",
    "                model = np.load(tmpcopyfileid)['model'][0]\n",
    "            np.savez(fileid,cm=cm_all,model=np.array([model]))\n",
    "        else: # perform cross-check\n",
    "            tr_data = data\n",
    "            tr_labels = labels\n",
    "            ts_data = data2\n",
    "            ts_labels = labels2\n",
    "            print 'Fitting on '+str(k1)+\"-\"+str(k2)+', testing on '+str(l)+'...'\n",
    "            model = pipe.fit(tr_data,tr_labels)\n",
    "            y_pred = model.predict(ts_data)\n",
    "            cm = confusion_matrix(y_pred=y_pred, y_true=ts_labels)\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            np.savez(fileid,cm=cm,model=np.array([model]))\n",
    "\n",
    "def init_steps(i,j,jmax,surf,surfla):\n",
    "    if j==jmax:\n",
    "        featsel = SelectKBest(k=1000,score_func= mutual_info_classif)\n",
    "    else:\n",
    "        featsel = SelectKBest(k='all',score_func= mutual_info_classif)\n",
    "    pipe = make_pipe_clf(scaler, featsel, decomp, classifiers[2])\n",
    "    for k1 in range(surf.shape[0]): # for every training surface1\n",
    "        for k2 in range(surf.shape[0]): # for every training surface2\n",
    "            if k2 > k1:\n",
    "                for l in range(surf.shape[0]): # for every testing surface\n",
    "                    tr_surf, tr_surfla = np.concatenate((surf[k1],surf[k2]),axis=0), np.concatenate((surfla[:,k1],surfla[:,k2]),axis=0)\n",
    "                    ts_surf, ts_surfla = surf[l], surfla[:,l]\n",
    "                    cross_fit(i,j,k1,k2,l,tr_surf,tr_surfla,ts_surf,ts_surfla,pipe)\n",
    "\n",
    "##### Parallelized -on surface level- training of models with 2 surfaces\n",
    "[Parallel(n_jobs=-1)([delayed(init_steps) (i,j,surf.shape[0]-1,surf[j,:,i],surfla[:,:,i]) for j in range(surf.shape[0])])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######## Cross surface validation, training with 3 surfaces each time, out of 6 surfaces in total\n",
    "def filename(i,j,k1,k2,k3,l):\n",
    "    return 'fs_'+str(i)+'_subfs_'+str(j)+'_tr1_'+str(k1)+'_tr2_'+str(k2)+'_tr3_'+str(k3)+'_ts_'+str(l)\n",
    "\n",
    "def cross_fit(i,j,k1,k2,k3,l,data,labels,data2,labels2,pipe):\n",
    "    filepath = datapath+'/results3/'\n",
    "    ensure_dir(filepath)\n",
    "    fileid = filepath+filename(i,j,k1,k2,k3,l)+'.npz'\n",
    "    if not os.path.isfile(fileid):\n",
    "        print i,j,k1,k2,k3,l\n",
    "        if k1==l or k2==l or k3==l: # perform K-fold      \n",
    "            print 'Fitting on '+str(k1)+\"-\"+str(k2)+\"-\"+str(k3)+', cross-validating on '+str(l)+'...'\n",
    "            if l == k1: # copy if existent from the other sibling file\n",
    "                tmpcopyfileid1 = filepath+filename(i,j,k1,k2,k3,k2)+'.npz'\n",
    "                tmpcopyfileid2 = filepath+filename(i,j,k1,k2,k3,k3)+'.npz'\n",
    "            elif l == k2:   # same as above\n",
    "                tmpcopyfileid1 = filepath+filename(i,j,k1,k2,k3,k1)+'.npz'\n",
    "                tmpcopyfileid2 = filepath+filename(i,j,k1,k2,k3,k3)+'.npz'\n",
    "            else:\n",
    "                tmpcopyfileid1 = filepath+filename(i,j,k1,k2,k3,k1)+'.npz'\n",
    "                tmpcopyfileid2 = filepath+filename(i,j,k1,k2,k3,k2)+'.npz'\n",
    "            if not os.path.isfile(tmpcopyfileid1) and not os.path.isfile(tmpcopyfileid2):\n",
    "                folds = cv.split(data, labels)\n",
    "                cm_all = np.zeros((2,2))\n",
    "                for fold, (train_ind, test_ind) in enumerate(folds):\n",
    "                    x_train, x_test = data[train_ind], data[test_ind]\n",
    "                    y_train, y_test = labels[train_ind], labels[test_ind]\n",
    "                    model = pipe.fit(x_train,y_train)\n",
    "                    y_pred = model.predict(x_test)\n",
    "                    cm = confusion_matrix(y_pred=y_pred, y_true=y_test)\n",
    "                    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "                    cm_all += cm/5.\n",
    "            else:\n",
    "                if os.path.isfile(tmpcopyfileid1):\n",
    "                    cm_all = np.load(tmpcopyfileid1)['cm']\n",
    "                    model = np.load(tmpcopyfileid1)['model'][0]\n",
    "                else:\n",
    "                    cm_all = np.load(tmpcopyfileid2)['cm']\n",
    "                    model = np.load(tmpcopyfileid2)['model'][0]\n",
    "            np.savez(fileid,cm=cm_all,model=np.array([model]))\n",
    "        else: # perform cross-check\n",
    "            tr_data = data\n",
    "            tr_labels = labels\n",
    "            ts_data = data2\n",
    "            ts_labels = labels2\n",
    "            print 'Fitting on '+str(k1)+\"-\"+str(k2)+\"-\"+str(k3)+', testing on '+str(l)+'...'\n",
    "            model = pipe.fit(tr_data,tr_labels)\n",
    "            y_pred = model.predict(ts_data)\n",
    "            cm = confusion_matrix(y_pred=y_pred, y_true=ts_labels)\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            np.savez(fileid,cm=cm,model=np.array([model]))\n",
    "\n",
    "def init_steps(i,j,jmax,surf,surfla):\n",
    "    if j==jmax:\n",
    "        featsel = SelectKBest(k=1000,score_func= mutual_info_classif)\n",
    "    else:\n",
    "        featsel = SelectKBest(k='all',score_func= mutual_info_classif)\n",
    "    pipe = make_pipe_clf(scaler, featsel, decomp, classifiers[2])\n",
    "    for k1 in range(surf.shape[0]): # for every training surface1\n",
    "        for k2 in range(surf.shape[0]): # for every training surface2\n",
    "            if k2 > k1:\n",
    "                for k3 in range(surf.shape[0]):\n",
    "                    if k3 > k2:\n",
    "                        for l in range(surf.shape[0]): # for every testing surface\n",
    "                            tr_surf, tr_surfla = np.concatenate((surf[k1],surf[k2],surf[k3]),axis=0), np.concatenate((surfla[:,k1],surfla[:,k2],surfla[:,k3]),axis=0)\n",
    "                            ts_surf, ts_surfla = surf[l], surfla[:,l]\n",
    "                            cross_fit(i,j,k1,k2,k3,l,tr_surf,tr_surfla,ts_surf,ts_surfla,pipe)\n",
    "\n",
    "##### Parallelized -on surface level- training of models with 3 surfaces\n",
    "[Parallel(n_jobs=-1)([delayed(init_steps) (i,j,surf.shape[0]-1,surf[j,:,i],surfla[:,:,i]) for j in range(surf.shape[0])])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######## Cross surface validation, training with 4 surfaces each time, out of 6 surfaces in total\n",
    "def filename(i,j,k1,k2,k3,k4,l):\n",
    "    return 'fs_'+str(i)+'_subfs_'+str(j)+'_tr1_'+str(k1)+'_tr2_'+str(k2)+'_tr3_'+str(k3)+'_tr4_'+str(k4)+'_ts_'+str(l)\n",
    "\n",
    "def cross_fit(i,j,k1,k2,k3,k4,l,data,labels,data2,labels2,pipe):\n",
    "    filepath = datapath+'/results4/'\n",
    "    ensure_dir(filepath)\n",
    "    fileid = filepath+filename(i,j,k1,k2,k3,k4,l)+'.npz'\n",
    "    if not os.path.isfile(fileid):\n",
    "        print i,j,k1,k2,k3,k4,l\n",
    "        if k1==l or k2==l or k3==l or k4==l: # perform K-fold      \n",
    "            print 'Fitting on '+str(k1)+\"-\"+str(k2)+\"-\"+str(k3)+\"-\"+str(k4)+', cross-validating on '+str(l)+'...'\n",
    "            if l == k1: # copy if existent from the other sibling file\n",
    "                tmpcopyfileid1 = filepath+filename(i,j,k1,k2,k3,k4,k2)+'.npz'\n",
    "                tmpcopyfileid2 = filepath+filename(i,j,k1,k2,k3,k4,k3)+'.npz'\n",
    "                tmpcopyfileid3 = filepath+filename(i,j,k1,k2,k3,k4,k4)+'.npz'\n",
    "            elif l == k2:   # same as above\n",
    "                tmpcopyfileid1 = filepath+filename(i,j,k1,k2,k3,k4,k1)+'.npz'\n",
    "                tmpcopyfileid2 = filepath+filename(i,j,k1,k2,k3,k4,k3)+'.npz'\n",
    "                tmpcopyfileid3 = filepath+filename(i,j,k1,k2,k3,k4,k4)+'.npz'\n",
    "            elif l == k3:   # same as above\n",
    "                tmpcopyfileid1 = filepath+filename(i,j,k1,k2,k3,k4,k1)+'.npz'\n",
    "                tmpcopyfileid2 = filepath+filename(i,j,k1,k2,k3,k4,k2)+'.npz'\n",
    "                tmpcopyfileid3 = filepath+filename(i,j,k1,k2,k3,k4,k4)+'.npz'\n",
    "            else:\n",
    "                tmpcopyfileid1 = filepath+filename(i,j,k1,k2,k3,k4,k1)+'.npz'\n",
    "                tmpcopyfileid2 = filepath+filename(i,j,k1,k2,k3,k4,k2)+'.npz'\n",
    "                tmpcopyfileid3 = filepath+filename(i,j,k1,k2,k3,k4,k3)+'.npz'\n",
    "            if not os.path.isfile(tmpcopyfileid1) and not os.path.isfile(tmpcopyfileid2) and not os.path.isfile(tmpcopyfileid3):\n",
    "                folds = cv.split(data, labels)\n",
    "                cm_all = np.zeros((2,2))\n",
    "                for fold, (train_ind, test_ind) in enumerate(folds):\n",
    "                    x_train, x_test = data[train_ind], data[test_ind]\n",
    "                    y_train, y_test = labels[train_ind], labels[test_ind]\n",
    "                    model = pipe.fit(x_train,y_train)\n",
    "                    y_pred = model.predict(x_test)\n",
    "                    cm = confusion_matrix(y_pred=y_pred, y_true=y_test)\n",
    "                    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "                    cm_all += cm/5.\n",
    "            else:\n",
    "                if os.path.isfile(tmpcopyfileid1):\n",
    "                    cm_all = np.load(tmpcopyfileid1)['cm']\n",
    "                    model = np.load(tmpcopyfileid1)['model'][0]\n",
    "                elif os.path.isfile(tmpcopyfileid2):\n",
    "                    cm_all = np.load(tmpcopyfileid2)['cm']\n",
    "                    model = np.load(tmpcopyfileid2)['model'][0]\n",
    "                elif os.path.isfile(tmpcopyfileid3):\n",
    "                    cm_all = np.load(tmpcopyfileid3)['cm']\n",
    "                    model = np.load(tmpcopyfileid3)['model'][0]\n",
    "            np.savez(fileid,cm=cm_all,model=np.array([model]))\n",
    "        else: # perform cross-check\n",
    "            tr_data = data\n",
    "            tr_labels = labels\n",
    "            ts_data = data2\n",
    "            ts_labels = labels2\n",
    "            print 'Fitting on '+str(k1)+\"-\"+str(k2)+\"-\"+str(k3)+\"-\"+str(k4)+', testing on '+str(l)+'...'\n",
    "            model = pipe.fit(tr_data,tr_labels)\n",
    "            y_pred = model.predict(ts_data)\n",
    "            cm = confusion_matrix(y_pred=y_pred, y_true=ts_labels)\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            np.savez(fileid,cm=cm,model=np.array([model]))\n",
    "\n",
    "def init_steps(i,j,jmax,surf,surfla):\n",
    "    if j==jmax:\n",
    "        featsel = SelectKBest(k=1000,score_func= mutual_info_classif)\n",
    "    else:\n",
    "        featsel = SelectKBest(k='all',score_func= mutual_info_classif)\n",
    "    pipe = make_pipe_clf(scaler, featsel, decomp, classifiers[2])\n",
    "    for k1 in range(surf.shape[0]): # for every training surface1\n",
    "        for k2 in range(surf.shape[0]): # for every training surface2\n",
    "            if k2 > k1:\n",
    "                for k3 in range(surf.shape[0]):\n",
    "                    if k3 > k2:\n",
    "                        for k4 in range(surf.shape[0]):\n",
    "                            if k4 > k3:\n",
    "                                for l in range(surf.shape[0]): # for every testing surface\n",
    "                                    tr_surf, tr_surfla = np.concatenate((surf[k1],surf[k2],surf[k3]),axis=0), np.concatenate((surfla[:,k1],surfla[:,k2],surfla[:,k3]),axis=0)\n",
    "                                    ts_surf, ts_surfla = surf[l], surfla[:,l]\n",
    "                                    cross_fit(i,j,k1,k2,k3,k4,l,tr_surf,tr_surfla,ts_surf,ts_surfla,pipe)\n",
    "\n",
    "##### Parallelized -on surface level- training of models with 4 surfaces                    \n",
    "[Parallel(n_jobs=-1)([delayed(init_steps) (i,j,surf.shape[0]-1,surf[j,:,i],surfla[:,:,i]) for j in range(surf.shape[0])])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Cross surface validation, training with 5 surfaces each time, out of 6 surfaces in total\n",
    "def filename(i,j,k1,k2,k3,k4,k5,l):\n",
    "    return 'fs_'+str(i)+'_subfs_'+str(j)+'_tr1_'+str(k1)+'_tr2_'+str(k2)+'_tr3_'+str(k3)+'_tr4_'+str(k4)+'_tr5_'+str(k5)+'_ts_'+str(l)\n",
    "\n",
    "def cross_fit(i,j,k1,k2,k3,k4,k5,l,data,labels,data2,labels2,pipe):\n",
    "    filepath = datapath+'/results5/'\n",
    "    ensure_dir(filepath)\n",
    "    fileid = filepath+filename(i,j,k1,k2,k3,k4,k5,l)+'.npz'\n",
    "    if not os.path.isfile(fileid):\n",
    "        print i,j,k1,k2,k3,k4,k5,l\n",
    "        if k1==l or k2==l or k3==l or k4==l or k5==l: # perform K-fold      \n",
    "            print 'Fitting on '+str(k1)+\"-\"+str(k2)+\"-\"+str(k3)+\"-\"+str(k4)+\"-\"+str(k5)+', cross-validating on '+str(l)+'...'\n",
    "            if l == k1: # copy if existent from the other sibling file\n",
    "                tmpcopyfileid1 = filepath+filename(i,j,k1,k2,k3,k4,k5,k2)+'.npz'\n",
    "                tmpcopyfileid2 = filepath+filename(i,j,k1,k2,k3,k4,k5,k3)+'.npz'\n",
    "                tmpcopyfileid3 = filepath+filename(i,j,k1,k2,k3,k4,k5,k4)+'.npz'\n",
    "                tmpcopyfileid4 = filepath+filename(i,j,k1,k2,k3,k4,k5,k5)+'.npz'\n",
    "            elif l == k2:   # same as above\n",
    "                tmpcopyfileid1 = filepath+filename(i,j,k1,k2,k3,k4,k5,k1)+'.npz'\n",
    "                tmpcopyfileid2 = filepath+filename(i,j,k1,k2,k3,k4,k5,k3)+'.npz'\n",
    "                tmpcopyfileid3 = filepath+filename(i,j,k1,k2,k3,k4,k5,k4)+'.npz'\n",
    "                tmpcopyfileid4 = filepath+filename(i,j,k1,k2,k3,k4,k5,k5)+'.npz'\n",
    "            elif l == k3:   # same as above\n",
    "                tmpcopyfileid1 = filepath+filename(i,j,k1,k2,k3,k4,k5,k1)+'.npz'\n",
    "                tmpcopyfileid2 = filepath+filename(i,j,k1,k2,k3,k4,k5,k2)+'.npz'\n",
    "                tmpcopyfileid3 = filepath+filename(i,j,k1,k2,k3,k4,k5,k4)+'.npz'\n",
    "                tmpcopyfileid4 = filepath+filename(i,j,k1,k2,k3,k4,k5,k5)+'.npz'\n",
    "            elif l == k4:   # same as above\n",
    "                tmpcopyfileid1 = filepath+filename(i,j,k1,k2,k3,k4,k5,k1)+'.npz'\n",
    "                tmpcopyfileid2 = filepath+filename(i,j,k1,k2,k3,k4,k5,k2)+'.npz'\n",
    "                tmpcopyfileid3 = filepath+filename(i,j,k1,k2,k3,k4,k5,k3)+'.npz'\n",
    "                tmpcopyfileid4 = filepath+filename(i,j,k1,k2,k3,k4,k5,k5)+'.npz'\n",
    "            else:\n",
    "                tmpcopyfileid1 = filepath+filename(i,j,k1,k2,k3,k4,k5,k1)+'.npz'\n",
    "                tmpcopyfileid2 = filepath+filename(i,j,k1,k2,k3,k4,k5,k2)+'.npz'\n",
    "                tmpcopyfileid3 = filepath+filename(i,j,k1,k2,k3,k4,k5,k3)+'.npz'\n",
    "                tmpcopyfileid4 = filepath+filename(i,j,k1,k2,k3,k4,k5,k4)+'.npz'\n",
    "            if not os.path.isfile(tmpcopyfileid1) and not os.path.isfile(tmpcopyfileid2) and not os.path.isfile(tmpcopyfileid3) and not os.path.isfile(tmpcopyfileid4):\n",
    "                folds = cv.split(data, labels)\n",
    "                cm_all = np.zeros((2,2))\n",
    "                for fold, (train_ind, test_ind) in enumerate(folds):\n",
    "                    x_train, x_test = data[train_ind], data[test_ind]\n",
    "                    y_train, y_test = labels[train_ind], labels[test_ind]\n",
    "                    model = pipe.fit(x_train,y_train)\n",
    "                    y_pred = model.predict(x_test)\n",
    "                    cm = confusion_matrix(y_pred=y_pred, y_true=y_test)\n",
    "                    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "                    cm_all += cm/5.\n",
    "            else:\n",
    "                if os.path.isfile(tmpcopyfileid1):\n",
    "                    cm_all = np.load(tmpcopyfileid1)['cm']\n",
    "                    model = np.load(tmpcopyfileid1)['model'][0]\n",
    "                elif os.path.isfile(tmpcopyfileid2):\n",
    "                    cm_all = np.load(tmpcopyfileid2)['cm']\n",
    "                    model = np.load(tmpcopyfileid2)['model'][0]\n",
    "                elif os.path.isfile(tmpcopyfileid3):\n",
    "                    cm_all = np.load(tmpcopyfileid3)['cm']\n",
    "                    model = np.load(tmpcopyfileid3)['model'][0]\n",
    "                elif os.path.isfile(tmpcopyfileid4):\n",
    "                    cm_all = np.load(tmpcopyfileid4)['cm']\n",
    "                    model = np.load(tmpcopyfileid4)['model'][0]\n",
    "            np.savez(fileid,cm=cm_all,model=np.array([model]))\n",
    "        else: # perform cross-check\n",
    "            tr_data = data\n",
    "            tr_labels = labels\n",
    "            ts_data = data2\n",
    "            ts_labels = labels2\n",
    "            print 'Fitting on '+str(k1)+\"-\"+str(k2)+\"-\"+str(k3)+\"-\"+str(k4)+\"-\"+str(k5)+', testing on '+str(l)+'...'\n",
    "            model = pipe.fit(tr_data,tr_labels)\n",
    "            y_pred = model.predict(ts_data)\n",
    "            cm = confusion_matrix(y_pred=y_pred, y_true=ts_labels)\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            np.savez(fileid,cm=cm,model=np.array([model]))\n",
    "\n",
    "def init_steps(i,j,jmax,surf,surfla):\n",
    "    if j==jmax:\n",
    "        featsel = SelectKBest(k=1000,score_func= mutual_info_classif)\n",
    "    else:\n",
    "        featsel = SelectKBest(k='all',score_func= mutual_info_classif)\n",
    "    pipe = make_pipe_clf(scaler, featsel, decomp, classifiers[2])\n",
    "    for k1 in range(surf.shape[0]): # for every training surface1\n",
    "        for k2 in range(surf.shape[0]): # for every training surface2\n",
    "            if k2 > k1:\n",
    "                for k3 in range(surf.shape[0]):\n",
    "                    if k3 > k2:\n",
    "                        for k4 in range(surf.shape[0]):\n",
    "                            if k4 > k3:\n",
    "                                for k5 in range(surf.shape[0]):\n",
    "                                    if k5 > k4:\n",
    "                                        for l in range(surf.shape[0]): # for every testing surface\n",
    "                                            tr_surf, tr_surfla = np.concatenate((surf[k1],surf[k2],surf[k3]),axis=0), np.concatenate((surfla[:,k1],surfla[:,k2],surfla[:,k3]),axis=0)\n",
    "                                            ts_surf, ts_surfla = surf[l], surfla[:,l]\n",
    "                                            cross_fit(i,j,k1,k2,k3,k4,k5,l,tr_surf,tr_surfla,ts_surf,ts_surfla,pipe)\n",
    "\n",
    "##### Parallelized -on surface level- training of models with 5 surfaces\n",
    "[Parallel(n_jobs=-1)([delayed(init_steps) (i,j,surf.shape[0]-1,surf[j,:,i],surfla[:,:,i]) for j in range(surf.shape[0])])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
