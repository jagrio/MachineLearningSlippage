{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    }
   ],
   "source": [
    "# Initial Code source by:                Gaël Varoquaux\n",
    "#                                        Andreas Müller\n",
    "# Modified for documentation by:         Jaques Grobler\n",
    "# Mainly Modified for private usage by:  Ioannis Agriomallos\n",
    "#                                        Ioanna Mitsioni\n",
    "# License: BSD 3 clause\n",
    "####################################################################################################################\n",
    "######==================== CURRENT CODE USAGE ======================================================================\n",
    "# -> Current code trains several ML Classifiers, to classify force input samples as stable (0) or slip (1)\n",
    "######---- Input\n",
    "# -> Input samples originate from optoforce sensors and are 3D (fx,fy,fz) and come from 3 different datasets, \n",
    "# containing several surfaces as well as slip-stable occurrences\n",
    "######---- Input transformation\n",
    "# -> Several pre-features are taken from these inputs.\n",
    "# -> Several time and frequency domain features are extracted from pre-feature windows. \n",
    "# (implemented in 'newfeatext.py') These windows have size w and are shifted by s on each sample\n",
    "# -> Then a feature selection-ranking is performed using MutualVariableInformation\n",
    "# -> Finally PCA is performed to keep the desired among the best selected features\n",
    "######---- Training of ML Classifiers\n",
    "# -> Several ML Classifiers are trained for all combinations of features-datasets\n",
    "######---- Stats report\n",
    "# -> Several stats are reported on the classification results of the trained models, \n",
    "# based on their accuracy (fscore and auc as well) on test data as well as previously unseen data\n",
    "######===================== POTENTIAL CODE EXTENSION FOR FURTHER MORE GENERIC USAGE ================================\n",
    "# -> As binary classification problem it may apply to all such problems given that:\n",
    "######----- Input\n",
    "# -> Input is loaded on datasets, which expect (n,1) input with (k,4) dimensions each,\n",
    "# where n data samples, k measurement in each sample, 3 force readings and 1 label for each measurement.\n",
    "######----- Input transformation\n",
    "# -> Pre-features must be changed or ommitted to fit your needs. \n",
    "# Here they served to transform the 3 (fx,fy,fz) force readings into one between (ft,fn,ft/fn,sf)\n",
    "# -> Features are performed on the desired pre-features. They take a (n,o,m) input matrix and return a (n,o,l),\n",
    "# where n data samples, o windows, m prefeatures, l features.\n",
    "# -> Feature selection gives a ranking on each l, so we keep b from the best\n",
    "# -> PCA is performed on the b selected features resulting in p dimensions (n,o,p) => (n*o,p)\n",
    "######----- Training of ML Classifiers\n",
    "# -> Each Classifier is trained on a subset of (n*o,p)\n",
    "######----- Stats\n",
    "# -> Change accordingly!\n",
    "####################################################################################################################\n",
    "print(__doc__)\n",
    "import time\n",
    "start_time = time.time()\n",
    "from copy import deepcopy, copy\n",
    "import math\n",
    "import scipy.io as sio\n",
    "import shutil\n",
    "import os, errno\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "from pylab import *\n",
    "from featext2 import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "# %matplotlib qt\n",
    "# inline (suitable for ipython only, shown inside browser!) or qt (suitable in general, shown in external window!)\n",
    "from matplotlib.colors import ListedColormap\n",
    "from mpl_toolkits.mplot3d import Axes3D #, axes3d\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold, ParameterGrid, KFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, normalize\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.decomposition import PCA, KernelPCA, TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, LassoCV, RandomizedLasso\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest, chi2, f_classif, mutual_info_classif, SelectFdr\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "import datetime\n",
    "import urllib\n",
    "import tarfile\n",
    "import joblib\n",
    "from joblib import Parallel, delayed, Memory\n",
    "from tempfile import mkdtemp\n",
    "import copy_reg\n",
    "import types\n",
    "import itertools\n",
    "import pywt\n",
    "from scipy import signal\n",
    "from scipy.signal import lfilter, butter\n",
    "from numpy.fft import fft, fftfreq, rfft\n",
    "\n",
    "# useful function for successful convertion from directories and lists to numpy arrays\n",
    "def _pickle_method(m):\n",
    "    if m.im_self is None:\n",
    "        return getattr, (m.im_class, m.im_func.func_name)\n",
    "    else:\n",
    "        return getattr, (m.im_self, m.im_func.func_name)\n",
    "copy_reg.pickle(types.MethodType, _pickle_method)\n",
    "\n",
    "# useful function for creating directory only if not existent\n",
    "def ensure_dir(directory):\n",
    "    try:\n",
    "        os.makedirs(directory)\n",
    "    except OSError as e:\n",
    "        if e.errno != errno.EEXIST:\n",
    "            raise\n",
    "            \n",
    "h = .2  # step size in the mesh\n",
    "\n",
    "######## TRAINING DEFAULTS\n",
    "cv = KFold(n_splits=5,random_state=42)\n",
    "scaler = StandardScaler() ;\n",
    "decomp = PCA(n_components=20)\n",
    "names = [\"NearNb\", \"RBFSVM1\", \"MLP1\", \"RandFor\"]\n",
    "classifiers = [KNeighborsClassifier(5),\n",
    "               SVC(gamma='auto', C=1),\n",
    "               MLPClassifier(solver='lbfgs',alpha=1e-4,hidden_layer_sizes=(10,10),random_state=1,verbose=True),\n",
    "               RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)]\n",
    "parameters_clf = [{'n_neighbors':[4,7,9,12]},\n",
    "                  {'kernel':['rbf'], 'C':[0.1,10,100,1000]},\n",
    "                  {'solver':['lbfgs'], 'alpha':[1e-5,1e-2], 'hidden_layer_sizes':[(10,10),(50,50)]},\n",
    "                  {'max_depth':[4,7,10,20],'n_estimators':[5,10,20],'max_features':[20,35,50]}]\n",
    "makepipe_parameters_clf = [{'classifier__'+key:p[key] for key in p} for p in parameters_clf]\n",
    "makepipe_parameters_clf += [{'feature_selection__k': (750,500,100), 'feature_selection__score_func': [mutual_info_classif]},\n",
    "                            {'decomp__n_components': (100,50)}]\n",
    "\n",
    "metric = ['accuracy','f1']\n",
    "download = 1 # Download pre-computed (1) data or compute them all anew (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################## INITIALISATION PARAMETERS #################################################\n",
    "window, shift, keepfromshift = 1024, 20, 10\n",
    "samplesperdataset = 10000\n",
    "havelabel = 1\n",
    "returntime = 0\n",
    "featlabel = 0         # 0: all features, 1: temporal, 2: frequency, 3: FFT only\n",
    "magnFFT = 0           # 0: FFT in magnitude format, 1: FFT in real and imag format, \n",
    "featall = 0           # 0: all, 1: feat1 (phinyomark's), 2: feat2 (golz's)\n",
    "featparam = [havelabel,featlabel,magnFFT,featall,returntime]\n",
    "metr = metric[0]      # 0: accuracy, 1: f1score\n",
    "CV = 5                # cross validation checks\n",
    "numfeat = 10 # number of features to show\n",
    "nfeat = 1000 # number of features to keep\n",
    "######################################## Initialize necessary names and paths #####################################\n",
    "datapath = 'tmp/'\n",
    "ensure_dir(datapath)\n",
    "datafile = datapath+'dataset.npz'\n",
    "validfile = datapath+'validation.mat'\n",
    "featpath = datapath+'features/'+str(window)+'_'+str(shift)+'/'\n",
    "ensure_dir(featpath)\n",
    "prefeatid = [0,4]   # only the prefeatures with corresponding ids will be computed\n",
    "prefeatname = 'prefeatures'+'_'+str(window)+'_'+str(shift)+'_'+str(keepfromshift)+'_'+str(samplesperdataset)\n",
    "prefeatfile = featpath+prefeatname+'.npz'\n",
    "featname = 'features'+'_'+str(window)+'_'+str(shift)+'_'+str(keepfromshift)+'_'+str(samplesperdataset)\n",
    "featfile = featpath+featname+'.npz'\n",
    "validfeatname = 'valid'+featname\n",
    "validfeatfile = featpath+validfeatname+'.npz'\n",
    "surffile = featpath+featname+'_2fing_6surf.npz'\n",
    "XYfile = featpath+featname+'_XY.npz'\n",
    "XYsplitfile = featpath+featname+'_XYsplit.npz'\n",
    "validsurffile = featpath+featname+'_2fing_6surf_valid.npz'\n",
    "validXYfile = featpath+featname+'_XY_valid.npz'\n",
    "validXYsplitfile = featpath+featname+'_XYsplit_valid.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################## Feature Names ###########################################################\n",
    "####################################################################################################################\n",
    "##  features:                                                                                  ||      if         ##\n",
    "##  |----------> time domain      :                                                            || samples = 1024  ##\n",
    "##  |------------|---> phinyomark : 11+3{shist} -----------------------------> = 14+0.0samples ||             14  ##\n",
    "##  |------------|---> golz       : 10+samples{acrol} -----------------------> = 10+1.0samples ||           1034  ##\n",
    "##  |----------> frequency domain :                                                                               ##\n",
    "##  |------------|---> phinyomark : 3{arco}+4{mf}+2(samples/2+1){RF,IF} -----> =  9+1.0samples ||           1033  ##\n",
    "##  |------------|---> golz       : 2(samples/2+1){AF,PF} -------------------> =  2+1.0samples ||           1026  ##\n",
    "##  |------------|--------|-------alltogether--------------------------------> = 35+3.0samples || numfeat = 3107  ##\n",
    "####################################################################################################################\n",
    "## Time Domain Phinyomark feats\n",
    "featnames = ['intsgnl', 'meanabs', 'meanabsslp', 'ssi', 'var', 'rms', 'rng', 'wavl', 'zerox', 'ssc', 'wamp', \n",
    "             'shist1', 'shist2', 'shist3']                                                   # 11+3{shist}\n",
    "## Frequency Domain Phinyomark feats\n",
    "featnames += ['arco1', 'arco2', 'arco3', 'mnf', 'mdf', 'mmnf', 'mmdf']                       # 3{arco}+4{mf}\n",
    "featnames += ['reFFT{:03d}'.format(i) for i in range(window/2+1)]                            # samples/2+1{RF}\n",
    "featnames += ['imFFT{:03d}'.format(i) for i in range(window/2+1)]                            # samples/2+1{IF}\n",
    "## Time Domain Golz feats\n",
    "featnames += ['meanv', 'stdr', 'mx', 'rngx', 'rngy', 'med', 'hjorth', 'sentr', 'se', 'ssk']  # 10\n",
    "featnames += ['acrol{:04d}'.format(i) for i in range(window)]                                # samples{acrol}\n",
    "## Frequency Domain Golz feats\n",
    "featnames += ['amFFT{:03d}'.format(i) for i in range(window/2+1)]                            # samples/2+1{AF}\n",
    "featnames += ['phFFT{:03d}'.format(i) for i in range(window/2+1)]                            # samples/2+1{PF}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Necessary  tmp/dataset.npz  already here!\n",
      "Necessary  tmp/validation.mat  already here!\n"
     ]
    }
   ],
   "source": [
    "########################################## Download necessary files ################################################\n",
    "# if download==1:\n",
    "#     downpath = featpath\n",
    "#     downfile = downpath+\"features.tar.gz\"\n",
    "#     downlink = \"https://www.dropbox.com/s/lx3ggezzq2wl2km/features.tar.gz?dl=1\"\n",
    "#     if not os.path.isfile(downfile):\n",
    "#         u = urllib.urlopen(downlink)\n",
    "#         data = u.read()\n",
    "#         print 'Completed downloading ',len(data)*1./(1024**3),'GB of ',downfile,'!'\n",
    "#         u.close()\n",
    "#         with open(downfile, \"wb\") as f :\n",
    "#             f.write(data)\n",
    "#     else:\n",
    "#         print 'Necessary ',downfile,'  already here!'\n",
    "#     print 'Extracting files...'\n",
    "#     directory = featpath\n",
    "#     def extract_nonexisting(archive):\n",
    "#         for name in archive.getnames():\n",
    "#             if os.path.exists(os.path.join(directory, name)):\n",
    "#                 print name, \"already exists\"\n",
    "#             else:\n",
    "#                 archive.extract(name, path=directory)\n",
    "#     archives = [name for name in os.listdir(directory) if name.endswith(\"tar.gz\")]\n",
    "#     for archive_name in archives:\n",
    "#         with tarfile.open(featpath+archive_name) as archive:\n",
    "#             extract_nonexisting(archive)\n",
    "# #     if (downfile.endswith(\"tar.gz\")):\n",
    "# #         tar = tarfile.open(downfile, \"r:gz\")\n",
    "# #         tar.extractall(path=downpath)\n",
    "# #         tar.close()\n",
    "#     print 'Completed extracting files!'\n",
    "########################################## Download necessary dataset #############################################\n",
    "if not os.path.isfile(datafile):\n",
    "#     downdata = \"https://www.dropbox.com/s/95znajbu6sga8iz/slipdataset3_C.mat?dl=1\"\n",
    "#     u = urllib.urlopen(downdata)\n",
    "#     data = u.read()\n",
    "#     print 'Completed downloading ',len(data)*1./(1024**2),'MB of ',datafile,'!'\n",
    "#     u.close()\n",
    "#     with open(datafile, \"wb\") as f :\n",
    "#         f.write(data)\n",
    "    print 'Necessary ',datafile,' not here! DO SOMETHING ABOUT IT!'\n",
    "else:\n",
    "    print 'Necessary ',datafile,' already here!'\n",
    "if not os.path.isfile(validfile):\n",
    "#     downdata = \"https://www.dropbox.com/s/95znajbu6sga8iz/slipdataset3_C.mat?dl=1\"\n",
    "#     u = urllib.urlopen(downdata)\n",
    "#     data = u.read()\n",
    "#     print 'Completed downloading ',len(data)*1./(1024**2),'MB of ',datafile,'!'\n",
    "#     u.close()\n",
    "#     with open(datafile, \"wb\") as f :\n",
    "#         f.write(data)\n",
    "    print 'Necessary ',validfile,' not here! DO SOMETHING ABOUT IT!'\n",
    "else:\n",
    "    print 'Necessary ',validfile,' already here!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "########################################## READ THE DATASET #######################################################\n",
    "# ################ DATASET, from each of the k fingers for all n surfaces (see fd for details) #############\n",
    "def data_prep(datafile,step=1,k=2):\n",
    "    print \"----------------------------- LOADING DATA and COMPUTING NECESSARY STRUCTS -----------------------------\"\n",
    "    if datafile[-3:]=='mat':\n",
    "        inp = sio.loadmat(datafile,struct_as_record=True)\n",
    "    elif datafile[-3:]=='npz':\n",
    "        inp = np.load(datafile)\n",
    "    else:\n",
    "        print \"Unsupported input file format. Supported types: .npz .mat\"\n",
    "        return -1\n",
    "    if k==2:\n",
    "        f1, f2, l1, l2, fd1, fd2 = inp['f1'], inp['f2'], inp['l1'], inp['l2'], inp['fd1'], inp['fd2']\n",
    "        print 1, '-> f1:', f1.shape, l1.shape, fd1.shape\n",
    "        print 2, '-> f2:', f2.shape, l2.shape, fd2.shape\n",
    "        ########################################### MERGE THE DATASETS ############################################\n",
    "        f = np.concatenate((f1,f2),axis=0)\n",
    "        l = np.concatenate((l1,l2),axis=0)\n",
    "        fd = np.concatenate((fd2,fd2),axis=0)\n",
    "    elif k==1:\n",
    "        f, l, fd = inp['f'], inp['l'], inp['fd']\n",
    "    else:\n",
    "        print \"Unsupported number of fingers k. Should be k in {1,2}\"\n",
    "    print 3, '-> f:', f.shape, l.shape, fd.shape\n",
    "    # membership of each sample, representing its portion in the dataset (first half finger1 and second half finger2)\n",
    "    member = np.zeros(len(f))\n",
    "    m1,m2 = len(f)/2, len(f)/2\n",
    "    member[:m1] = np.ones(m1)*1./m1\n",
    "    member[-m2:] = np.ones(m2)*1./m2\n",
    "    print 4, '-> m1,m2:', m1, m2, sum(member[:m1]), sum(member[-m2:])\n",
    "    ########################################### MERGE f and l #####################################################\n",
    "    while f.ndim>1:\n",
    "        f = f[:,0]\n",
    "        l = l[:,0]\n",
    "    for i in range(len(f)):\n",
    "        while l[i].ndim<2:\n",
    "            l[i] = l[i][:,np.newaxis]\n",
    "    f = np.array([np.concatenate((f[i],l[i]),axis=1) for i in range(len(f))])\n",
    "    print 5, '-> f=f+l:', f.shape, \":\", [fi.shape for fi in f]\n",
    "    ########################################### SUBSAMPLING #######################################################\n",
    "    # step = 1 # NO SAMPLING\n",
    "    if step!=1:\n",
    "        f = np.array([fi[::step,:] for fi in f])\n",
    "        print 6, '-> fsampled:',f.shape, \":\", [fi.shape for fi in f]\n",
    "    return f,l,fd,member,m1,m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################################################################################\n",
    "def make_pipe_clf(scaler,feature_selection,decomp,clf):\n",
    "    # first normalize, then perform feature selection, followed by PCA and finally the desired classifier.\n",
    "    pipeline = Pipeline([('scaler', scaler),\n",
    "                         ('feature_selection', feature_selection),\n",
    "                         ('decomp', decomp),         \n",
    "                         ('classifier', clf) ])\n",
    "    return pipeline\n",
    "###########################################################################################\n",
    "def make_pipe(scaler,feature_selection,decomp,order):\n",
    "    # first normalize, then perform feature selection, followed by PCA. \n",
    "    pipeline = Pipeline([('scaler', scaler),\n",
    "                         ('feature_selection', feature_selection),\n",
    "                         ('decomp', decomp) ])\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gs_fun(clf,params,cv,x,y):\n",
    "    grid_search = GridSearchCV(estimator=clf,param_grid= params, cv = cv, n_jobs=-1, verbose = 0)\n",
    "    grid_search.fit(x,y)\n",
    "    print(\"------ Grid search cv results for %0.8s ------\" %clf)\n",
    "    print(\"Best score: %0.4f\" %grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(params.keys()):\n",
    "         print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    return best_parameters, grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "########################################### PRE-FEATURES ###########################################################\n",
    "############# DEFINITION\n",
    "# featnum 0 : sf    = (fx^2+fy^2+fz^2)^0.5\n",
    "#         1 : ft    = (fx^2+fy^2)^0.5\n",
    "#         2 : fn    = |fz|\n",
    "#         3 : ft/fn = (fx^2+fy^2)^0.5/|fz|\n",
    "# input (nxm) -> keep (nx3) -> compute pre-feature and return (nx1)\n",
    "\n",
    "def sf(f):\n",
    "    return np.power(np.sum(np.power(f[:,:3],2),axis=1),0.5)\n",
    "def ft(f):\n",
    "    return np.power(np.sum(np.power(f[:,:2],2),axis=1),0.5)\n",
    "def fn(f):\n",
    "    return np.abs(f[:,2])\n",
    "def ftn(f):\n",
    "    retft = ft(f)\n",
    "    retfn = fn(f)\n",
    "    retft[retfn<=1e-2] = 0\n",
    "    return np.divide(retft,retfn+np.finfo(float).eps)\n",
    "def lab(f):\n",
    "    return np.abs(f[:,-1])\n",
    "############# PREFEATURE COMPUTATION\n",
    "def compute_prefeat(f):\n",
    "    print \"---------------------------------------- COMPUTING PREFEATURES -----------------------------------------\"\n",
    "    prefeatfn = np.array([sf,ft,fn,ftn,lab]) # convert to np.array to be easily indexed by a list\n",
    "    prefeat = np.array([np.array([prfn(f[i]) for prfn in prefeatfn[prefeatid]]).transpose() for i in range(len(f))])\n",
    "    print prefeat.shape,\":\",[p.shape for p in prefeat]\n",
    "    return prefeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# AVG Computation time of ALL features in secs ########################################\n",
    "def avg_feat_comp_time(prefeat):\n",
    "    print \"------------------------------------- AVG FEATURE COMPUTATION TIME -------------------------------------\"\n",
    "    t1 = time.time()\n",
    "    tmpfeat = [feat(prefeat[k][i:i+window,:2],*featparam) for k in range(20) for i in range(100)] # avg over 20*100 times\n",
    "    print 'Avg feature computation time (millisec): ', (time.time()-t1)/(100*20)*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################################# FEATURE COMPUTATION ##################################################\n",
    "def tmpfeatfilename(p,name,mode='all'):\n",
    "    allfeatpath = featpath+'AllFeatures/'\n",
    "    ensure_dir(allfeatpath)\n",
    "    if mode == 'all':\n",
    "        return allfeatpath+name+str(p)+'.pkl.z'\n",
    "    elif mode == 'red':\n",
    "        return allfeatpath+name+str(p)+'_red'+str(samplesperdataset)+'.pkl.z'\n",
    "    \n",
    "# Computation of all features in parallel or loading if already computed\n",
    "def feature_extraction(prefeat, member, featfile=featfile, name='feat_'):\n",
    "    print \"----------------------------------------- FEATURE EXTRACTION -------------------------------------------\"\n",
    "    if os.path.isfile(featfile):\n",
    "        start_time = time.time()\n",
    "        features = np.load(featfile)['features']\n",
    "        labels = np.load(featfile)['labels']\n",
    "        print(\"Features FOUND PRECOMPUTED! Feature Loading DONE in: %s seconds \" % (time.time() - start_time))\n",
    "    else:\n",
    "        start_time = time.time()\n",
    "        features = []\n",
    "        labels = []\n",
    "        for ixp in range(len(prefeat)):\n",
    "            p = prefeat[ixp]\n",
    "            now = time.time()\n",
    "            tmpfn = tmpfeatfilename(ixp,name)\n",
    "            tmpfnred = tmpfeatfilename(ixp,name,'red')\n",
    "            if not os.path.isfile(tmpfnred):\n",
    "                if not os.path.isfile(tmpfn):\n",
    "                    # Computation of all features in PARALLEL by ALL cores\n",
    "                    tmp = np.array([Parallel(n_jobs=-1)([delayed(feat) (p[k:k+window],*featparam) for k in range(0,len(p)-window,shift)])])\n",
    "                    with open(tmpfn,'wb') as fo:\n",
    "                        joblib.dump(tmp,fo)\n",
    "                    print 'sample:',ixp, ', time(sec):', '{:.2f}'.format(time.time()-now), tmpfn, ' computing... ', tmp.shape\n",
    "                else:\n",
    "                    with open(tmpfn,'rb') as fo:\n",
    "                        tmp = joblib.load(fo)\n",
    "                    print 'sample:',ixp, ', time(sec):', '{:.2f}'.format(time.time()-now), tmpfn, ' already here!', tmp.shape\n",
    "                # keep less from each feature vector but keep number of samples for each dataset almost equal\n",
    "                tmpskip = int(round(tmp.shape[1]/(member[ixp]*samplesperdataset)))\n",
    "                if tmpskip == 0: \n",
    "                    tmpskip = 1\n",
    "                # Save reduced size features\n",
    "                tmp = tmp[0,::tmpskip,:,:]\n",
    "                with open(tmpfnred,'wb') as fo:\n",
    "                    joblib.dump(tmp,fo)\n",
    "                print 'sample:',ixp, ', time(sec):', '{:.2f}'.format(time.time()-now), tmpfnred, tmp.shape\n",
    "        for ixp in range(len(prefeat)):  \n",
    "            tmpfnred = tmpfeatfilename(ixp,name,'red')\n",
    "            with open(tmpfnred,'rb') as fo:\n",
    "                tmp = joblib.load(fo)\n",
    "            print 'sample:',ixp, ', time(sec):', '{:.2f}'.format(time.time()-now), tmpfnred, 'already here!', tmp.shape\n",
    "            features.append(tmp[:,:,:-1])\n",
    "            labels.append(tmp[:,0,-1])\n",
    "        print(\"Features NOT FOUND PRECOMPUTED! Feature Computation DONE in: %s seconds \" % (time.time() - start_time))\n",
    "        features = np.array(features)\n",
    "        labels = np.array(labels)\n",
    "        print 'features: ',features.shape,[ftmp.shape for ftmp in features]\n",
    "        print 'labels: ', labels.shape,[l.shape for l in labels]\n",
    "        np.savez(featfile,features=features,labels=labels)\n",
    "    print 'features: ', features.shape, ', labels: ', labels.shape\n",
    "#     for i in range(features.shape[0]):\n",
    "#         print i, np.array(features[i]).shape, np.array(labels[i]).shape\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###### Keeping the purely stable and slip parts of label, thus omitting some samples around sign change points \n",
    "def label_cleaning(prefeat,labels,member,history=500):\n",
    "    print \"------------ KEEPING LABEL's PURE (STABLE, SLIP) PHASE PARTS (TRIMMING AROUND CHANGE POINTS)------------\"\n",
    "    lbl_approx = []\n",
    "    for i in range(len(prefeat)):\n",
    "        tmpd = np.abs(np.diff(prefeat[i][:,-1].astype(int),n=1,axis=0))\n",
    "        if np.sum(tmpd) > 0:\n",
    "            tmpind = np.array(range(len(tmpd)))[tmpd > 0]   # find the sign change points\n",
    "            tmpindrng = []\n",
    "            for j in range(len(tmpind)):\n",
    "                length = history                # keep/throw a portion of the signal's length around change points\n",
    "                tmprng = np.array(range(tmpind[j]-length,tmpind[j]+length))\n",
    "                tmprng = tmprng[tmprng>=0]      # make sure inside singal's x-range\n",
    "                tmprng = tmprng[tmprng<prefeat[i].shape[0]]\n",
    "                tmpindrng += tmprng.tolist()\n",
    "            tmpindrng = np.array(tmpindrng).flatten()\n",
    "            tmp_lbl = deepcopy(prefeat[i][:,-1])\n",
    "            tmp_lbl[tmpindrng] = -1\n",
    "            lbl_approx.append(tmp_lbl)\n",
    "        else:\n",
    "            lbl_approx.append(prefeat[i][:,-1])\n",
    "    # for i in range(len(lbl_approx)):\n",
    "    #     print i, lbl_approx[i].shape, prefeat[i].shape\n",
    "\n",
    "    new_labels = deepcopy(labels)\n",
    "    for ixp in range(len(lbl_approx)):\n",
    "        p = lbl_approx[ixp]\n",
    "        tmp = np.array([p[k+window] for k in range(0,len(p)-window,shift)])\n",
    "        tmpskip = int(round(tmp.shape[0]/(member[ixp]*samplesperdataset)))\n",
    "        if tmpskip == 0: \n",
    "            tmpskip = 1\n",
    "        # Sampling appropriately\n",
    "        tmp = tmp[::tmpskip]\n",
    "        if len(tmp) > len(labels[ixp]):\n",
    "            tmp = tmp[:-1]\n",
    "        new_labels[ixp] = tmp\n",
    "    print 'new_labels: ', new_labels.shape\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## GATHERING into complete arrays ready for FITTING #######################################\n",
    "def computeXY(features,labels,new_labels,m1,m2,XYfile=XYfile,XYsplitfile=XYsplitfile):\n",
    "    print \"------------------------------ COMPUTING X,Y for CLASSIFIERS' INPUT ------------------------------------\"\n",
    "    if os.path.isfile(XYfile) and os.path.isfile(XYsplitfile):\n",
    "        X = np.load(XYfile)['X']\n",
    "        Y = np.load(XYfile)['Y']\n",
    "        Yn = np.load(XYfile)['Yn']\n",
    "        Xsp = np.load(XYsplitfile)['X']\n",
    "        Ysp = np.load(XYsplitfile)['Y']\n",
    "        print(\"XY files FOUND PRECOMPUTED!\")\n",
    "    else:\n",
    "        # gathering features X,Xsp and labels Y,Ysp,Yn into one array each\n",
    "        ind,X,Xsp,Y,Ysp,Yn = {},{},{},{},{},{}\n",
    "        ind[2] = range(features.shape[0])                                      # indeces for both fingers\n",
    "        ind[0] = range(features.shape[0])[:m1]                                 # indeces for finger1\n",
    "        ind[1] = range(features.shape[0])[-m2:]                                # indeces for finger2\n",
    "        ind = np.array([i for _,i in ind.items()])                             # convert to array\n",
    "        for k in range(len(ind)):\n",
    "            X[k] = features[ind[k]]                                            # input feature matrix\n",
    "            Y[k] = labels[ind[k]]                                              # output label vector\n",
    "            Yn[k] = new_labels[ind[k]]                                         # output new_label vector\n",
    "            print 'Before -> X[',k,']: ',X[k].shape,', Y[',k,']: ',Y[k].shape,', Yn[',k,']: ',Yn[k].shape\n",
    "            X[k] = np.concatenate(X[k],axis=0)\n",
    "            Y[k] = np.concatenate(Y[k],axis=0)\n",
    "            Yn[k] = np.concatenate(Yn[k],axis=0)\n",
    "            print 'Gathered -> X[',k,']: ',X[k].shape,', Y[',k,']: ',Y[k].shape,', Yn[',k,']: ',Yn[k].shape\n",
    "            X[k] = np.array([X[k][:,:,i] for i in range(X[k].shape[2])])\n",
    "            tmp_sampling = int(round(X[k].shape[1]*1./samplesperdataset))\n",
    "            X[k] = X[k][0,::tmp_sampling,:]\n",
    "            Y[k] = Y[k][::tmp_sampling]\n",
    "            Yn[k] = Yn[k][::tmp_sampling]\n",
    "            print 'Gathered, sampled to max ',samplesperdataset,' -> X[',k,']: ',X[k].shape,', Y[',k,']: ',Y[k].shape,', Yn[',k,']: ',Yn[k].shape\n",
    "            keepind = Yn[k]>=0\n",
    "            Xsp[k] = X[k][keepind,:]\n",
    "            Ysp[k] = Yn[k][keepind]\n",
    "            print 'Split -> Xsp[',k,']: ',Xsp[k].shape,', Ysp[',k,']: ',Ysp[k].shape\n",
    "        X = np.array([i for _,i in X.items()])\n",
    "        Xsp = np.array([i for _,i in Xsp.items()])\n",
    "        Y = np.array([i for _,i in Y.items()])\n",
    "        Ysp = np.array([i for _,i in Ysp.items()])\n",
    "        Yn = np.array([i for _,i in Yn.items()])\n",
    "        np.savez(XYfile,X=X,Y=Y,Yn=Yn)\n",
    "        np.savez(XYsplitfile, X=Xsp, Y=Ysp)\n",
    "    print 'X,Y [0,1,2]: ', X[0].shape, Y[0].shape, X[1].shape, Y[1].shape, X[2].shape, Y[2].shape\n",
    "    print 'Xsp,Ysp [0,1,2]: ', Xsp[0].shape, Ysp[0].shape, Xsp[1].shape, Ysp[1].shape, Xsp[2].shape, Ysp[2].shape\n",
    "    return X,Y,Yn,Xsp,Ysp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find the corresponding indeces of the desired features inside feature vector,\n",
    "# and link them with their names and level of abstraction\n",
    "def get_feat_id(feat_ind, printit=0, sample_window=window): \n",
    "    # get the feat inds wrt their source : 3rd level\n",
    "    norm_time_phin = range(0,14)\n",
    "    norm_freq_phin = range(norm_time_phin[-1] + 1, norm_time_phin[-1] + 9 + sample_window + 1)\n",
    "    norm_time_golz = range(norm_freq_phin[-1] + 1, norm_freq_phin[-1] + 10 + sample_window + 1)\n",
    "    norm_freq_golz = range(norm_time_golz[-1] + 1, norm_time_golz[-1] + 2 + sample_window + 1)\n",
    "    # get the feat inds wrt their domain : 2nd level \n",
    "    norm_time_feats = norm_time_phin + norm_time_golz\n",
    "    norm_freq_feats = norm_freq_phin + norm_freq_golz\n",
    "    # get the feat inds wrt their prefeat: 1st level \n",
    "    norm_feats = norm_time_feats + norm_freq_feats\n",
    "\n",
    "    # get the feat inds wrt their source : 3rd level\n",
    "    disp = norm_feats[-1]+1\n",
    "    ftfn_time_phin = range(disp ,disp + 14)\n",
    "    ftfn_freq_phin = range(ftfn_time_phin[-1] + 1, ftfn_time_phin[-1] + 9 + sample_window + 1)\n",
    "    ftfn_time_golz = range(ftfn_freq_phin[-1] + 1, ftfn_freq_phin[-1] + 10 + sample_window + 1)\n",
    "    ftfn_freq_golz = range(ftfn_time_golz[-1] + 1, ftfn_time_golz[-1] + 2 + sample_window + 1)\n",
    "    # get the feat inds wrt their domain : 2nd level \n",
    "    ftfn_time_feats = ftfn_time_phin + ftfn_time_golz\n",
    "    ftfn_freq_feats = ftfn_freq_phin + ftfn_freq_golz\n",
    "    # get the feat inds wrt their prefeat: 1st level \n",
    "    ftfn_feats = ftfn_time_feats + ftfn_freq_feats\n",
    "\n",
    "    # create the final \"reference dictionary\"\n",
    "    id_list = [np.zeros((len(ftfn_feats + norm_feats),1)) for i in range(3)] #3 np.arrays, id_list[0] = level 1 etc\n",
    "    id_list[0][:norm_feats[-1]+1] = 0 # 0 signifies norm / 1 signifies ft/fn\n",
    "    id_list[0][norm_feats[-1]+1:] = 1\n",
    "\n",
    "    id_list[1][:norm_time_phin[-1]+1] = 0 #0 signifies time / 1 signifies freq\n",
    "    id_list[1][norm_time_phin[-1]+1:norm_freq_phin[-1]+1] = 1\n",
    "    id_list[1][norm_freq_phin[-1]+1:norm_time_golz[-1]+1] = 0\n",
    "    id_list[1][norm_time_golz[-1]+1:norm_freq_golz[-1]+1] = 1\n",
    "    id_list[1][norm_freq_golz[-1]+1:ftfn_time_phin[-1]+1] = 0\n",
    "    id_list[1][ftfn_time_phin[-1]+1:ftfn_freq_phin[-1]+1] = 1\n",
    "    id_list[1][ftfn_freq_phin[-1]+1:ftfn_time_golz[-1]+1] = 0\n",
    "    id_list[1][ftfn_time_golz[-1]+1:] = 1\n",
    "\n",
    "    id_list[2][:norm_freq_phin[-1]+1] = 0 #0 signifies phinyomark / 1 signifies golz\n",
    "    id_list[2][norm_freq_phin[-1]+1:norm_freq_golz[-1]+1] = 1\n",
    "    id_list[2][norm_freq_golz[-1]+1:ftfn_freq_phin[-1]+1] = 0\n",
    "    id_list[2][ftfn_freq_phin[-1]+1:] = 1 \n",
    "    \n",
    "    full_path_id = [np.zeros((len(feat_ind),5)) for i in range(len(feat_ind))]\n",
    "   \n",
    "    for ind, val in enumerate(feat_ind):\n",
    "        full_path_id[ind] = [val, id_list[2][val], id_list[1][val], id_list[0][val]]\n",
    "        if (printit==1):\n",
    "            if(full_path_id[ind][1]==0):\n",
    "                lvl3 = 'Phin'\n",
    "            else:\n",
    "                lvl3 = 'Golz'\n",
    "            if(full_path_id[ind][2]==0):\n",
    "                lvl2 = 'Time'\n",
    "            else:\n",
    "                lvl2 = 'Freq'\n",
    "            if(full_path_id[ind][3]==0):\n",
    "                lvl1 = 'Norm'\n",
    "            else:\n",
    "                lvl1 = 'Ft/Fn'\n",
    "            print(feat_ind[ind],featnames[val%(norm_feats[-1]+1)],lvl3,lvl2,lvl1)\n",
    "    \n",
    "    return(full_path_id,norm_time_feats,norm_freq_feats)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feat_occ(feat_masks):\n",
    "    #get the number of occurences for each feature after SelectKbest\n",
    "    feat_occ = Counter(feat_masks)\n",
    "    return feat_occ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split input data in k*n equal slices which represent n different surfaces sampled from k fingers\n",
    "def surface_split(data_X, data_Y, n=6, k=2):\n",
    "    # n different surfaces, with the convention that data_X contains k*n almost equally sized data\n",
    "    # where the n first are acquired from finger1 ... and the n last from fingerk. \n",
    "    # step of n: 0 upto (k-1)*n, 1 upto (k-1)*n+1, 2 upto (k-1)*n+2, ... correspond to the same surface (finger1 upto fingerk)\n",
    "    # assuming k=2, namely 2 fingers case, unless stated differently\n",
    "    keep = data_X.shape[0]-np.mod(data_X.shape[0],k*n)\n",
    "    surfaces_pre = np.array(np.split(data_X[:keep,:],k*n))\n",
    "    surf_labels_pre = np.array(np.split(data_Y[:keep],k*n))\n",
    "    surfaces, surf_labels = {},{}\n",
    "    for i in range(n):\n",
    "        inds = range(i,k*n,n)\n",
    "        surfaces[inds[0]] = np.concatenate((surfaces_pre[inds[0]], surfaces_pre[inds[1]]), axis = 0)\n",
    "        surf_labels[inds[0]] = np.concatenate((surf_labels_pre[inds[0]], surf_labels_pre[inds[1]]), axis = 0)\n",
    "    surfaces = np.array([i for _,i in surfaces.items()])\n",
    "    surf_labels = np.array([i for _,i in surf_labels.items()])\n",
    "    return surfaces, surf_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feat_subsets(data,fs_ind,ofs=len(featnames)):\n",
    "    _,tf,ff = get_feat_id(range(ofs))\n",
    "    amfft_inds = []\n",
    "    temp1 = deepcopy(data)\n",
    "    \n",
    "    for i in range(len(featnames)):\n",
    "        if (featnames[i].startswith('amFFT')):\n",
    "            amfft_inds.append(i)\n",
    "\n",
    "    if (fs_ind == 2):\n",
    "        ff2 = [ff[i]+ofs for i in range(len(ff))]\n",
    "        tf2 = [tf[i]+ofs for i in range(len(tf))]\n",
    "        amfft2 = [amfft_inds[i]+ofs for i in range(len(amfft_inds))]\n",
    "        freqf = ff2 + ff\n",
    "        timef = tf2 + tf\n",
    "        amfft = amfft_inds + amfft2\n",
    "    else:\n",
    "        freqf = ff\n",
    "        timef = tf\n",
    "        amfft = amfft_inds\n",
    "\n",
    "    X_amfft = temp1[:,amfft]\n",
    "    X_time = np.delete(temp1,freqf,axis=1)\n",
    "    X_freq_all = np.delete(temp1,timef,axis=1)\n",
    "    X_both = data\n",
    "    return X_amfft, X_freq_all, X_time, X_both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## Prepare the dataset split for each surface ######################################\n",
    "def computeXY_persurf(Xsp,Ysp,surffile=surffile):\n",
    "    print \"------------------------- COMPUTING X,Y per surface CLASSIFIERS' INPUT ---------------------------------\"\n",
    "    if os.path.isfile(surffile):\n",
    "        surf = np.load(surffile)['surf']       # input array containing computed features for each surface\n",
    "        surfla = np.load(surffile)['surfla']   # corresponding label\n",
    "    else:\n",
    "        surf, surfla = [], []\n",
    "        for i in range(len(prefeatid)-1): # for each featureset (corresponding to each prefeature, here only |f|)\n",
    "            surf1, surfla1 = surface_split(Xsp[2], Ysp[2])\n",
    "            tmpsurf = deepcopy(surf1)\n",
    "            tmpsurfla = deepcopy(surfla1)\n",
    "            tmpsurfsubfeat = []\n",
    "            for j in range(tmpsurf.shape[0]+1): # for each surface\n",
    "                print i,j,surf1.shape\n",
    "                if j == tmpsurf.shape[0]:\n",
    "                    tmpsurfsubfeat.append(feat_subsets(tmpsurf[j-1,:-1,:],i)) # ommit a sample for converting to array\n",
    "                else:\n",
    "                    tmpsurfsubfeat.append(feat_subsets(tmpsurf[j],i)) # keep all subfeaturesets\n",
    "            surf.append(tmpsurfsubfeat)\n",
    "            surfla.append(surfla1)\n",
    "        # surf dims: (featuresets, surfaces, prefeaturesets) with each one enclosing (samples, features)\n",
    "        surf = np.array(surf).transpose()[:,:-1,:]\n",
    "        # surfla dims: (samples, surfaces, prefeaturesets)\n",
    "        surfla = np.array(surfla).transpose()\n",
    "        np.savez(surffile,surf=surf,surfla=surfla)\n",
    "    print surf.shape, surfla.shape\n",
    "    return surf, surfla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######## Cross surface validation, TRAINING with 1 surface each time, out of 6 surfaces in total\n",
    "##### total: 4 (featuresets) * [comb(6,1)*6] (surface combinations: trained on 1 - tested on 1) * 1 (prefeatureset)\n",
    "##### = 4*6*6*1 = 144 different runs-files.\n",
    "##### note that comb(n,r) = n!/(r!(n-r)!)\n",
    "def filename1(i,j,k,l):\n",
    "    return 'fs_'+str(i)+'_subfs_'+str(j)+'_tr_'+str(k)+'_ts_'+str(l)\n",
    "\n",
    "def cross_fit1(i,j,k,kmax,l,data,labels,data2,labels2,pipe):\n",
    "    filepath = datapath+'/results1/'\n",
    "    ensure_dir(filepath)\n",
    "    fileid = filepath+filename1(i,j,k,l)+'.npz'\n",
    "    if not os.path.isfile(fileid):\n",
    "        print i,j,k,l\n",
    "        if k==l: # perform K-fold cross-validation       \n",
    "            folds = cv.split(data, labels)\n",
    "            cm_all = np.zeros((2,2))\n",
    "            for fold, (train_ind, test_ind) in enumerate(folds):\n",
    "                x_train, x_test = data[train_ind], data[test_ind]\n",
    "                y_train, y_test = labels[train_ind], labels[test_ind]\n",
    "                model = pipe.fit(x_train,y_train)\n",
    "                y_pred = model.predict(x_test)\n",
    "                cm = confusion_matrix(y_pred=y_pred, y_true=y_test)\n",
    "                cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "                cm_all += cm/5.\n",
    "            np.savez(fileid,cm=cm_all,model=np.array([model]))\n",
    "        else: # perform cross-check\n",
    "            tr_data = data\n",
    "            tr_labels = labels\n",
    "            ts_data = data2\n",
    "            ts_labels = labels2\n",
    "            # Check if model already existent, but not the cross-validated one (on the same surface)\n",
    "            model = []\n",
    "            for m in range(kmax):\n",
    "                tmpcopyfileid = filepath+filename1(i,j,k,m)+'.npz'\n",
    "                if k!=m and os.path.isfile(tmpcopyfileid):\n",
    "                    print 'Found precomputed model of '+str(k)+', tested on '+str(m)+'. Testing on '+str(l)+'...'\n",
    "                    model = np.load(tmpcopyfileid)['model'][0]\n",
    "                    break\n",
    "            if model==[]: # model not found precomputed\n",
    "                print 'Fitting on '+str(k)+', testing on '+str(l)+'...'\n",
    "                model = pipe.fit(tr_data,tr_labels)\n",
    "            y_pred = model.predict(ts_data)\n",
    "            cm = confusion_matrix(y_pred=y_pred, y_true=ts_labels)\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            np.savez(fileid,cm=cm,model=np.array([model]))\n",
    "\n",
    "def init_steps1(i,j,jmax,surf,surfla):\n",
    "    if j==jmax:\n",
    "        featsel = SelectKBest(k=1000,score_func= mutual_info_classif)\n",
    "    else:\n",
    "        featsel = SelectKBest(k='all',score_func= mutual_info_classif)\n",
    "    pipe = make_pipe_clf(scaler, featsel, decomp, classifiers[2])\n",
    "    for k in range(surf.shape[0]): # for every training surface\n",
    "        for l in range(surf.shape[0]): # for every testing surface\n",
    "            cross_fit1(i,j,k,surf.shape[0],l,surf[k],surfla[:,k],surf[l],surfla[:,l],pipe)\n",
    "\n",
    "def train_1_surface(surf,surfla,n=-1):\n",
    "    print \"--------------------------- TRAINING all combinations per 1 surface ------------------------------------\"\n",
    "    ##### Parallelized -on surface level- training of models with 1 surface\n",
    "    for i in range(len(prefeatid)-1):\n",
    "        _ = [Parallel(n_jobs=n)([delayed(init_steps1) (i,j,surf.shape[0]-1,surf[j,:,i],surfla[:,:,i]) \n",
    "                                  for j in range(surf.shape[0])])]            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Cross surface validation, training with 2 surfaces each time, out of 6 surfaces in total\n",
    "##### total: 4 (featuresets) * [comb(6,2)*6] (surface combinations: trained on 2 - tested on 1) * 1 (prefeatureset)\n",
    "##### = 4*15*6*1 = 360 different runs-files.\n",
    "##### note that comb(n,r) = n!/(r!(n-r)!)\n",
    "def filename2(i,j,k1,k2,l):\n",
    "    return 'fs_'+str(i)+'_subfs_'+str(j)+'_tr1_'+str(k1)+'_tr2_'+str(k2)+'_ts_'+str(l)\n",
    "\n",
    "def cross_fit2(i,j,k1,k2,kmax,l,data,labels,data2,labels2,pipe):\n",
    "    filepath = datapath+'/results2/'\n",
    "    ensure_dir(filepath)\n",
    "    fileid = filepath+filename2(i,j,k1,k2,l)+'.npz'\n",
    "    if not os.path.isfile(fileid):\n",
    "        print i,j,k1,k2,l\n",
    "        if k1==l or k2==l: # perform K-fold      \n",
    "            print 'Fitting on '+str(k1)+\"-\"+str(k2)+', cross-validating on '+str(l)+'...'\n",
    "            if l == k1: # copy if existent from the other sibling file\n",
    "                tmpcopyfileid = filepath+filename2(i,j,k1,k2,k2)+'.npz'\n",
    "            else:   # same as above\n",
    "                tmpcopyfileid = filepath+filename2(i,j,k1,k2,k1)+'.npz'                \n",
    "            if not os.path.isfile(tmpcopyfileid):\n",
    "                folds = cv.split(data, labels)\n",
    "                cm_all = np.zeros((2,2))\n",
    "                for fold, (train_ind, test_ind) in enumerate(folds):\n",
    "                    x_train, x_test = data[train_ind], data[test_ind]\n",
    "                    y_train, y_test = labels[train_ind], labels[test_ind]\n",
    "                    model = pipe.fit(x_train,y_train)\n",
    "                    y_pred = model.predict(x_test)\n",
    "                    cm = confusion_matrix(y_pred=y_pred, y_true=y_test)\n",
    "                    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "                    cm_all += cm/5.\n",
    "            else:\n",
    "                cm_all = np.load(tmpcopyfileid)['cm']\n",
    "                model = np.load(tmpcopyfileid)['model'][0]\n",
    "            np.savez(fileid,cm=cm_all,model=np.array([model]))\n",
    "        else: # perform cross-check\n",
    "            tr_data = data\n",
    "            tr_labels = labels\n",
    "            ts_data = data2\n",
    "            ts_labels = labels2\n",
    "            model = []\n",
    "            for m in range(kmax):\n",
    "                tmpcopyfileid = filepath+filename2(i,j,k1,k2,m)+'.npz'\n",
    "                if k1!=m and k2!=m and os.path.isfile(tmpcopyfileid):\n",
    "                    print 'Found precomputed model of '+str(k1)+str(k2)+', tested on '+str(m)+'. Testing on '+str(l)+'...'\n",
    "                    model = np.load(tmpcopyfileid)['model'][0]\n",
    "                    break\n",
    "            if model==[]: # model not found precomputed\n",
    "                print 'Fitting on '+str(k1)+\"-\"+str(k2)+', testing on '+str(l)+'...'\n",
    "                model = pipe.fit(tr_data,tr_labels)\n",
    "            y_pred = model.predict(ts_data)\n",
    "            cm = confusion_matrix(y_pred=y_pred, y_true=ts_labels)\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            np.savez(fileid,cm=cm,model=np.array([model]))\n",
    "\n",
    "def init_steps2(i,j,jmax,surf,surfla):\n",
    "    if j==jmax:\n",
    "        featsel = SelectKBest(k=1000,score_func= mutual_info_classif)\n",
    "    else:\n",
    "        featsel = SelectKBest(k='all',score_func= mutual_info_classif)\n",
    "    pipe = make_pipe_clf(scaler, featsel, decomp, classifiers[2])\n",
    "    for k1 in range(surf.shape[0]): # for every training surface1\n",
    "        for k2 in range(surf.shape[0]): # for every training surface2\n",
    "            if k2 > k1:\n",
    "                for l in range(surf.shape[0]): # for every testing surface\n",
    "                    tr_surf, tr_surfla = np.concatenate((surf[k1],surf[k2]),axis=0), np.concatenate((surfla[:,k1],surfla[:,k2]),axis=0)\n",
    "                    ts_surf, ts_surfla = surf[l], surfla[:,l]\n",
    "                    cross_fit2(i,j,k1,k2,surf.shape[0],l,tr_surf,tr_surfla,ts_surf,ts_surfla,pipe)\n",
    "\n",
    "def train_2_surface(surf,surfla,n=-1):\n",
    "    print \"--------------------------- TRAINING all combinations per 2 surfaces -----------------------------------\"    \n",
    "    ##### Parallelized -on surface level- training of models with 2 surfaces\n",
    "    for i in range(len(prefeatid)-1):\n",
    "        _ = [Parallel(n_jobs=n)([delayed(init_steps2) (i,j,surf.shape[0]-1,surf[j,:,i],surfla[:,:,i]) \n",
    "                                 for j in range(surf.shape[0])])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######## Cross surface validation, training with 3 surfaces each time, out of 6 surfaces in total\n",
    "##### total: 4 (featuresets) * [comb(6,3)*6] (surface combinations: trained on 3 - tested on 1) * 1 (prefeatureset)\n",
    "##### = 4*20*6*1 = 480 different runs-files.\n",
    "##### note that comb(n,r) = n!/(r!(n-r)!)\n",
    "def filename3(i,j,k1,k2,k3,l):\n",
    "    return 'fs_'+str(i)+'_subfs_'+str(j)+'_tr1_'+str(k1)+'_tr2_'+str(k2)+'_tr3_'+str(k3)+'_ts_'+str(l)\n",
    "\n",
    "def cross_fit3(i,j,k1,k2,k3,kmax,l,data,labels,data2,labels2,pipe):\n",
    "    filepath = datapath+'/results3/'\n",
    "    ensure_dir(filepath)\n",
    "    fileid = filepath+filename3(i,j,k1,k2,k3,l)+'.npz'\n",
    "    if not os.path.isfile(fileid):\n",
    "        print i,j,k1,k2,k3,l\n",
    "        if k1==l or k2==l or k3==l: # perform K-fold      \n",
    "            print 'Fitting on '+str(k1)+\"-\"+str(k2)+\"-\"+str(k3)+', cross-validating on '+str(l)+'...'\n",
    "            if l == k1: # copy if existent from the other sibling file\n",
    "                tmpcopyfileid1 = filepath+filename3(i,j,k1,k2,k3,k2)+'.npz'\n",
    "                tmpcopyfileid2 = filepath+filename3(i,j,k1,k2,k3,k3)+'.npz'\n",
    "            elif l == k2:   # same as above\n",
    "                tmpcopyfileid1 = filepath+filename3(i,j,k1,k2,k3,k1)+'.npz'\n",
    "                tmpcopyfileid2 = filepath+filename3(i,j,k1,k2,k3,k3)+'.npz'\n",
    "            else:\n",
    "                tmpcopyfileid1 = filepath+filename3(i,j,k1,k2,k3,k1)+'.npz'\n",
    "                tmpcopyfileid2 = filepath+filename3(i,j,k1,k2,k3,k2)+'.npz'\n",
    "            if not os.path.isfile(tmpcopyfileid1) and not os.path.isfile(tmpcopyfileid2):\n",
    "                folds = cv.split(data, labels)\n",
    "                cm_all = np.zeros((2,2))\n",
    "                for fold, (train_ind, test_ind) in enumerate(folds):\n",
    "                    x_train, x_test = data[train_ind], data[test_ind]\n",
    "                    y_train, y_test = labels[train_ind], labels[test_ind]\n",
    "                    model = pipe.fit(x_train,y_train)\n",
    "                    y_pred = model.predict(x_test)\n",
    "                    cm = confusion_matrix(y_pred=y_pred, y_true=y_test)\n",
    "                    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "                    cm_all += cm/5.\n",
    "            else:\n",
    "                if os.path.isfile(tmpcopyfileid1):\n",
    "                    cm_all = np.load(tmpcopyfileid1)['cm']\n",
    "                    model = np.load(tmpcopyfileid1)['model'][0]\n",
    "                else:\n",
    "                    cm_all = np.load(tmpcopyfileid2)['cm']\n",
    "                    model = np.load(tmpcopyfileid2)['model'][0]\n",
    "            np.savez(fileid,cm=cm_all,model=np.array([model]))\n",
    "        else: # perform cross-check\n",
    "            tr_data = data\n",
    "            tr_labels = labels\n",
    "            ts_data = data2\n",
    "            ts_labels = labels2\n",
    "            model = []\n",
    "            for m in range(kmax):\n",
    "                tmpcopyfileid = filepath+filename3(i,j,k1,k2,k3,m)+'.npz'\n",
    "                if k1!=m and k2!=m and k3!=m and os.path.isfile(tmpcopyfileid):\n",
    "                    print 'Found precomputed model of '+str(k1)+str(k2)+str(k3)+', tested on '+str(m)+'. Testing on '+str(l)+'...'\n",
    "                    model = np.load(tmpcopyfileid)['model'][0]\n",
    "                    break\n",
    "            if model==[]: # model not found precomputed\n",
    "                print 'Fitting on '+str(k1)+\"-\"+str(k2)+\"-\"+str(k3)+', testing on '+str(l)+'...'\n",
    "                model = pipe.fit(tr_data,tr_labels)\n",
    "            y_pred = model.predict(ts_data)\n",
    "            cm = confusion_matrix(y_pred=y_pred, y_true=ts_labels)\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            np.savez(fileid,cm=cm,model=np.array([model]))\n",
    "\n",
    "def init_steps3(i,j,jmax,surf,surfla):\n",
    "    if j==jmax:\n",
    "        featsel = SelectKBest(k=1000,score_func= mutual_info_classif)\n",
    "    else:\n",
    "        featsel = SelectKBest(k='all',score_func= mutual_info_classif)\n",
    "    pipe = make_pipe_clf(scaler, featsel, decomp, classifiers[2])\n",
    "    for k1 in range(surf.shape[0]): # for every training surface1\n",
    "        for k2 in range(surf.shape[0]): # for every training surface2\n",
    "            if k2 > k1:\n",
    "                for k3 in range(surf.shape[0]):\n",
    "                    if k3 > k2:\n",
    "                        for l in range(surf.shape[0]): # for every testing surface\n",
    "                            tr_surf, tr_surfla = np.concatenate((surf[k1],surf[k2],surf[k3]),axis=0), np.concatenate((surfla[:,k1],surfla[:,k2],surfla[:,k3]),axis=0)\n",
    "                            ts_surf, ts_surfla = surf[l], surfla[:,l]\n",
    "                            cross_fit3(i,j,k1,k2,k3,surf.shape[0],l,tr_surf,tr_surfla,ts_surf,ts_surfla,pipe)\n",
    "\n",
    "def train_3_surface(surf,surfla,n=-1):\n",
    "    print \"--------------------------- TRAINING all combinations per 3 surfaces -----------------------------------\"\n",
    "    ##### Parallelized -on surface level- training of models with 3 surfaces\n",
    "    for i in range(len(prefeatid)-1):\n",
    "        _ = [Parallel(n_jobs=n)([delayed(init_steps3) (i,j,surf.shape[0]-1,surf[j,:,i],surfla[:,:,i]) \n",
    "                                 for j in range(surf.shape[0])])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######## Cross surface validation, training with 4 surfaces each time, out of 6 surfaces in total\n",
    "##### total: 4 (featuresets) * [comb(6,4)*6] (surface combinations: trained on 4 - tested on 1) * 1 (prefeatureset)\n",
    "##### = 4*15*6*1 = 360 different runs-files.\n",
    "##### note that comb(n,r) = n!/(r!(n-r)!)\n",
    "def filename4(i,j,k1,k2,k3,k4,l):\n",
    "    return 'fs_'+str(i)+'_subfs_'+str(j)+'_tr1_'+str(k1)+'_tr2_'+str(k2)+'_tr3_'+str(k3)+'_tr4_'+str(k4)+'_ts_'+str(l)\n",
    "\n",
    "def cross_fit4(i,j,k1,k2,k3,k4,kmax,l,data,labels,data2,labels2,pipe):\n",
    "    filepath = datapath+'/results4/'\n",
    "    ensure_dir(filepath)\n",
    "    fileid = filepath+filename4(i,j,k1,k2,k3,k4,l)+'.npz'\n",
    "    if not os.path.isfile(fileid):\n",
    "        print i,j,k1,k2,k3,k4,l\n",
    "        if k1==l or k2==l or k3==l or k4==l: # perform K-fold      \n",
    "            print 'Fitting on '+str(k1)+\"-\"+str(k2)+\"-\"+str(k3)+\"-\"+str(k4)+', cross-validating on '+str(l)+'...'\n",
    "            if l == k1: # copy if existent from the other sibling file\n",
    "                tmpcopyfileid1 = filepath+filename4(i,j,k1,k2,k3,k4,k2)+'.npz'\n",
    "                tmpcopyfileid2 = filepath+filename4(i,j,k1,k2,k3,k4,k3)+'.npz'\n",
    "                tmpcopyfileid3 = filepath+filename4(i,j,k1,k2,k3,k4,k4)+'.npz'\n",
    "            elif l == k2:   # same as above\n",
    "                tmpcopyfileid1 = filepath+filename4(i,j,k1,k2,k3,k4,k1)+'.npz'\n",
    "                tmpcopyfileid2 = filepath+filename4(i,j,k1,k2,k3,k4,k3)+'.npz'\n",
    "                tmpcopyfileid3 = filepath+filename4(i,j,k1,k2,k3,k4,k4)+'.npz'\n",
    "            elif l == k3:   # same as above\n",
    "                tmpcopyfileid1 = filepath+filename4(i,j,k1,k2,k3,k4,k1)+'.npz'\n",
    "                tmpcopyfileid2 = filepath+filename4(i,j,k1,k2,k3,k4,k2)+'.npz'\n",
    "                tmpcopyfileid3 = filepath+filename4(i,j,k1,k2,k3,k4,k4)+'.npz'\n",
    "            else:\n",
    "                tmpcopyfileid1 = filepath+filename4(i,j,k1,k2,k3,k4,k1)+'.npz'\n",
    "                tmpcopyfileid2 = filepath+filename4(i,j,k1,k2,k3,k4,k2)+'.npz'\n",
    "                tmpcopyfileid3 = filepath+filename4(i,j,k1,k2,k3,k4,k3)+'.npz'\n",
    "            if not os.path.isfile(tmpcopyfileid1) and not os.path.isfile(tmpcopyfileid2) and not os.path.isfile(tmpcopyfileid3):\n",
    "                folds = cv.split(data, labels)\n",
    "                cm_all = np.zeros((2,2))\n",
    "                for fold, (train_ind, test_ind) in enumerate(folds):\n",
    "                    x_train, x_test = data[train_ind], data[test_ind]\n",
    "                    y_train, y_test = labels[train_ind], labels[test_ind]\n",
    "                    model = pipe.fit(x_train,y_train)\n",
    "                    y_pred = model.predict(x_test)\n",
    "                    cm = confusion_matrix(y_pred=y_pred, y_true=y_test)\n",
    "                    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "                    cm_all += cm/5.\n",
    "            else:\n",
    "                if os.path.isfile(tmpcopyfileid1):\n",
    "                    cm_all = np.load(tmpcopyfileid1)['cm']\n",
    "                    model = np.load(tmpcopyfileid1)['model'][0]\n",
    "                elif os.path.isfile(tmpcopyfileid2):\n",
    "                    cm_all = np.load(tmpcopyfileid2)['cm']\n",
    "                    model = np.load(tmpcopyfileid2)['model'][0]\n",
    "                elif os.path.isfile(tmpcopyfileid3):\n",
    "                    cm_all = np.load(tmpcopyfileid3)['cm']\n",
    "                    model = np.load(tmpcopyfileid3)['model'][0]\n",
    "            np.savez(fileid,cm=cm_all,model=np.array([model]))\n",
    "        else: # perform cross-check\n",
    "            tr_data = data\n",
    "            tr_labels = labels\n",
    "            ts_data = data2\n",
    "            ts_labels = labels2\n",
    "            model = []\n",
    "            for m in range(kmax):\n",
    "                tmpcopyfileid = filepath+filename4(i,j,k1,k2,k3,k4,m)+'.npz'\n",
    "                if k1!=m and k2!=m and k3!=m and k4!=m and os.path.isfile(tmpcopyfileid):\n",
    "                    print 'Found precomputed model of '+str(k1)+str(k2)+str(k3)+str(k4)+', tested on '+str(m)+'. Testing on '+str(l)+'...'\n",
    "                    model = np.load(tmpcopyfileid)['model'][0]\n",
    "                    break\n",
    "            if model==[]: # model not found precomputed\n",
    "                print 'Fitting on '+str(k1)+\"-\"+str(k2)+\"-\"+str(k3)+\"-\"+str(k4)+', testing on '+str(l)+'...'\n",
    "                model = pipe.fit(tr_data,tr_labels)\n",
    "            y_pred = model.predict(ts_data)\n",
    "            cm = confusion_matrix(y_pred=y_pred, y_true=ts_labels)\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            np.savez(fileid,cm=cm,model=np.array([model]))\n",
    "\n",
    "def init_steps4(i,j,jmax,surf,surfla):\n",
    "    if j==jmax:\n",
    "        featsel = SelectKBest(k=1000,score_func= mutual_info_classif)\n",
    "    else:\n",
    "        featsel = SelectKBest(k='all',score_func= mutual_info_classif)\n",
    "    pipe = make_pipe_clf(scaler, featsel, decomp, classifiers[2])\n",
    "    for k1 in range(surf.shape[0]): # for every training surface1\n",
    "        for k2 in range(surf.shape[0]): # for every training surface2\n",
    "            if k2 > k1:\n",
    "                for k3 in range(surf.shape[0]):\n",
    "                    if k3 > k2:\n",
    "                        for k4 in range(surf.shape[0]):\n",
    "                            if k4 > k3:\n",
    "                                for l in range(surf.shape[0]): # for every testing surface\n",
    "                                    tr_surf, tr_surfla = np.concatenate((surf[k1],surf[k2],surf[k3]),axis=0), np.concatenate((surfla[:,k1],surfla[:,k2],surfla[:,k3]),axis=0)\n",
    "                                    ts_surf, ts_surfla = surf[l], surfla[:,l]\n",
    "                                    cross_fit4(i,j,k1,k2,k3,k4,surf.shape[0],l,tr_surf,tr_surfla,ts_surf,ts_surfla,pipe)\n",
    "\n",
    "def train_4_surface(surf,surfla,n=-1):\n",
    "    print \"--------------------------- TRAINING all combinations per 4 surfaces -----------------------------------\"        \n",
    "    ##### Parallelized -on surface level- training of models with 4 surfaces                    \n",
    "    for i in range(len(prefeatid)-1):\n",
    "        _ = [Parallel(n_jobs=n)([delayed(init_steps4) (i,j,surf.shape[0]-1,surf[j,:,i],surfla[:,:,i]) \n",
    "                                 for j in range(surf.shape[0])])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Cross surface validation, training with 5 surfaces each time, out of 6 surfaces in total\n",
    "##### total: 4 (featuresets) * [comb(6,5)*6] (surface combinations: trained on 5 - tested on 1) * 1 (prefeatureset)\n",
    "##### = 4*6*6*1 = 144 different runs-files.\n",
    "##### note that comb(n,r) = n!/(r!(n-r)!)\n",
    "def filename5(i,j,k1,k2,k3,k4,k5,l):\n",
    "    return 'fs_'+str(i)+'_subfs_'+str(j)+'_tr1_'+str(k1)+'_tr2_'+str(k2)+'_tr3_'+str(k3)+'_tr4_'+str(k4)+'_tr5_'+str(k5)+'_ts_'+str(l)\n",
    "\n",
    "def cross_fit5(i,j,k1,k2,k3,k4,k5,kmax,l,data,labels,data2,labels2,pipe):\n",
    "    filepath = datapath+'/results5/'\n",
    "    ensure_dir(filepath)\n",
    "    fileid = filepath+filename5(i,j,k1,k2,k3,k4,k5,l)+'.npz'\n",
    "    if not os.path.isfile(fileid):\n",
    "        print i,j,k1,k2,k3,k4,k5,l\n",
    "        if k1==l or k2==l or k3==l or k4==l or k5==l: # perform K-fold      \n",
    "            print 'Fitting on '+str(k1)+\"-\"+str(k2)+\"-\"+str(k3)+\"-\"+str(k4)+\"-\"+str(k5)+', cross-validating on '+str(l)+'...'\n",
    "            if l == k1: # copy if existent from the other sibling file\n",
    "                tmpcopyfileid1 = filepath+filename5(i,j,k1,k2,k3,k4,k5,k2)+'.npz'\n",
    "                tmpcopyfileid2 = filepath+filename5(i,j,k1,k2,k3,k4,k5,k3)+'.npz'\n",
    "                tmpcopyfileid3 = filepath+filename5(i,j,k1,k2,k3,k4,k5,k4)+'.npz'\n",
    "                tmpcopyfileid4 = filepath+filename5(i,j,k1,k2,k3,k4,k5,k5)+'.npz'\n",
    "            elif l == k2:   # same as above\n",
    "                tmpcopyfileid1 = filepath+filename5(i,j,k1,k2,k3,k4,k5,k1)+'.npz'\n",
    "                tmpcopyfileid2 = filepath+filename5(i,j,k1,k2,k3,k4,k5,k3)+'.npz'\n",
    "                tmpcopyfileid3 = filepath+filename5(i,j,k1,k2,k3,k4,k5,k4)+'.npz'\n",
    "                tmpcopyfileid4 = filepath+filename5(i,j,k1,k2,k3,k4,k5,k5)+'.npz'\n",
    "            elif l == k3:   # same as above\n",
    "                tmpcopyfileid1 = filepath+filename5(i,j,k1,k2,k3,k4,k5,k1)+'.npz'\n",
    "                tmpcopyfileid2 = filepath+filename5(i,j,k1,k2,k3,k4,k5,k2)+'.npz'\n",
    "                tmpcopyfileid3 = filepath+filename5(i,j,k1,k2,k3,k4,k5,k4)+'.npz'\n",
    "                tmpcopyfileid4 = filepath+filename5(i,j,k1,k2,k3,k4,k5,k5)+'.npz'\n",
    "            elif l == k4:   # same as above\n",
    "                tmpcopyfileid1 = filepath+filename5(i,j,k1,k2,k3,k4,k5,k1)+'.npz'\n",
    "                tmpcopyfileid2 = filepath+filename5(i,j,k1,k2,k3,k4,k5,k2)+'.npz'\n",
    "                tmpcopyfileid3 = filepath+filename5(i,j,k1,k2,k3,k4,k5,k3)+'.npz'\n",
    "                tmpcopyfileid4 = filepath+filename5(i,j,k1,k2,k3,k4,k5,k5)+'.npz'\n",
    "            else:\n",
    "                tmpcopyfileid1 = filepath+filename5(i,j,k1,k2,k3,k4,k5,k1)+'.npz'\n",
    "                tmpcopyfileid2 = filepath+filename5(i,j,k1,k2,k3,k4,k5,k2)+'.npz'\n",
    "                tmpcopyfileid3 = filepath+filename5(i,j,k1,k2,k3,k4,k5,k3)+'.npz'\n",
    "                tmpcopyfileid4 = filepath+filename5(i,j,k1,k2,k3,k4,k5,k4)+'.npz'\n",
    "            if not os.path.isfile(tmpcopyfileid1) and not os.path.isfile(tmpcopyfileid2) and not os.path.isfile(tmpcopyfileid3) and not os.path.isfile(tmpcopyfileid4):\n",
    "                folds = cv.split(data, labels)\n",
    "                cm_all = np.zeros((2,2))\n",
    "                for fold, (train_ind, test_ind) in enumerate(folds):\n",
    "                    x_train, x_test = data[train_ind], data[test_ind]\n",
    "                    y_train, y_test = labels[train_ind], labels[test_ind]\n",
    "                    model = pipe.fit(x_train,y_train)\n",
    "                    y_pred = model.predict(x_test)\n",
    "                    cm = confusion_matrix(y_pred=y_pred, y_true=y_test)\n",
    "                    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "                    cm_all += cm/5.\n",
    "            else:\n",
    "                if os.path.isfile(tmpcopyfileid1):\n",
    "                    cm_all = np.load(tmpcopyfileid1)['cm']\n",
    "                    model = np.load(tmpcopyfileid1)['model'][0]\n",
    "                elif os.path.isfile(tmpcopyfileid2):\n",
    "                    cm_all = np.load(tmpcopyfileid2)['cm']\n",
    "                    model = np.load(tmpcopyfileid2)['model'][0]\n",
    "                elif os.path.isfile(tmpcopyfileid3):\n",
    "                    cm_all = np.load(tmpcopyfileid3)['cm']\n",
    "                    model = np.load(tmpcopyfileid3)['model'][0]\n",
    "                elif os.path.isfile(tmpcopyfileid4):\n",
    "                    cm_all = np.load(tmpcopyfileid4)['cm']\n",
    "                    model = np.load(tmpcopyfileid4)['model'][0]\n",
    "            np.savez(fileid,cm=cm_all,model=np.array([model]))\n",
    "        else: # perform cross-check\n",
    "            tr_data = data\n",
    "            tr_labels = labels\n",
    "            ts_data = data2\n",
    "            ts_labels = labels2\n",
    "            model = []\n",
    "            for m in range(kmax):\n",
    "                tmpcopyfileid = filepath+filename5(i,j,k1,k2,k3,k4,k5,m)+'.npz'\n",
    "                if k1!=m and k2!=m and k3!=m and k4!=m and k5!=m and os.path.isfile(tmpcopyfileid):\n",
    "                    print 'Found precomputed model of '+str(k1)+str(k2)+str(k3)+str(k4)+str(k5)+', tested on '+str(m)+'. Testing on '+str(l)+'...'\n",
    "                    model = np.load(tmpcopyfileid)['model'][0]\n",
    "                    break\n",
    "            if model==[]: # model not found precomputed\n",
    "                print 'Fitting on '+str(k1)+\"-\"+str(k2)+\"-\"+str(k3)+\"-\"+str(k4)+\"-\"+str(k5)+', testing on '+str(l)+'...'\n",
    "                model = pipe.fit(tr_data,tr_labels)\n",
    "            y_pred = model.predict(ts_data)\n",
    "            cm = confusion_matrix(y_pred=y_pred, y_true=ts_labels)\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            np.savez(fileid,cm=cm,model=np.array([model]))\n",
    "\n",
    "def init_steps5(i,j,jmax,surf,surfla):\n",
    "    if j==jmax:\n",
    "        featsel = SelectKBest(k=1000,score_func= mutual_info_classif)\n",
    "    else:\n",
    "        featsel = SelectKBest(k='all',score_func= mutual_info_classif)\n",
    "    pipe = make_pipe_clf(scaler, featsel, decomp, classifiers[2])\n",
    "    for k1 in range(surf.shape[0]): # for every training surface1\n",
    "        for k2 in range(surf.shape[0]): # for every training surface2\n",
    "            if k2 > k1:\n",
    "                for k3 in range(surf.shape[0]):\n",
    "                    if k3 > k2:\n",
    "                        for k4 in range(surf.shape[0]):\n",
    "                            if k4 > k3:\n",
    "                                for k5 in range(surf.shape[0]):\n",
    "                                    if k5 > k4:\n",
    "                                        for l in range(surf.shape[0]): # for every testing surface\n",
    "                                            tr_surf, tr_surfla = np.concatenate((surf[k1],surf[k2],surf[k3]),axis=0), np.concatenate((surfla[:,k1],surfla[:,k2],surfla[:,k3]),axis=0)\n",
    "                                            ts_surf, ts_surfla = surf[l], surfla[:,l]\n",
    "                                            cross_fit5(i,j,k1,k2,k3,k4,k5,surf.shape[0],l,tr_surf,tr_surfla,ts_surf,ts_surfla,pipe)\n",
    "\n",
    "def train_5_surface(surf,surfla,n=-1):\n",
    "    print \"--------------------------- TRAINING all combinations per 5 surfaces -----------------------------------\"        \n",
    "    ##### Parallelized -on surface level- training of models with 5 surfaces\n",
    "    for i in range(len(prefeatid)-1):\n",
    "        _ = [Parallel(n_jobs=n)([delayed(init_steps5) (i,j,surf.shape[0]-1,surf[j,:,i],surfla[:,:,i]) \n",
    "                                 for j in range(surf.shape[0])])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------- LOADING DATA and COMPUTING NECESSARY STRUCTS -----------------------------\n",
      "1 -> f1: (36,) (36,) (36, 4)\n",
      "2 -> f2: (36,) (36,) (36, 4)\n",
      "3 -> f: (72,) (72,) (72, 4)\n",
      "4 -> m1,m2: 36 36 1.0 1.0\n",
      "5 -> f=f+l: (72,) : [(345002, 4), (105001, 4), (210001, 4), (225002, 4), (130001, 4), (65001, 4), (195001, 4), (65001, 4), (130001, 4), (195001, 4), (65001, 4), (130001, 4), (225002, 4), (65001, 4), (130001, 4), (195001, 4), (65001, 4), (130001, 4), (75001, 4), (130001, 4), (195001, 4), (195001, 4), (130001, 4), (65001, 4), (65001, 4), (130001, 4), (195001, 4), (195001, 4), (130001, 4), (65001, 4), (65001, 4), (130001, 4), (195001, 4), (130001, 4), (195001, 4), (65001, 4), (345002, 4), (105001, 4), (210001, 4), (225002, 4), (130001, 4), (65001, 4), (195001, 4), (65001, 4), (130001, 4), (195001, 4), (65001, 4), (130001, 4), (225002, 4), (65001, 4), (130001, 4), (195001, 4), (65001, 4), (130001, 4), (75001, 4), (130001, 4), (195001, 4), (195001, 4), (130001, 4), (65001, 4), (65001, 4), (130001, 4), (195001, 4), (195001, 4), (130001, 4), (65001, 4), (65001, 4), (130001, 4), (195001, 4), (130001, 4), (195001, 4), (65001, 4)]\n",
      "---------------------------------------- COMPUTING PREFEATURES -----------------------------------------\n",
      "(72,) : [(345002, 2), (105001, 2), (210001, 2), (225002, 2), (130001, 2), (65001, 2), (195001, 2), (65001, 2), (130001, 2), (195001, 2), (65001, 2), (130001, 2), (225002, 2), (65001, 2), (130001, 2), (195001, 2), (65001, 2), (130001, 2), (75001, 2), (130001, 2), (195001, 2), (195001, 2), (130001, 2), (65001, 2), (65001, 2), (130001, 2), (195001, 2), (195001, 2), (130001, 2), (65001, 2), (65001, 2), (130001, 2), (195001, 2), (130001, 2), (195001, 2), (65001, 2), (345002, 2), (105001, 2), (210001, 2), (225002, 2), (130001, 2), (65001, 2), (195001, 2), (65001, 2), (130001, 2), (195001, 2), (65001, 2), (130001, 2), (225002, 2), (65001, 2), (130001, 2), (195001, 2), (65001, 2), (130001, 2), (75001, 2), (130001, 2), (195001, 2), (195001, 2), (130001, 2), (65001, 2), (65001, 2), (130001, 2), (195001, 2), (195001, 2), (130001, 2), (65001, 2), (65001, 2), (130001, 2), (195001, 2), (130001, 2), (195001, 2), (65001, 2)]\n",
      "----------------------------------------- FEATURE EXTRACTION -------------------------------------------\n",
      "sample: 0 , time(sec): 0.04 tmp/features/1024_20/AllFeatures/feat_0_red10000.pkl.z already here! (278, 3107, 2)\n",
      "sample: 1 , time(sec): 0.06 tmp/features/1024_20/AllFeatures/feat_1_red10000.pkl.z already here! (274, 3107, 2)\n",
      "sample: 2 , time(sec): 0.09 tmp/features/1024_20/AllFeatures/feat_2_red10000.pkl.z already here! (275, 3107, 2)\n",
      "sample: 3 , time(sec): 0.12 tmp/features/1024_20/AllFeatures/feat_3_red10000.pkl.z already here! (280, 3107, 2)\n",
      "sample: 4 , time(sec): 0.15 tmp/features/1024_20/AllFeatures/feat_4_red10000.pkl.z already here! (281, 3107, 2)\n",
      "sample: 5 , time(sec): 0.19 tmp/features/1024_20/AllFeatures/feat_5_red10000.pkl.z already here! (267, 3107, 2)\n",
      "sample: 6 , time(sec): 0.22 tmp/features/1024_20/AllFeatures/feat_6_red10000.pkl.z already here! (278, 3107, 2)\n",
      "sample: 7 , time(sec): 0.24 tmp/features/1024_20/AllFeatures/feat_7_red10000.pkl.z already here! (267, 3107, 2)\n",
      "sample: 8 , time(sec): 0.27 tmp/features/1024_20/AllFeatures/feat_8_red10000.pkl.z already here! (281, 3107, 2)\n",
      "sample: 9 , time(sec): 0.30 tmp/features/1024_20/AllFeatures/feat_9_red10000.pkl.z already here! (278, 3107, 2)\n",
      "sample: 10 , time(sec): 0.32 tmp/features/1024_20/AllFeatures/feat_10_red10000.pkl.z already here! (267, 3107, 2)\n",
      "sample: 11 , time(sec): 0.35 tmp/features/1024_20/AllFeatures/feat_11_red10000.pkl.z already here! (281, 3107, 2)\n",
      "sample: 12 , time(sec): 0.38 tmp/features/1024_20/AllFeatures/feat_12_red10000.pkl.z already here! (280, 3107, 2)\n",
      "sample: 13 , time(sec): 0.41 tmp/features/1024_20/AllFeatures/feat_13_red10000.pkl.z already here! (267, 3107, 2)\n",
      "sample: 14 , time(sec): 0.44 tmp/features/1024_20/AllFeatures/feat_14_red10000.pkl.z already here! (281, 3107, 2)\n",
      "sample: 15 , time(sec): 0.47 tmp/features/1024_20/AllFeatures/feat_15_red10000.pkl.z already here! (278, 3107, 2)\n",
      "sample: 16 , time(sec): 0.50 tmp/features/1024_20/AllFeatures/feat_16_red10000.pkl.z already here! (267, 3107, 2)\n",
      "sample: 17 , time(sec): 0.53 tmp/features/1024_20/AllFeatures/feat_17_red10000.pkl.z already here! (281, 3107, 2)\n",
      "sample: 18 , time(sec): 0.56 tmp/features/1024_20/AllFeatures/feat_18_red10000.pkl.z already here! (285, 3107, 2)\n",
      "sample: 19 , time(sec): 0.59 tmp/features/1024_20/AllFeatures/feat_19_red10000.pkl.z already here! (281, 3107, 2)\n",
      "sample: 20 , time(sec): 0.62 tmp/features/1024_20/AllFeatures/feat_20_red10000.pkl.z already here! (278, 3107, 2)\n",
      "sample: 21 , time(sec): 0.65 tmp/features/1024_20/AllFeatures/feat_21_red10000.pkl.z already here! (278, 3107, 2)\n",
      "sample: 22 , time(sec): 0.68 tmp/features/1024_20/AllFeatures/feat_22_red10000.pkl.z already here! (281, 3107, 2)\n",
      "sample: 23 , time(sec): 0.71 tmp/features/1024_20/AllFeatures/feat_23_red10000.pkl.z already here! (267, 3107, 2)\n",
      "sample: 24 , time(sec): 0.73 tmp/features/1024_20/AllFeatures/feat_24_red10000.pkl.z already here! (267, 3107, 2)\n",
      "sample: 25 , time(sec): 0.76 tmp/features/1024_20/AllFeatures/feat_25_red10000.pkl.z already here! (281, 3107, 2)\n",
      "sample: 26 , time(sec): 0.79 tmp/features/1024_20/AllFeatures/feat_26_red10000.pkl.z already here! (278, 3107, 2)\n",
      "sample: 27 , time(sec): 0.82 tmp/features/1024_20/AllFeatures/feat_27_red10000.pkl.z already here! (278, 3107, 2)\n",
      "sample: 28 , time(sec): 0.85 tmp/features/1024_20/AllFeatures/feat_28_red10000.pkl.z already here! (281, 3107, 2)\n",
      "sample: 29 , time(sec): 0.88 tmp/features/1024_20/AllFeatures/feat_29_red10000.pkl.z already here! (267, 3107, 2)\n",
      "sample: 30 , time(sec): 0.90 tmp/features/1024_20/AllFeatures/feat_30_red10000.pkl.z already here! (267, 3107, 2)\n",
      "sample: 31 , time(sec): 0.93 tmp/features/1024_20/AllFeatures/feat_31_red10000.pkl.z already here! (281, 3107, 2)\n",
      "sample: 32 , time(sec): 0.96 tmp/features/1024_20/AllFeatures/feat_32_red10000.pkl.z already here! (278, 3107, 2)\n",
      "sample: 33 , time(sec): 0.99 tmp/features/1024_20/AllFeatures/feat_33_red10000.pkl.z already here! (281, 3107, 2)\n",
      "sample: 34 , time(sec): 1.02 tmp/features/1024_20/AllFeatures/feat_34_red10000.pkl.z already here! (278, 3107, 2)\n",
      "sample: 35 , time(sec): 1.05 tmp/features/1024_20/AllFeatures/feat_35_red10000.pkl.z already here! (267, 3107, 2)\n",
      "sample: 36 , time(sec): 1.08 tmp/features/1024_20/AllFeatures/feat_36_red10000.pkl.z already here! (278, 3107, 2)\n",
      "sample: 37 , time(sec): 1.11 tmp/features/1024_20/AllFeatures/feat_37_red10000.pkl.z already here! (274, 3107, 2)\n",
      "sample: 38 , time(sec): 1.14 tmp/features/1024_20/AllFeatures/feat_38_red10000.pkl.z already here! (275, 3107, 2)\n",
      "sample: 39 , time(sec): 1.16 tmp/features/1024_20/AllFeatures/feat_39_red10000.pkl.z already here! (280, 3107, 2)\n",
      "sample: 40 , time(sec): 1.19 tmp/features/1024_20/AllFeatures/feat_40_red10000.pkl.z already here! (281, 3107, 2)\n",
      "sample: 41 , time(sec): 1.22 tmp/features/1024_20/AllFeatures/feat_41_red10000.pkl.z already here! (267, 3107, 2)\n",
      "sample: 42 , time(sec): 1.25 tmp/features/1024_20/AllFeatures/feat_42_red10000.pkl.z already here! (278, 3107, 2)\n",
      "sample: 43 , time(sec): 1.28 tmp/features/1024_20/AllFeatures/feat_43_red10000.pkl.z already here! (267, 3107, 2)\n",
      "sample: 44 , time(sec): 1.31 tmp/features/1024_20/AllFeatures/feat_44_red10000.pkl.z already here! (281, 3107, 2)\n",
      "sample: 45 , time(sec): 1.34 tmp/features/1024_20/AllFeatures/feat_45_red10000.pkl.z already here! (278, 3107, 2)\n",
      "sample: 46 , time(sec): 1.36 tmp/features/1024_20/AllFeatures/feat_46_red10000.pkl.z already here! (267, 3107, 2)\n",
      "sample: 47 , time(sec): 1.39 tmp/features/1024_20/AllFeatures/feat_47_red10000.pkl.z already here! (281, 3107, 2)\n",
      "sample: 48 , time(sec): 1.42 tmp/features/1024_20/AllFeatures/feat_48_red10000.pkl.z already here! (280, 3107, 2)\n",
      "sample: 49 , time(sec): 1.45 tmp/features/1024_20/AllFeatures/feat_49_red10000.pkl.z already here! (267, 3107, 2)\n",
      "sample: 50 , time(sec): 1.48 tmp/features/1024_20/AllFeatures/feat_50_red10000.pkl.z already here! (281, 3107, 2)\n",
      "sample: 51 , time(sec): 1.51 tmp/features/1024_20/AllFeatures/feat_51_red10000.pkl.z already here! (278, 3107, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample: 52 , time(sec): 1.54 tmp/features/1024_20/AllFeatures/feat_52_red10000.pkl.z already here! (267, 3107, 2)\n",
      "sample: 53 , time(sec): 1.57 tmp/features/1024_20/AllFeatures/feat_53_red10000.pkl.z already here! (281, 3107, 2)\n",
      "sample: 54 , time(sec): 1.60 tmp/features/1024_20/AllFeatures/feat_54_red10000.pkl.z already here! (285, 3107, 2)\n",
      "sample: 55 , time(sec): 1.63 tmp/features/1024_20/AllFeatures/feat_55_red10000.pkl.z already here! (281, 3107, 2)\n",
      "sample: 56 , time(sec): 1.66 tmp/features/1024_20/AllFeatures/feat_56_red10000.pkl.z already here! (278, 3107, 2)\n",
      "sample: 57 , time(sec): 1.69 tmp/features/1024_20/AllFeatures/feat_57_red10000.pkl.z already here! (278, 3107, 2)\n",
      "sample: 58 , time(sec): 1.72 tmp/features/1024_20/AllFeatures/feat_58_red10000.pkl.z already here! (281, 3107, 2)\n",
      "sample: 59 , time(sec): 1.75 tmp/features/1024_20/AllFeatures/feat_59_red10000.pkl.z already here! (267, 3107, 2)\n",
      "sample: 60 , time(sec): 1.77 tmp/features/1024_20/AllFeatures/feat_60_red10000.pkl.z already here! (267, 3107, 2)\n",
      "sample: 61 , time(sec): 1.80 tmp/features/1024_20/AllFeatures/feat_61_red10000.pkl.z already here! (281, 3107, 2)\n",
      "sample: 62 , time(sec): 1.83 tmp/features/1024_20/AllFeatures/feat_62_red10000.pkl.z already here! (278, 3107, 2)\n",
      "sample: 63 , time(sec): 1.86 tmp/features/1024_20/AllFeatures/feat_63_red10000.pkl.z already here! (278, 3107, 2)\n",
      "sample: 64 , time(sec): 1.89 tmp/features/1024_20/AllFeatures/feat_64_red10000.pkl.z already here! (281, 3107, 2)\n",
      "sample: 65 , time(sec): 1.91 tmp/features/1024_20/AllFeatures/feat_65_red10000.pkl.z already here! (267, 3107, 2)\n",
      "sample: 66 , time(sec): 1.94 tmp/features/1024_20/AllFeatures/feat_66_red10000.pkl.z already here! (267, 3107, 2)\n",
      "sample: 67 , time(sec): 1.97 tmp/features/1024_20/AllFeatures/feat_67_red10000.pkl.z already here! (281, 3107, 2)\n",
      "sample: 68 , time(sec): 2.00 tmp/features/1024_20/AllFeatures/feat_68_red10000.pkl.z already here! (278, 3107, 2)\n",
      "sample: 69 , time(sec): 2.03 tmp/features/1024_20/AllFeatures/feat_69_red10000.pkl.z already here! (281, 3107, 2)\n",
      "sample: 70 , time(sec): 2.05 tmp/features/1024_20/AllFeatures/feat_70_red10000.pkl.z already here! (278, 3107, 2)\n",
      "sample: 71 , time(sec): 2.08 tmp/features/1024_20/AllFeatures/feat_71_red10000.pkl.z already here! (267, 3107, 2)\n",
      "Features NOT FOUND PRECOMPUTED! Feature Computation DONE in: 2.08284306526 seconds \n",
      "features:  (72,) [(278, 3107, 1), (274, 3107, 1), (275, 3107, 1), (280, 3107, 1), (281, 3107, 1), (267, 3107, 1), (278, 3107, 1), (267, 3107, 1), (281, 3107, 1), (278, 3107, 1), (267, 3107, 1), (281, 3107, 1), (280, 3107, 1), (267, 3107, 1), (281, 3107, 1), (278, 3107, 1), (267, 3107, 1), (281, 3107, 1), (285, 3107, 1), (281, 3107, 1), (278, 3107, 1), (278, 3107, 1), (281, 3107, 1), (267, 3107, 1), (267, 3107, 1), (281, 3107, 1), (278, 3107, 1), (278, 3107, 1), (281, 3107, 1), (267, 3107, 1), (267, 3107, 1), (281, 3107, 1), (278, 3107, 1), (281, 3107, 1), (278, 3107, 1), (267, 3107, 1), (278, 3107, 1), (274, 3107, 1), (275, 3107, 1), (280, 3107, 1), (281, 3107, 1), (267, 3107, 1), (278, 3107, 1), (267, 3107, 1), (281, 3107, 1), (278, 3107, 1), (267, 3107, 1), (281, 3107, 1), (280, 3107, 1), (267, 3107, 1), (281, 3107, 1), (278, 3107, 1), (267, 3107, 1), (281, 3107, 1), (285, 3107, 1), (281, 3107, 1), (278, 3107, 1), (278, 3107, 1), (281, 3107, 1), (267, 3107, 1), (267, 3107, 1), (281, 3107, 1), (278, 3107, 1), (278, 3107, 1), (281, 3107, 1), (267, 3107, 1), (267, 3107, 1), (281, 3107, 1), (278, 3107, 1), (281, 3107, 1), (278, 3107, 1), (267, 3107, 1)]\n",
      "labels:  (72,) [(278,), (274,), (275,), (280,), (281,), (267,), (278,), (267,), (281,), (278,), (267,), (281,), (280,), (267,), (281,), (278,), (267,), (281,), (285,), (281,), (278,), (278,), (281,), (267,), (267,), (281,), (278,), (278,), (281,), (267,), (267,), (281,), (278,), (281,), (278,), (267,), (278,), (274,), (275,), (280,), (281,), (267,), (278,), (267,), (281,), (278,), (267,), (281,), (280,), (267,), (281,), (278,), (267,), (281,), (285,), (281,), (278,), (278,), (281,), (267,), (267,), (281,), (278,), (278,), (281,), (267,), (267,), (281,), (278,), (281,), (278,), (267,)]\n",
      "features:  (72,) , labels:  (72,)\n",
      "------------------------------------- AVG FEATURE COMPUTATION TIME -------------------------------------\n",
      "Avg feature computation time (millisec):  2.09702205658\n",
      "------------ KEEPING LABEL's PURE (STABLE, SLIP) PHASE PARTS (TRIMMING AROUND CHANGE POINTS)------------\n",
      "new_labels:  (72,)\n",
      "------------------------------ COMPUTING X,Y for CLASSIFIERS' INPUT ------------------------------------\n",
      "Before -> X[ 0 ]:  (36,) , Y[ 0 ]:  (36,) , Yn[ 0 ]:  (36,)\n",
      "Gathered -> X[ 0 ]:  (9935, 3107, 1) , Y[ 0 ]:  (9935,) , Yn[ 0 ]:  (9935,)\n",
      "Gathered, sampled to max  10000  -> X[ 0 ]:  (9935, 3107) , Y[ 0 ]:  (9935,) , Yn[ 0 ]:  (9935,)\n",
      "Split -> Xsp[ 0 ]:  (8826, 3107) , Ysp[ 0 ]:  (8826,)\n",
      "Before -> X[ 1 ]:  (36,) , Y[ 1 ]:  (36,) , Yn[ 1 ]:  (36,)\n",
      "Gathered -> X[ 1 ]:  (9935, 3107, 1) , Y[ 1 ]:  (9935,) , Yn[ 1 ]:  (9935,)\n",
      "Gathered, sampled to max  10000  -> X[ 1 ]:  (9935, 3107) , Y[ 1 ]:  (9935,) , Yn[ 1 ]:  (9935,)\n",
      "Split -> Xsp[ 1 ]:  (8826, 3107) , Ysp[ 1 ]:  (8826,)\n",
      "Before -> X[ 2 ]:  (72,) , Y[ 2 ]:  (72,) , Yn[ 2 ]:  (72,)\n",
      "Gathered -> X[ 2 ]:  (19870, 3107, 1) , Y[ 2 ]:  (19870,) , Yn[ 2 ]:  (19870,)\n",
      "Gathered, sampled to max  10000  -> X[ 2 ]:  (9935, 3107) , Y[ 2 ]:  (9935,) , Yn[ 2 ]:  (9935,)\n",
      "Split -> Xsp[ 2 ]:  (8826, 3107) , Ysp[ 2 ]:  (8826,)\n",
      "X,Y [0,1,2]:  (9935, 3107) (9935,) (9935, 3107) (9935,) (9935, 3107) (9935,)\n",
      "Xsp,Ysp [0,1,2]:  (8826, 3107) (8826,) (8826, 3107) (8826,) (8826, 3107) (8826,)\n",
      "------------------------- COMPUTING X,Y per surface CLASSIFIERS' INPUT ---------------------------------\n",
      "0 0 (6, 1470, 3107)\n",
      "0 1 (6, 1470, 3107)\n",
      "0 2 (6, 1470, 3107)\n",
      "0 3 (6, 1470, 3107)\n",
      "0 4 (6, 1470, 3107)\n",
      "0 5 (6, 1470, 3107)\n",
      "0 6 (6, 1470, 3107)\n",
      "(4, 6, 1) (1470, 6, 1)\n",
      "--------------------------- TRAINING all combinations per 1 surface ------------------------------------\n",
      "--------------------------- TRAINING all combinations per 2 surfaces -----------------------------------\n",
      "--------------------------- TRAINING all combinations per 3 surfaces -----------------------------------\n",
      "--------------------------- TRAINING all combinations per 4 surfaces -----------------------------------\n",
      "--------------------------- TRAINING all combinations per 5 surfaces -----------------------------------\n"
     ]
    }
   ],
   "source": [
    "############################################## TRAINING PROCEDURE #################################################\n",
    "f,l,fd,member,m1,m2 = data_prep(datafile)\n",
    "prefeat = compute_prefeat(f)\n",
    "features, labels = feature_extraction(prefeat, member)\n",
    "avg_feat_comp_time(prefeat)\n",
    "new_labels = label_cleaning(prefeat,labels,member)\n",
    "X,Y,Yn,Xsp,Ysp = computeXY(features,labels,new_labels,m1,m2)\n",
    "surf, surfla = computeXY_persurf(Xsp,Ysp)\n",
    "train_1_surface(surf,surfla)\n",
    "train_2_surface(surf,surfla)\n",
    "train_3_surface(surf,surfla)\n",
    "train_4_surface(surf,surfla)\n",
    "train_5_surface(surf,surfla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------- LOADING DATA and COMPUTING NECESSARY STRUCTS -----------------------------\n",
      "1 -> f1: (1, 1) (1, 1) (1, 1)\n",
      "2 -> f2: (1, 1) (1, 1) (1, 1)\n",
      "3 -> f: (2, 1) (2, 1) (2, 1)\n",
      "4 -> m1,m2: 1 1 1.0 1.0\n",
      "5 -> f=f+l: (2, 65000, 4) : [(65000, 4), (65000, 4)]\n",
      "---------------------------------------- COMPUTING PREFEATURES -----------------------------------------\n",
      "(2, 65000, 2) : [(65000, 2), (65000, 2)]\n",
      "----------------------------------------- FEATURE EXTRACTION -------------------------------------------\n",
      "sample: 0 , time(sec): 1.93 tmp/features/1024_20/AllFeatures/validfeat_0_red10000.pkl.z already here! (3199, 3107, 2)\n",
      "sample: 1 , time(sec): 2.35 tmp/features/1024_20/AllFeatures/validfeat_1_red10000.pkl.z already here! (3199, 3107, 2)\n",
      "Features NOT FOUND PRECOMPUTED! Feature Computation DONE in: 2.3490691185 seconds \n",
      "features:  (2, 3199, 3107, 1) [(3199, 3107, 1), (3199, 3107, 1)]\n",
      "labels:  (2, 3199) [(3199,), (3199,)]\n",
      "features:  (2, 3199, 3107, 1) , labels:  (2, 3199)\n",
      "------------------------------------- AVG FEATURE COMPUTATION TIME -------------------------------------\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-30468b5b389b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprefeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_prefeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_extraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefeat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmember\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidfeatfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'validfeat_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mavg_feat_comp_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mnew_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_cleaning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefeat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmember\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mYn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mXsp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mYsp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomputeXY\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidXYfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidXYsplitfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-bb3e46ab3193>\u001b[0m in \u001b[0;36mavg_feat_comp_time\u001b[0;34m(prefeat)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"------------------------------------- AVG FEATURE COMPUTATION TIME -------------------------------------\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtmpfeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefeat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfeatparam\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# avg over 20*100 times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'Avg feature computation time (millisec): '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 0 with size 2"
     ]
    }
   ],
   "source": [
    "############################################## TESTING PROCEDURE ##################################################\n",
    "f,l,fd,member,m1,m2 = data_prep(validfile)\n",
    "prefeat = compute_prefeat(f)\n",
    "features, labels = feature_extraction(prefeat, member, validfeatfile, 'validfeat_')\n",
    "avg_feat_comp_time(prefeat)\n",
    "new_labels = label_cleaning(prefeat,labels,member)\n",
    "X,Y,Yn,Xsp,Ysp = computeXY(features,labels,new_labels,m1,m2,validXYfile,validXYsplitfile)\n",
    "surf, surfla = computeXY_persurf(Xsp,Ysp,validsurffile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
