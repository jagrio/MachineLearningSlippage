{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn.semi_supervised import LabelSpreading, LabelPropagation\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest, chi2, f_classif, mutual_info_classif\n",
    "import time\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_val_score\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, LassoCV, RandomizedLasso\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest, chi2, f_classif, mutual_info_classif\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold #, GridSearchCV, cross_val_score\n",
    "\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib\n",
    "from mpl_toolkits.mplot3d import Axes3D #, axes3d\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "import operator\n",
    "# import winsound\n",
    "\n",
    "\n",
    "datapath = 'tmp/'\n",
    "featurepath = 'features/1024_1/'\n",
    "XYfile = datapath + featurepath + 'newfeatures_1024_1_10_10000_XY.npz'\n",
    "############# GATHERING into one complete array\n",
    "#X = np.load(XYfile)['X']\n",
    "with np.load(XYfile) as inpf:\n",
    "    X = inpf['X']\n",
    "    Y = inpf['Y']\n",
    "#Y = np.load(XYfile)['Y']\n",
    "\n",
    "   \n",
    "# print ('gathered features: ', X[0].shape, Y[0].shape, X[1].shape, Y[1].shape, X[2].shape, Y[2].shape, np.sum(Y[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# copied \n",
    "window = 1024\n",
    "standardScaler = preprocessing.StandardScaler()\n",
    "# x = np.copy(X[0])\n",
    "X[0] = standardScaler.fit_transform(X[0])\n",
    "## Time Domain Phinyomark feats\n",
    "featnames = ['intsgnl', 'meanabs', 'meanabsslp', 'ssi', 'var', 'rms', 'rng', 'wavl', 'zerox', 'ssc', 'wamp', \n",
    "             'shist1', 'shist2', 'shist3']                                                   # 11+3{shist}\n",
    "## Frequency Domain Phinyomark feats\n",
    "featnames += ['arco1', 'arco2', 'arco3', 'mnf', 'mdf', 'mmnf', 'mmdf']                       # 3{arco}+4{mf}\n",
    "featnames += ['reFFT{:03d}'.format(i) for i in range(window//2+1)]                            # samples/2+1{RF}\n",
    "featnames += ['imFFT{:03d}'.format(i) for i in range(window//2+1)]                            # samples/2+1{IF}\n",
    "## Time Domain Golz feats\n",
    "featnames += ['meanv', 'stdr', 'mx', 'rngx', 'rngy', 'med', 'hjorth', 'sentr', 'se', 'ssk']  # 10\n",
    "featnames += ['acrol{:04d}'.format(i) for i in range(window)]                                # samples{acrol}\n",
    "## Frequency Domain Golz feats\n",
    "featnames += ['amFFT{:03d}'.format(i) for i in range(window//2+1)]                            # samples/2+1{AF}\n",
    "featnames += ['phFFT{:03d}'.format(i) for i in range(window//2+1)]                            # samples/2+1{PF}\n",
    "featnames += ['ffaf']                                                                        # 1{ffaf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "######################################## FEATURE SELECTION #########################################################\n",
    "# feature names\n",
    "\n",
    "\n",
    "# import joblib\n",
    "# from joblib import Parallel, delayed, Memory\n",
    "shift =1 ; keepfromshift = 10; samplesperdataset = 10000\n",
    "\n",
    "tmpind = {}\n",
    "tmpind[2] = range(X[0].shape[1])\n",
    "tmpind[0] = range(X[0].shape[1]//2)\n",
    "tmpind[1] = range(X[0].shape[1]//2,X[0].shape[1])\n",
    "tmpind = np.array([i for _,i in tmpind.items()])\n",
    "\n",
    "featpath = datapath+'features/'+str(window)+'_'+str(shift)+'/'\n",
    "featname = 'newfeatures'+'_'+str(window)+'_'+str(shift)+'_'+str(keepfromshift)+'_'+str(samplesperdataset)\n",
    "\n",
    "\n",
    "\n",
    "numfeat = 10 # number of features to show\n",
    "nfeat = 1000 # number of features to keep\n",
    "#namesid = ['sf{:04d}'.format(i) if i<X[0].shape[1]/2 else 'ftn{:04d}'.format(i) for i in range(X[0].shape[1])]\n",
    "namesid = [['sf{:04d}'.format(i) for i in range(X[0].shape[1]//2)],\n",
    "           ['ftn{:04d}'.format(i) for i in range(X[0].shape[1]//2)],\n",
    "           ['sf{:04d}'.format(i) if i<X[0].shape[1]//2 else 'ftn{:04d}'.format(i%X[0].shape[1]//2) for i in range(X[0].shape[1])]]\n",
    "namesf = [['sf_{}'.format(featnames[i]) for i in range(X[0].shape[1]//2)],\n",
    "         ['ftn_{}'.format(featnames[i]) for i in range(X[0].shape[1]//2)],\n",
    "         ['sf_{}'.format(featnames[i]) if i<X[0].shape[1]//2 else 'ftn_{}'.format(featnames[i%X[0].shape[1]//2]) for i in range(X[0].shape[1])]]\n",
    "########## use RandomizedLasso of MutualInfo as the model to select features and find their importances ############\n",
    "featselfile = featpath+featname+'_featselall'+'.npz'\n",
    "tmpskip = [int(len(tmpind[i])/3000) for i in range(len(tmpind))]\n",
    "#tmpskip = [1 for i in range(3)]\n",
    "start_time = time.time()\n",
    "if os.path.isfile(featselfile):\n",
    "#     print(\"hey yooooo\")\n",
    "    #rlasso = np.load(featselfile)['rlasso'].tolist()\n",
    "    featsel = np.load(featselfile)['featsel'].tolist()\n",
    "#     print(\"Selected Features FOUND PRECOMPUTED! Loading DONE in: %s seconds \" % (time.time() - start_time))\n",
    "else:                                                                                                    \n",
    "    with warnings.catch_warnings():\n",
    "#         print(\"You're in for a long ride\")\n",
    "        warnings.simplefilter('ignore', UserWarning)\n",
    "        warnings.simplefilter('ignore', ConvergenceWarning)\n",
    "        featsel = [[SelectKBest(mutual_info_classif,'all').fit(X[i][:-1:tmpskip[i],tmpind[j]],Y[i][:-1:tmpskip[i]]) \n",
    "                    for j in range(3)] for i in range(1)]\n",
    "#         print(\"Selected Features NOT FOUND PRECOMPUTED! Selection DONE in: %s seconds \" % (time.time() - start_time))\n",
    "        # loop for all datasets (12,3,all) and all features (sf,ftn,all)\n",
    "        #featsel = [[RandomizedLasso(fit_intercept=False,normalize=False).fit(X[i][:-1:tmpskip[i],tmpind[j]],Y[i][:-1:tmpskip[i]]) \n",
    "        #Parallel(n_jobs=-1)([delayed(feat) (p[k:k+window],*featparam) for k in range(0,len(p)-window,shift)])\n",
    "#         featsel = [Parallel(n_jobs=-1)([delayed(SelectKBest(mutual_info_classif,'all').fit) \n",
    "#                     (X[i][:-1:tmpskip[i],tmpind[j]],Y[i][:-1:tmpskip[i]])\n",
    "#                     for i,j in itertools.product(range(len(X)),range(3))])]\n",
    "#         featsel = np.array([[featsel[0][i*len(range(3))+j] for j in range(3)] for i in range(len(X))])\n",
    "#                    for j in range(3) for i in range(len(X))]\n",
    "        \n",
    "        #np.savez(featselfile,rlasso=np.array(rlasso))\n",
    "        np.savez(featselfile,featsel=featsel)\n",
    "# rank features\n",
    "# print \"----> Features sorted by their rank (std norm):\"\n",
    "bestix = {}\n",
    "wrstix = {}\n",
    "# for i in range(len(X)): # for all diff data sets\n",
    "# Im only using the first one so the for above doesnt apply\n",
    "for i in range (3):\n",
    "    bestix[i] = {}\n",
    "    wrstix[i] = {}\n",
    "    for j in range(3): # for all diff feature sets\n",
    "#         tmp = sorted(zip(map(lambda x: round(x,4), featsel[i][j].scores_),namesf[j]),reverse=True)\n",
    "#         print(j)\n",
    "        bestix[i][j] = np.array(featsel[i][j].scores_).argsort()[:][::-1]\n",
    "        #wrstix[i][j] = np.array(featsel[i][j].scores_).argsort()[:nfeat][::1]\n",
    "        wrstix[i][j] = np.array(featsel[i][j].scores_).argsort()[:][::1]\n",
    "#         print 'Best '+str(numfeat)+': ',tmp[:numfeat], bestix[i][j].shape, bestix[i][j][:numfeat]\n",
    "#         print 'Worst '+str(numfeat)+': ',tmp[-numfeat:], wrstix[i][j].shape, wrstix[i][j][:numfeat]\n",
    "bestix = np.array([[ij for _,ij in i.items()] for _,i in bestix.items()])\n",
    "wrstix = np.array([[ij for _,ij in i.items()] for _,i in wrstix.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(featsel).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "############################### DIMENSIONALITY REDUCTION VIA FEATURE SELECTION #####################################\n",
    "# fig = plt.figure(figsize=(8,6))\n",
    "r,c = np.array(featsel).shape[0],np.array(featsel).shape[1]\n",
    "Xfs = {}\n",
    "# matplotlib.rcParams['text.usetex'] = True\n",
    "for i in range(r):\n",
    "    Xfs[i] = {}\n",
    "    for j in range(c):\n",
    "        Xfs[i][j] = np.array(featsel[i][j].transform(X[i][:,tmpind[j]]))[:,bestix[i][j][:nfeat]]\n",
    "#         print (Xfs[i][j].shape)\n",
    "#         ax = fig.add_subplot(r,c,i*c+j+1)\n",
    "#         ax.plot(featsel[i][j].scores_,'b')\n",
    "#         ax.scatter(bestix[i][j][:nfeat],featsel[i][j].scores_[bestix[i][j][:nfeat]],color='r')\n",
    "#         if i==0 and j==0:\n",
    "#             ax.set_title(r'$\\zeta_1=\\sigma(|f|)$')\n",
    "#             ax.set_ylabel(r'$d_1$')\n",
    "#         if i==0 and j==1:\n",
    "#             ax.set_title(r'$\\zeta_2=\\sigma(|f_t/f_n|)$')\n",
    "#         if i==0 and j==2:\n",
    "#             ax.set_title(r'$\\zeta_3=\\zeta_1\\bigcup \\zeta_2$')\n",
    "#         if i==1 and j==0:\n",
    "#             ax.set_ylabel(r'$d_2$')\n",
    "#         if i==2 and j==0:\n",
    "#             ax.set_ylabel(r'$d_3=d_1\\bigcup d_2$')\n",
    "#         if i==2:\n",
    "#             ax.set_xlabel('features')\n",
    "Xfs = np.array([[i for _,i in Xfsi.items()] for _,Xfsi in Xfs.items()])\n",
    "#plt.show()\n",
    "# savefig('plots/'+featname+'_featsel.png', bbox_inches='tight')\n",
    "# savefig('plots/featureselection.eps',format='eps',dpi=50,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "################################### DIMENSIONALITY REDUCTION #######################################################\n",
    "featdimredfile = featpath+featname+'_dimred'+'.npz'\n",
    "ncomp = 50\n",
    "tmpskip = 1\n",
    "# print(Xfs[0].shape[0], Xfs[0].shape[1])\n",
    "\n",
    "if os.path.isfile(featdimredfile):\n",
    "    inpdimred = np.load(featdimredfile)\n",
    "    pca = inpdimred['pca'].tolist()\n",
    "    Xpca = inpdimred['Xpca']\n",
    "    tmpix = inpdimred['tmpix']\n",
    "else:\n",
    "    # PCA\n",
    "    r,c = Xfs.shape[0],Xfs.shape[1]\n",
    "    Xpca,pca,tmpix = {},{},{}\n",
    "    for i in range(r):\n",
    "        Xpca[i],pca[i] = {},{}\n",
    "        tmpix[i] = range(0,Xfs[i][0].shape[0],tmpskip)\n",
    "        for j in range(c):\n",
    "            pca[i][j] = PCA(n_components=ncomp, random_state=0)\n",
    "            pca[i][j].fit(Xfs[i][j][tmpix[i],:])\n",
    "            Xpca[i][j] = pca[i][j].transform(Xfs[i][j][tmpix[i],:])\n",
    "    pca = np.array([[i for _,i in pcai.items()] for _,pcai in pca.items()])\n",
    "    Xpca = np.array([[i for _,i in Xpcai.items()] for _,Xpcai in Xpca.items()])\n",
    "    tmpix = np.array([i for _,i  in tmpix.items()])\n",
    "    np.savez(featdimredfile,pca=pca,Xpca=Xpca,tmpix=tmpix)\n",
    "# for i in range(r):\n",
    "#     for j in range(c):\n",
    "#         print (Xpca[i][j].shape, np.array(tmpix[i]).shape ,(pca[i][j].explained_variance_ratio_))\n",
    "#         ################################## VISUALIZATION OF DIMENSIONALITY REDUCTION ###############################\n",
    "#         fig = plt.figure(figsize=(6,2))\n",
    "#         fig.suptitle(str(i)+str(j), fontsize=14, fontweight='bold')\n",
    "#         #ax = Axes3D(fig)\n",
    "#         #fig = plt.figure()\n",
    "#         #ax = fig.add_subplot(111, projection='3d')\n",
    "#         ax = fig.add_subplot(141, projection='3d')\n",
    "#         #ax = fig.add_subplot(141)\n",
    "#         ax.scatter(Xpca[i][j][:,0],Xpca[i][j][:,1],Xpca[i][j][:,2],c=Y[i][:,np.newaxis]*np.array([[1,1,0]]))\n",
    "#         ax = fig.add_subplot(142)\n",
    "#         ax.scatter(Xpca[i][j][:,0],Xpca[i][j][:,1],c=Y[i][:,np.newaxis]*np.array([[1,1,0]]))\n",
    "#         ax = fig.add_subplot(143)\n",
    "#         ax.scatter(Xpca[i][j][:,0],Xpca[i][j][:,2],c=Y[i][:,np.newaxis]*np.array([[1,1,0]]))\n",
    "#         ax = fig.add_subplot(144)\n",
    "#         ax.scatter(Xpca[i][j][:,1],Xpca[i][j][:,2],c=Y[i][:,np.newaxis]*np.array([[1,1,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "######################################## DATASETS PREPARATION ######################################################\n",
    "datasets = [(Xpca[i][j],Y[i][tmpix[i]]) for j in range(c) for i in range(r)]\n",
    "# print (np.array(datasets).shape)\n",
    "for ds in datasets:\n",
    "    X1,Y1 = ds\n",
    "#     print (X1.shape,Y1.shape)\n",
    "#     tems = np.sum(Y1)\n",
    "#     print(tems)\n",
    "#     plt.hist(Y1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20149, 50), (20149,), 0.47114000694823566)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1.shape, Y1.shape, sum(Y1)/len(Y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer loop 0\n",
      "Classifier 0\n",
      "Inner fold 0 \n",
      "Inner fold 1 \n",
      "Inner fold 2 \n",
      "Inner fold 3 \n",
      "Inner fold 4 \n",
      "Classifier 1\n",
      "Inner fold 0 \n",
      "Inner fold 1 \n",
      "Inner fold 2 \n",
      "Inner fold 3 \n",
      "Inner fold 4 \n",
      "Classifier 2\n",
      "Inner fold 0 \n",
      "Inner fold 1 \n",
      "Inner fold 2 \n",
      "Inner fold 3 \n",
      "Inner fold 4 \n",
      "Classifier 3\n",
      "Inner fold 0 \n",
      "Inner fold 1 \n",
      "Inner fold 2 \n",
      "Inner fold 3 \n",
      "Inner fold 4 \n",
      "Classifier 4\n",
      "Inner fold 0 \n",
      "Inner fold 1 \n",
      "Inner fold 2 \n",
      "Inner fold 3 \n",
      "Inner fold 4 \n",
      "[ 0.78668755  0.83391089  0.83453147  0.84767015  0.87580578]\n",
      "Best classifier for outer fold 0 is MLPClassifier(activation='relu', alpha=1, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
      "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False) \n",
      "Inner score: 0.875806 VS Outer score: 0.864718\n",
      "Outer loop 1\n",
      "Classifier 0\n",
      "Inner fold 0 \n",
      "Inner fold 1 \n",
      "Inner fold 2 \n",
      "Inner fold 3 \n",
      "Inner fold 4 \n",
      "Classifier 1\n",
      "Inner fold 0 \n",
      "Inner fold 1 \n",
      "Inner fold 2 \n",
      "Inner fold 3 \n",
      "Inner fold 4 \n",
      "Classifier 2\n",
      "Inner fold 0 \n",
      "Inner fold 1 \n",
      "Inner fold 2 \n",
      "Inner fold 3 \n",
      "Inner fold 4 \n",
      "Classifier 3\n",
      "Inner fold 0 \n",
      "Inner fold 1 \n",
      "Inner fold 2 \n",
      "Inner fold 3 \n",
      "Inner fold 4 \n",
      "Classifier 4\n",
      "Inner fold 0 \n",
      "Inner fold 1 \n",
      "Inner fold 2 \n",
      "Inner fold 3 \n",
      "Inner fold 4 \n",
      "[ 0.78844949  0.83108311  0.83244626  0.83926262  0.87160832]\n",
      "Best classifier for outer fold 1 is MLPClassifier(activation='relu', alpha=1, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
      "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False) \n",
      "Inner score: 0.871608 VS Outer score: 0.887952\n",
      "Outer loop 2\n",
      "Classifier 0\n",
      "Inner fold 0 \n",
      "Inner fold 1 \n",
      "Inner fold 2 \n",
      "Inner fold 3 \n",
      "Inner fold 4 \n",
      "Classifier 1\n",
      "Inner fold 0 \n",
      "Inner fold 1 \n",
      "Inner fold 2 \n",
      "Inner fold 3 \n",
      "Inner fold 4 \n",
      "Classifier 2\n",
      "Inner fold 0 \n",
      "Inner fold 1 \n",
      "Inner fold 2 \n",
      "Inner fold 3 \n",
      "Inner fold 4 \n",
      "Classifier 3\n",
      "Inner fold 0 \n",
      "Inner fold 1 \n",
      "Inner fold 2 \n",
      "Inner fold 3 \n",
      "Inner fold 4 \n",
      "Classifier 4\n",
      "Inner fold 0 \n",
      "Inner fold 1 \n",
      "Inner fold 2 \n",
      "Inner fold 3 \n",
      "Inner fold 4 \n",
      "[ 0.78733263  0.83195013  0.83628718  0.844591    0.87136056]\n",
      "Best classifier for outer fold 2 is MLPClassifier(activation='relu', alpha=1, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
      "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False) \n",
      "Inner score: 0.871361 VS Outer score: 0.874566\n",
      "Outer loop 3\n",
      "Classifier 0\n",
      "Inner fold 0 \n",
      "Inner fold 1 \n",
      "Inner fold 2 \n",
      "Inner fold 3 \n",
      "Inner fold 4 \n",
      "Classifier 1\n",
      "Inner fold 0 \n",
      "Inner fold 1 \n",
      "Inner fold 2 \n",
      "Inner fold 3 \n",
      "Inner fold 4 \n",
      "Classifier 2\n",
      "Inner fold 0 \n",
      "Inner fold 1 \n",
      "Inner fold 2 \n",
      "Inner fold 3 \n",
      "Inner fold 4 \n",
      "Classifier 3\n",
      "Inner fold 0 \n",
      "Inner fold 1 \n",
      "Inner fold 2 \n",
      "Inner fold 3 \n",
      "Inner fold 4 \n",
      "Classifier 4\n",
      "Inner fold 0 \n",
      "Inner fold 1 \n",
      "Inner fold 2 \n",
      "Inner fold 3 \n",
      "Inner fold 4 \n",
      "[ 0.79080527  0.83480058  0.84050308  0.85215162  0.87470621]\n",
      "Best classifier for outer fold 3 is MLPClassifier(activation='relu', alpha=1, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
      "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False) \n",
      "Inner score: 0.874706 VS Outer score: 0.874566\n",
      "Outer loop 4\n",
      "Classifier 0\n",
      "Inner fold 0 \n",
      "Inner fold 1 \n",
      "Inner fold 2 \n",
      "Inner fold 3 \n",
      "Inner fold 4 \n",
      "Classifier 1\n",
      "Inner fold 0 \n",
      "Inner fold 1 \n",
      "Inner fold 2 \n",
      "Inner fold 3 \n",
      "Inner fold 4 \n",
      "Classifier 2\n",
      "Inner fold 0 \n",
      "Inner fold 1 \n",
      "Inner fold 2 \n",
      "Inner fold 3 \n",
      "Inner fold 4 \n",
      "Classifier 3\n",
      "Inner fold 0 \n",
      "Inner fold 1 \n",
      "Inner fold 2 \n",
      "Inner fold 3 \n",
      "Inner fold 4 \n",
      "Classifier 4\n",
      "Inner fold 0 \n",
      "Inner fold 1 \n",
      "Inner fold 2 \n",
      "Inner fold 3 \n",
      "Inner fold 4 \n",
      "[ 0.78845256  0.83566814  0.83876611  0.84793649  0.87123526]\n",
      "Best classifier for outer fold 4 is MLPClassifier(activation='relu', alpha=1, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
      "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False) \n",
      "Inner score: 0.871235 VS Outer score: 0.872583\n",
      "CPU times: user 2min 26s, sys: 1min 9s, total: 3min 36s\n",
      "Wall time: 2min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Nested CV. \n",
    "# The basic idea is that cross-validation is used to assess the performance of a method for fitting a model, \n",
    "# not of the model itself. If you need to perform model selection, then you need to perform that independently \n",
    "# in each fold of the cross-validation procedure, as it is an integral part of the model fitting procedure. \n",
    "# If you use a cross-validation based model selection procedure, this means you end up with nested cross-validation.\n",
    "# It is helpful to consider the purpose of each cross-validation - one is for model selection, the other for performance\n",
    "# estimation. I would make my final model by fitting the model (including model selection) to the whole dataset, after\n",
    "# using nested cross-validation to get an idea of the performance I could reasonably expect to get from that model.\n",
    "\n",
    "\n",
    "\n",
    "classifiers = [#SVC(kernel = \"linear\")]#,\n",
    "       SVC(gamma = 2.5, C = 10),#, \n",
    "       GaussianNB(),\n",
    "       KNeighborsClassifier(4),\n",
    "       KNeighborsClassifier(5),\n",
    "       MLPClassifier(alpha = 1)]\n",
    "\n",
    "X,y = datasets[1]\n",
    "\n",
    "# classifiers = [SVC( gamma = 2.5)]\n",
    "# print(len(classifiers))\n",
    "n_outer_folds = 5\n",
    "n_inner_folds = 5\n",
    "# outer_scores = np.zeros((len(top_classifiers), n_outer_folds)) #use this if you wanna keep the score of multiple \"best\" classifiers from the inner cv\n",
    "outer_scores  = np.zeros(n_outer_folds)\n",
    "# print(outer_scores.shape, inner_scores.shape, inner_mean_scores.shape)\n",
    "state = 42 \n",
    "x_tot = np.copy(X) ; y_tot = np.copy(y)\n",
    "\n",
    "\n",
    "# outer_scores = []\n",
    "#outer loop i.e. the one that evaluates the inner \"best\" model\n",
    "outer_kfold = KFold(n_splits = n_outer_folds, shuffle = True, random_state = state)\n",
    "outer = outer_kfold.split(x_tot)\n",
    "# print(\"chkpoint 2\")\n",
    "\n",
    "for fold_out, (train_ind_out, test_ind_out) in enumerate(outer):\n",
    "    print(\"Outer loop %d\" %fold_out)\n",
    "    x_trn_out, x_tst_out = x_tot[train_ind_out], x_tot[test_ind_out]\n",
    "    y_trn_out, y_tst_out = y_tot[train_ind_out], y_tot[test_ind_out]  \n",
    "#     print(\"outer data split\")\n",
    "    inner_scores = np.zeros((len(classifiers),n_inner_folds))\n",
    "    inner_mean_scores = np.zeros(len(classifiers))\n",
    "    for cl_ind,clf in enumerate(classifiers):\n",
    "        print(\"Classifier %d\" %cl_ind)\n",
    "        inner_kfold = KFold( n_splits = n_inner_folds, shuffle = True, random_state = state)\n",
    "        inner = inner_kfold.split(x_trn_out)\n",
    "#         print(\"classifier loaded\")\n",
    "        \n",
    "        for fold_in, (train_ind_in, test_ind_in) in enumerate(inner): \n",
    "            print(\"Inner fold %d \" %fold_in)\n",
    "            x_trn_in, x_tst_in = x_trn_out[train_ind_in], x_trn_out[test_ind_in]\n",
    "            y_trn_in, y_tst_in = y_trn_out[train_ind_in], y_trn_out[test_ind_in]\n",
    "#             print(\"inner data split\")\n",
    "            clf.fit(x_trn_in, y_trn_in)\n",
    "#             print(\"clf fit\")\n",
    "            \n",
    "            #inner scores calculated for every inner fold\n",
    "            inner_scores[cl_ind][fold_in] = clf.score(x_tst_in, y_tst_in)\n",
    "#             print(\"inner scores calculated\")\n",
    "#         print(inner_scores)\n",
    "    # get the mean performance for every classifier\n",
    "    inner_mean_scores = np.mean(inner_scores, axis = 1)\n",
    "#     print(\"inner mean score calculated\")\n",
    "    print(inner_mean_scores)\n",
    "        \n",
    "#     get the index of the best classifier for inner cv\n",
    "    best_clf = inner_mean_scores.argmax()\n",
    "    best_mean_score = inner_mean_scores.max()\n",
    "    print(\"Best classifier for outer fold %d is %s \" %(fold_out, classifiers[best_clf]))\n",
    "#     print(\"Best classifier for outer fold %d is %d \" %(fold_out, best_clf))\n",
    "        \n",
    "    #fit the best classifier on the outer test data\n",
    "    classifiers[best_clf].fit(x_trn_out, y_trn_out)\n",
    "    #get the outer score \n",
    "    outer_scores[fold_out] = classifiers[best_clf].score(x_tst_out, y_tst_out)\n",
    "    print(\"Inner score: %f VS Outer score: %f\" %(best_mean_score, outer_scores[fold_out]))\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
