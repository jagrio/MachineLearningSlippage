{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mainly Edited for private usage by:  Ioannis Agriomallos\n",
      "                                        Ioanna Mitsioni\n",
      "License: BSD 3 clause\n",
      "\n",
      "============= CURRENT CODE USAGE =============\n",
      "Current code trains MLP Classifiers, to classify force input samples as stable (0) or slip (1)\n",
      "---- Input\n",
      "-> Input samples originate from optoforce sensors and are 3D (fx,fy,fz) and come from 2 different datasets, \n",
      "   one training, containing several surfaces as well as slip-stable occurrences, \n",
      "   and one validation, containing 1 surface with slip-stable occurrences on a completely unseen task-setup.\n",
      "---- Input transformation\n",
      "-> Several pre-features can be taken from these inputs, but here |f| is kept.\n",
      "-> Several time and frequency domain features are extracted from pre-feature windows. \n",
      "  (implemented in 'featext.py') These windows have size w and are shifted by s on each sample\n",
      "-> Then a feature selection-ranking is performed using MutualVariableInformation\n",
      "-> Finally PCA is performed to keep a reduced set among the best selected features\n",
      "---- Training of ML Classifiers\n",
      "-> Several MLP Classifiers are trained for all combinations of selected featuresets-datasets\n",
      "---- Results\n",
      "-> Stats of classification results are kept inside each .npz along with the respective trained model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Mainly Edited for private usage by:  Ioannis Agriomallos\n",
    "                                        Ioanna Mitsioni\n",
    "License: BSD 3 clause\n",
    "\n",
    "============= CURRENT CODE USAGE =============\n",
    "Current code trains MLP Classifiers, to classify force input samples as stable (0) or slip (1)\n",
    "---- Input\n",
    "-> Input samples originate from optoforce sensors and are 3D (fx,fy,fz) and come from 2 different datasets, \n",
    "   one training, containing several surfaces as well as slip-stable occurrences, \n",
    "   and one validation, containing 1 surface with slip-stable occurrences on a completely unseen task-setup.\n",
    "---- Input transformation\n",
    "-> Several pre-features can be taken from these inputs, but here |f| is kept.\n",
    "-> Several time and frequency domain features are extracted from pre-feature windows. \n",
    "  (implemented in 'featext.py') These windows have size w and are shifted by s on each sample\n",
    "-> Then a feature selection-ranking is performed using MutualVariableInformation\n",
    "-> Finally PCA is performed to keep a reduced set among the best selected features\n",
    "---- Training of ML Classifiers\n",
    "-> Several MLP Classifiers are trained for all combinations of selected featuresets-datasets\n",
    "---- Results\n",
    "-> Stats of classification results are kept inside each .npz along with the respective trained model\n",
    "\"\"\"\n",
    "print(__doc__)\n",
    "import time\n",
    "start_time = time.time()\n",
    "from copy import deepcopy, copy\n",
    "import math\n",
    "import scipy.io as sio\n",
    "import shutil\n",
    "import os, errno\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "from pylab import *\n",
    "from featext import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "# %matplotlib qt\n",
    "# inline (suitable for ipython only, shown inside browser!) or qt (suitable in general, shown in external window!)\n",
    "from matplotlib.colors import ListedColormap\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import re\n",
    "import datetime\n",
    "import urllib\n",
    "import tarfile\n",
    "import joblib\n",
    "from joblib import Parallel, delayed, Memory\n",
    "from tempfile import mkdtemp\n",
    "import copy_reg\n",
    "import types\n",
    "import itertools\n",
    "\n",
    "def _pickle_method(m):\n",
    "    \"\"\"Useful function for successful convertion from directories and lists to numpy arrays\"\"\"\n",
    "    if m.im_self is None:\n",
    "        return getattr, (m.im_class, m.im_func.func_name)\n",
    "    else:\n",
    "        return getattr, (m.im_self, m.im_func.func_name)\n",
    "copy_reg.pickle(types.MethodType, _pickle_method)\n",
    "\n",
    "def ensure_dir(directory):\n",
    "    \"\"\"Useful function for creating directory only if not existent\"\"\"\n",
    "    try:\n",
    "        os.makedirs(directory)\n",
    "    except OSError as e:\n",
    "        if e.errno != errno.EEXIST:\n",
    "            raise\n",
    "            \n",
    "h = .2  # step size in the mesh\n",
    "\n",
    "######## TRAINING DEFAULTS\n",
    "cv = KFold(n_splits=5,random_state=42)\n",
    "scaler = StandardScaler() ;\n",
    "decomp = PCA(n_components=20)\n",
    "names = [\"NearNb\", \"RBFSVM1\", \"MLP1\", \"RandFor\"]\n",
    "classifiers = [KNeighborsClassifier(5),\n",
    "               SVC(gamma='auto', C=1),\n",
    "               MLPClassifier(solver='lbfgs',alpha=1e-4,hidden_layer_sizes=(10,10),random_state=1,verbose=True),\n",
    "               RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)]\n",
    "\n",
    "download = 1 # Download pre-computed (1) data or compute them all anew (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############ INITIALISATION PARAMETERS ############\n",
    "window, shift = 1024, 20\n",
    "samplesperdataset = 10000\n",
    "havelabel = 1\n",
    "returntime = 0\n",
    "featlabel = 0         # 0: all features, 1: temporal, 2: frequency, 3: FFT only\n",
    "magnFFT = 0           # 0: FFT in magnitude format, 1: FFT in real and imag format, \n",
    "featall = 0           # 0: all, 1: feat1 (phinyomark's), 2: feat2 (golz's)\n",
    "featparam = [havelabel,featlabel,magnFFT,featall,returntime]\n",
    "CV = 5                # cross validation checks\n",
    "numfeat = 10          # number of features to show\n",
    "nfeat = 1000          # number of features to keep\n",
    "###### Initialize necessary names and paths\n",
    "datapath = 'data/'\n",
    "ensure_dir(datapath)\n",
    "datafile = datapath+'dataset.npz'\n",
    "validfile = datapath+'validation.mat'\n",
    "featpath = datapath+'features/'+str(window)+'_'+str(shift)+'/'\n",
    "ensure_dir(featpath)\n",
    "prefeatname = 'prefeatures'+'_'+str(window)+'_'+str(shift)+'_'+str(samplesperdataset)\n",
    "prefeatfile = featpath+prefeatname+'.npz'\n",
    "featname = 'features'+'_'+str(window)+'_'+str(shift)+'_'+str(samplesperdataset)\n",
    "featfile = featpath+featname+'.npz'\n",
    "validfeatname = 'valid'+featname\n",
    "validfeatfile = featpath+validfeatname+'.npz'\n",
    "surffile = featpath+featname+'_2fing_6surf.npz'\n",
    "XYfile = featpath+featname+'_XY.npz'\n",
    "XYsplitfile = featpath+featname+'_XYsplit.npz'\n",
    "validsurffile = featpath+validfeatname+'_2fing_6surf.npz'\n",
    "validXYfile = featpath+validfeatname+'_XY.npz'\n",
    "validXYsplitfile = featpath+validfeatname+'_XYsplit.npz'\n",
    "respath = datapath+'results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############ Feature Names ############\n",
    "\"\"\"features:                                                                       ||      if       \n",
    "   |--> time domain      :                                                         || samples = 1024\n",
    "   |----|---> phinyomark : 11+3{shist} --------------------------> = 14+0.0samples ||             14\n",
    "   |----|---> golz       : 10+samples{acrol} --------------------> = 10+1.0samples ||           1034\n",
    "   |--> frequency domain :                                                                          \n",
    "   |----|---> phinyomark : 3{arco}+4{mf}+2(samples/2+1){RF,IF} --> =  9+1.0samples ||           1033\n",
    "   |----|---> golz       : 2(samples/2+1){AF,PF} ----------------> =  2+1.0samples ||           1026\n",
    "   |----|----------------|-------alltogether---------------------> = 35+3.0samples || numfeat = 3107\n",
    "\"\"\"\n",
    "## Time Domain Phinyomark feats\n",
    "featnames = ['intsgnl', 'meanabs', 'meanabsslp', 'ssi', 'var', 'rms', 'rng', 'wavl', 'zerox', 'ssc', 'wamp', \n",
    "             'shist1', 'shist2', 'shist3']                                                   # 11+3{shist}\n",
    "## Frequency Domain Phinyomark feats\n",
    "featnames += ['arco1', 'arco2', 'arco3', 'mnf', 'mdf', 'mmnf', 'mmdf']                       # 3{arco}+4{mf}\n",
    "featnames += ['reFFT{:03d}'.format(i) for i in range(window/2+1)]                            # samples/2+1{RF}\n",
    "featnames += ['imFFT{:03d}'.format(i) for i in range(window/2+1)]                            # samples/2+1{IF}\n",
    "## Time Domain Golz feats\n",
    "featnames += ['meanv', 'stdr', 'mx', 'rngx', 'rngy', 'med', 'hjorth', 'sentr', 'se', 'ssk']  # 10\n",
    "featnames += ['acrol{:04d}'.format(i) for i in range(window)]                                # samples{acrol}\n",
    "## Frequency Domain Golz feats\n",
    "featnames += ['amFFT{:03d}'.format(i) for i in range(window/2+1)]                            # samples/2+1{AF}\n",
    "featnames += ['phFFT{:03d}'.format(i) for i in range(window/2+1)]                            # samples/2+1{PF}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Necessary  data/dataset.npz  already here!\n",
      "Necessary  data/validation.mat  already here!\n",
      "Necessary  data/features/1024_20/features_1024_20_10000.npz  already here!\n",
      "Necessary  data/features/1024_20/validfeatures_1024_20_10000.npz  already here!\n"
     ]
    }
   ],
   "source": [
    "############ Download necessary files ############\n",
    "def download_file(datafile, targetlink):\n",
    "    \"\"\"Function for checking if targetfile exists, else downloading it from targetlink to targetpath+targetfile\"\"\"\n",
    "    if not os.path.isfile(datafile):\n",
    "        print 'Necessary ', datafile, ' not here! Downloading...'\n",
    "        u = urllib.urlopen(targetlink)\n",
    "        data = u.read()\n",
    "        print 'Completed downloading ','{:.2f}'.format(len(data)*1./(1024**2)),'MB of ',datafile,'!'\n",
    "        u.close()\n",
    "        with open(datafile, \"wb\") as f :\n",
    "            f.write(data)\n",
    "        print 'Necessary ', datafile, ' completed saving!'\n",
    "    else:\n",
    "        print 'Necessary ', datafile, ' already here!'\n",
    "\n",
    "def extract_file(zipfile):\n",
    "    \n",
    "    return unzipfile\n",
    "        \n",
    "####### Download necessary dataset\n",
    "datafile = datapath+'dataset.npz'\n",
    "validfile = datapath+'validation.mat'\n",
    "datalink = 'https://www.dropbox.com/s/j88wmtx1vvpik1m/dataset.npz?dl=1'\n",
    "validlink = 'https://www.dropbox.com/s/r8jl57lij28ljrw/validation.mat?dl=1'\n",
    "download_file(datafile, datalink)\n",
    "download_file(validfile, validlink)\n",
    "####### Download features, if not wanting to compute them and not already there\n",
    "if download==1:\n",
    "    featlink = 'https://www.dropbox.com/s/qvk9pcvlir06zse/features_1024_20_10000.npz?dl=1'\n",
    "    validfeatlink = 'https://www.dropbox.com/s/sghqwifo8rxwbcs/validfeatures_1024_20_10000.npz?dl=1'\n",
    "    download_file(featfile, featlink)\n",
    "    download_file(validfeatfile, validfeatlink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "############ READ THE DATASET ############\n",
    "def data_prep(datafile,step=1,k=2):\n",
    "    \"\"\"Prepare dataset, from each of the k fingers for all n surfaces (see fd for details)\n",
    "    -> datafile : input file either in .npz or in .mat form\n",
    "    -> step     : increasing sampling step, decreases sampling frequency of input, which is 1KHz initially\n",
    "    -> k        : number of fingers logging data\n",
    "    ----- input format ----- either 'fi', 'li', 'fdi', with i in {1,...,k} for each finger\n",
    "                             or     'f', 'l', 'fd' for a finger\n",
    "                             corresponding to force, label and details respectively\n",
    "    <- f,l,fd   : output force, label and details for each experiment in the dataset\n",
    "    <- member   : how much each dataset is represented, \n",
    "                  to skip samples effectively and keep dimensions correct\n",
    "    <- m1, m2   : portion of data belonging to finger1 and finger2\n",
    "    \"\"\"\n",
    "    print \"---------------------------- LOADING DATA and COMPUTING NECESSARY STRUCTS ----------------------------\"\n",
    "    if datafile[-3:]=='mat':\n",
    "        inp = sio.loadmat(datafile,struct_as_record=True)\n",
    "    elif datafile[-3:]=='npz':\n",
    "        inp = np.load(datafile)\n",
    "    else:\n",
    "        print \"Unsupported input file format. Supported types: .npz .mat\"\n",
    "        return -1\n",
    "    if k==2:\n",
    "        f1, f2, l1, l2, fd1, fd2 = inp['f1'], inp['f2'], inp['l1'], inp['l2'], inp['fd1'], inp['fd2']\n",
    "        print 1, '-> f1:', f1.shape, l1.shape, fd1.shape\n",
    "        print 2, '-> f2:', f2.shape, l2.shape, fd2.shape\n",
    "        ####### MERGE THE DATASETS\n",
    "        f = np.concatenate((f1,f2),axis=0)\n",
    "        l = np.concatenate((l1,l2),axis=0)\n",
    "        fd = np.concatenate((fd2,fd2),axis=0)\n",
    "    elif k==1:\n",
    "        f, l, fd = inp['f'], inp['l'], inp['fd']\n",
    "    else:\n",
    "        print \"Unsupported number of fingers k. Should be k in {1,2}\"\n",
    "    print 3, '-> f:', f.shape, l.shape, fd.shape\n",
    "    # membership of each sample, representing its portion in the dataset \n",
    "    # (first half finger1 and second half finger2)\n",
    "    member = np.zeros(len(f))\n",
    "    m1,m2 = len(f)/2, len(f)/2\n",
    "    member[:m1] = np.ones(m1)*1./m1\n",
    "    member[-m2:] = np.ones(m2)*1./m2\n",
    "    print 4, '-> m1,m2:', m1, m2, sum(member[:m1]), sum(member[-m2:])\n",
    "    ####### MERGE f and l\n",
    "    while f.ndim>1:\n",
    "        f = f[:,0]\n",
    "        l = l[:,0]\n",
    "    for i in range(len(f)):\n",
    "        while l[i].ndim<2:\n",
    "            l[i] = l[i][:,np.newaxis]\n",
    "    f = np.array([np.concatenate((f[i],l[i]),axis=1) for i in range(len(f))])\n",
    "    print 5, '-> f=f+l:', f.shape, \":\", [fi.shape for fi in f]\n",
    "    ####### SUBSAMPLING\n",
    "    # step = 1 # NO SAMPLING\n",
    "    if step!=1:\n",
    "        f = np.array([fi[::step,:] for fi in f])\n",
    "        print 6, '-> fsampled:',f.shape, \":\", [fi.shape for fi in f]\n",
    "    return f,l,fd,member,m1,m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############ PRE-FEATURES ############\n",
    "###### DEFINITION\n",
    "# featnum 0 : sf    = (fx^2+fy^2+fz^2)^0.5\n",
    "#         1 : ft    = (fx^2+fy^2)^0.5\n",
    "#         2 : fn    = |fz|\n",
    "#         3 : ft/fn = (fx^2+fy^2)^0.5/|fz|\n",
    "# input (nxm) -> keep (nx3) -> compute pre-feature and return (nx1)\n",
    "\n",
    "def sf(f):\n",
    "    \"\"\"Computation of norm (sf) of force (f)\"\"\"\n",
    "    return np.power(np.sum(np.power(f[:,:3],2),axis=1),0.5)\n",
    "def ft(f):\n",
    "    \"\"\"Computation of tangential (ft) of force (f)\"\"\"\n",
    "    return np.power(np.sum(np.power(f[:,:2],2),axis=1),0.5)\n",
    "def fn(f):\n",
    "    \"\"\"Computation of normal (fn) of force (f)\"\"\"\n",
    "    return np.abs(f[:,2])\n",
    "def ftn(f):\n",
    "    \"\"\"Computation of tangential (ft) to normal (fn) ratio of force (f), \n",
    "    corresponding to the friction cone boundary\n",
    "    \"\"\"\n",
    "    retft = ft(f)\n",
    "    retfn = fn(f)\n",
    "    retft[retfn<=1e-2] = 0\n",
    "    return np.divide(retft,retfn+np.finfo(float).eps)\n",
    "def lab(f):\n",
    "    \"\"\"Label embedded in input f\"\"\"\n",
    "    return np.abs(f[:,-1])\n",
    "###### COMPUTATION\n",
    "prefeatfn = np.array([sf,ft,fn,ftn,lab]) # convert to np.array to be easily indexed by a list\n",
    "prefeatnames = np.array(['fnorm','ft','fn','ftdivfn','label'])\n",
    "prefeatid = [0,4]     # only the prefeatures with corresponding ids will be computed\n",
    "def compute_prefeat(f):\n",
    "    \"\"\"Prefeature computation\n",
    "    -> f       : input force as an i by n by 4 matrix\n",
    "    <- prefeat : corresponding force profiles\n",
    "    \"\"\"\n",
    "    print \"--------------------------------------- COMPUTING PREFEATURES ----------------------------------------\"\n",
    "    prefeat = [np.array([prfn(f[i]) for prfn in prefeatfn[prefeatid]]).transpose() for i in range(len(f))]\n",
    "    prefeat.append(prefeat[-1][:-1])\n",
    "    prefeat = np.array(prefeat)[:-1]\n",
    "    print prefeat.shape,\":\",[p.shape for p in prefeat]\n",
    "    return prefeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############ AVG Computation time of ALL features in secs ############\n",
    "def avg_feat_comp_time(prefeat):\n",
    "    \"\"\"Average computation time for feature extraction\n",
    "    -> prefeat : desired prefeature input\n",
    "    \"\"\"\n",
    "    print \"------------------------------------ AVG FEATURE COMPUTATION TIME ------------------------------------\"\n",
    "    t1 = time.time()\n",
    "    m = int(ceil(0.2*len(prefeat)))\n",
    "    # avg over m*100 times\n",
    "    tmpfeat = [feat(prefeat[k][i:i+window,:2],*featparam) for k in range(m) for i in range(100)]\n",
    "    print 'Avg feature computation time (millisec): ', (time.time() - t1) / (100 * m) * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############ FEATURE COMPUTATION ############\n",
    "def tmpfeatfilename(p,name,mode='all'):\n",
    "    \"\"\"Filename for feature computation and intermittent saving\n",
    "    -> p    : prefeat id\n",
    "    -> name : desired prefix name for tmp filenames\n",
    "    -> mode : whether keeping whole feature matrix ('all') or sampling rows ('red') to reduce size\n",
    "    <- corresponding output filename\n",
    "    \"\"\"\n",
    "    allfeatpath = featpath+'AllFeatures/'\n",
    "    ensure_dir(allfeatpath)\n",
    "    if mode == 'all':\n",
    "        return allfeatpath+name+str(p)+'.pkl.z'\n",
    "    elif mode == 'red':\n",
    "        return allfeatpath+name+str(p)+'_red'+str(samplesperdataset)+'.pkl.z'\n",
    "    \n",
    "def feature_extraction(prefeat, member, featfile=featfile, name='feat_'):\n",
    "    \"\"\"Computation of all features in parallel or loading if already computed\n",
    "    -> prefeat          : computed prefeatures\n",
    "    -> member           : how much each dataset is represented, \n",
    "                          to skip samples effectively and keep dimensions correct\n",
    "    -> featfile         : desired final feature filename             \n",
    "    -> name             : desired per dataset feature temporary filenames\n",
    "    <- features, labels : computed features and corresponding labels\n",
    "    \"\"\"\n",
    "    print \"---------------------------------------- FEATURE EXTRACTION ------------------------------------------\"\n",
    "    if os.path.isfile(featfile):\n",
    "        start_time = time.time()\n",
    "        features = np.load(featfile)['features']\n",
    "        labels = np.load(featfile)['labels']\n",
    "        print(\"Features FOUND PRECOMPUTED! Feature Loading DONE in: %s seconds \" % (time.time() - start_time))\n",
    "    else:\n",
    "        start_time = time.time()\n",
    "        features = []\n",
    "        labels = []\n",
    "        for ixp in range(len(prefeat)):\n",
    "            p = prefeat[ixp]\n",
    "            now = time.time()\n",
    "            tmpfn = tmpfeatfilename(ixp,name)\n",
    "            tmpfnred = tmpfeatfilename(ixp,name,'red')\n",
    "            if not os.path.isfile(tmpfnred):\n",
    "                if not os.path.isfile(tmpfn):\n",
    "                    # Computation of all features in PARALLEL by ALL cores\n",
    "                    tmp = np.array([Parallel(n_jobs=-1)([delayed(feat) (p[k:k+window],*featparam) \n",
    "                                                         for k in range(0,len(p)-window,shift)])])\n",
    "                    with open(tmpfn,'wb') as fo:\n",
    "                        joblib.dump(tmp,fo)\n",
    "                    print 'sample:', ixp, ', time(sec):', '{:.2f}'.format(time.time()-now), tmpfn \\\n",
    "                                                        , ' computing... ', tmp.shape\n",
    "                else:\n",
    "                    with open(tmpfn,'rb') as fo:\n",
    "                        tmp = joblib.load(fo)\n",
    "                    print 'sample:', ixp, ', time(sec):', '{:.2f}'.format(time.time()-now), tmpfn \\\n",
    "                                                        , ' already here!', tmp.shape\n",
    "                # keep less from each feature vector but keep number of samples for each dataset almost equal\n",
    "                tmpskip = int(round(tmp.shape[1]/(member[ixp]*samplesperdataset)))\n",
    "                if tmpskip == 0: \n",
    "                    tmpskip = 1\n",
    "                # Save reduced size features\n",
    "                tmp = tmp[0,::tmpskip,:,:]\n",
    "                with open(tmpfnred,'wb') as fo:\n",
    "                    joblib.dump(tmp,fo)\n",
    "                print 'sample:',ixp, ', time(sec):', '{:.2f}'.format(time.time()-now), tmpfnred, tmp.shape\n",
    "        for ixp in range(len(prefeat)):  \n",
    "            tmpfnred = tmpfeatfilename(ixp,name,'red')\n",
    "            with open(tmpfnred,'rb') as fo:\n",
    "                tmp = joblib.load(fo)\n",
    "            print 'sample:', ixp, ', time(sec):', '{:.2f}'.format(time.time()-now), tmpfnred, 'already here!' \\\n",
    "                                                                                  , tmp.shape\n",
    "            features.append(tmp[:,:,:-1])\n",
    "            labels.append(tmp[:,0,-1])\n",
    "        print(\"Features NOT FOUND PRECOMPUTED! Feature Computation DONE in: %s sec \" % (time.time() - start_time))\n",
    "        features.append(tmp[:-1,:,:-1])\n",
    "        features = np.array(features)[:-1]\n",
    "        labels.append(tmp[:-1,0,-1])\n",
    "        labels = np.array(labels)[:-1]\n",
    "        print 'features: ',features.shape,[ftmp.shape for ftmp in features]\n",
    "        print 'labels: ', labels.shape,[l.shape for l in labels]\n",
    "        np.savez(featfile,features=features,labels=labels)\n",
    "    print 'features: ', features.shape, ', labels: ', labels.shape\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############ LABEL TRIMMING ############\n",
    "def label_cleaning(prefeat,labels,member,history=500):\n",
    "    \"\"\"Keep the purely stable and slip parts of label, thus omitting some samples around sign change points\n",
    "    -> prefeat    : computed prefeatures\n",
    "    -> labels     : main structure, where the trimming will be performed around change points\n",
    "    -> member     : how much each dataset is represented, to skip samples effectively and keep dimensions correct\n",
    "    -> history    : how much samples to throw away around change points\n",
    "    <- new_labels : the trimmed labels\n",
    "    \"\"\"\n",
    "    print \"----------- KEEPING LABEL's PURE (STABLE, SLIP) PHASE PARTS (TRIMMING AROUND CHANGE POINTS)-----------\"\n",
    "    lbl_approx = []\n",
    "    for i in range(len(prefeat)):\n",
    "        tmpd = np.abs(np.diff(prefeat[i][:,-1].astype(int),n=1,axis=0))\n",
    "        if np.sum(tmpd) > 0:\n",
    "            tmpind = np.array(range(len(tmpd)))[tmpd > 0]   # find the sign change points\n",
    "            tmpindrng = []\n",
    "            for j in range(len(tmpind)):\n",
    "                length = history                # keep/throw a portion of the signal's length around change points\n",
    "                tmprng = np.array(range(tmpind[j]-length,tmpind[j]+length))\n",
    "                tmprng = tmprng[tmprng>=0]      # make sure inside singal's x-range\n",
    "                tmprng = tmprng[tmprng<prefeat[i].shape[0]]\n",
    "                tmpindrng += tmprng.tolist()\n",
    "            tmpindrng = np.array(tmpindrng).flatten()\n",
    "            tmp_lbl = deepcopy(prefeat[i][:,-1])\n",
    "            tmp_lbl[tmpindrng] = -1\n",
    "            lbl_approx.append(tmp_lbl)\n",
    "        else:\n",
    "            lbl_approx.append(prefeat[i][:,-1])\n",
    "    new_labels = deepcopy(labels)\n",
    "    for ixp in range(len(lbl_approx)):\n",
    "        p = lbl_approx[ixp]\n",
    "        tmp = np.array([p[k+window] for k in range(0,len(p)-window,shift)])\n",
    "        tmpskip = int(round(tmp.shape[0]/(member[ixp]*samplesperdataset)))\n",
    "        if tmpskip == 0: \n",
    "            tmpskip = 1\n",
    "        # Sampling appropriately\n",
    "        tmp = tmp[::tmpskip]\n",
    "        if len(tmp) > len(labels[ixp]):\n",
    "            tmp = tmp[:-1]\n",
    "        new_labels[ixp] = tmp\n",
    "    print 'new_labels: ', new_labels.shape\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############ GATHERING into complete arrays ready for FITTING ############\n",
    "def computeXY(features,labels,new_labels,m1,m2,XYfile=XYfile,XYsplitfile=XYsplitfile):\n",
    "    \"\"\"\n",
    "    -> features       : computed features as input data\n",
    "    -> labels         : corresponding labels\n",
    "    -> new_labels     : labels trimmed around change point\n",
    "    -> m1, m2         : portion of data belonging to finger1 and finger2\n",
    "    -> XY[split]file  : desired output filenames\n",
    "    <- X,Y,Yn,Xsp,Ysp : X corresponds to the data, Y the label, and *sp to the trimmed label's versions\n",
    "    \"\"\"\n",
    "    print \"----------------------------- COMPUTING X,Y for CLASSIFIERS' INPUT -----------------------------------\"\n",
    "    if os.path.isfile(XYfile) and os.path.isfile(XYsplitfile):\n",
    "        X = np.load(XYfile)['X']\n",
    "        Y = np.load(XYfile)['Y']\n",
    "        Yn = np.load(XYfile)['Yn']\n",
    "        Xsp = np.load(XYsplitfile)['X']\n",
    "        Ysp = np.load(XYsplitfile)['Y']\n",
    "        print(\"XY files FOUND PRECOMPUTED!\")\n",
    "    else:\n",
    "        # gathering features X,Xsp and labels Y,Ysp,Yn into one array each\n",
    "        ind,X,Xsp,Y,Ysp,Yn = {},{},{},{},{},{}\n",
    "        ind[2] = range(features.shape[0])                                      # indeces for both fingers\n",
    "        ind[0] = range(features.shape[0])[:m1]                                 # indeces for finger1\n",
    "        ind[1] = range(features.shape[0])[-m2:]                                # indeces for finger2\n",
    "        ind = np.array([i for _,i in ind.items()])                             # convert to array\n",
    "        for k in range(len(ind)):\n",
    "            X[k] = features[ind[k]]                                            # input feature matrix\n",
    "            Y[k] = labels[ind[k]]                                              # output label vector\n",
    "            Yn[k] = new_labels[ind[k]]                                         # output new_label vector\n",
    "            print 'Before -> X[',k,']: ',X[k].shape,', Y[',k,']: ',Y[k].shape,', Yn[',k,']: ',Yn[k].shape\n",
    "            X[k] = np.concatenate(X[k],axis=0)\n",
    "            Y[k] = np.concatenate(Y[k],axis=0)\n",
    "            Yn[k] = np.concatenate(Yn[k],axis=0)\n",
    "            print 'Gathered -> X[',k,']: ',X[k].shape,', Y[',k,']: ',Y[k].shape,', Yn[',k,']: ',Yn[k].shape\n",
    "            X[k] = np.array([X[k][:,:,i] for i in range(X[k].shape[2])])\n",
    "            tmp_sampling = int(round(X[k].shape[1]*1./samplesperdataset))\n",
    "            if tmp_sampling == 0:\n",
    "                tmp_sampling = 1\n",
    "            X[k] = X[k][0,::tmp_sampling,:]\n",
    "            Y[k] = Y[k][::tmp_sampling]\n",
    "            Yn[k] = Yn[k][::tmp_sampling]\n",
    "            print 'Gathered, sampled to max ', samplesperdataset, ' -> X[', k,']: ', X[k].shape, ', Y[', k \\\n",
    "                                             , ']: ', Y[k].shape, ', Yn[', k,']: ', Yn[k].shape\n",
    "            keepind = Yn[k]>=0\n",
    "            Xsp[k] = X[k][keepind,:]\n",
    "            Ysp[k] = Yn[k][keepind]\n",
    "            print 'Split -> Xsp[',k,']: ',Xsp[k].shape,', Ysp[',k,']: ',Ysp[k].shape\n",
    "        X = np.array([i for _,i in X.items()])\n",
    "        Xsp = np.array([i for _,i in Xsp.items()])\n",
    "        Y = np.array([i for _,i in Y.items()])\n",
    "        Ysp = np.array([i for _,i in Ysp.items()])\n",
    "        Yn = np.array([i for _,i in Yn.items()])\n",
    "        np.savez(XYfile,X=X,Y=Y,Yn=Yn)\n",
    "        np.savez(XYsplitfile, X=Xsp, Y=Ysp)\n",
    "    print 'X,Y [0,1,2]: ', X[0].shape, Y[0].shape, X[1].shape, Y[1].shape, X[2].shape, Y[2].shape\n",
    "    print 'Xsp,Ysp [0,1,2]: ', Xsp[0].shape, Ysp[0].shape, Xsp[1].shape, Ysp[1].shape, Xsp[2].shape, Ysp[2].shape\n",
    "    return X,Y,Yn,Xsp,Ysp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############ Prepare the indeces for each feature ############\n",
    "def get_feat_id(feat_ind, printit=0, sample_window=window): \n",
    "    \"\"\"Find the corresponding indeces of the desired features inside feature vector,\n",
    "    and link them with their names and level of abstraction\n",
    "    -> feat_ind        : range of indeces\n",
    "    -> printit         : print output indeces (1) or not (0)\n",
    "    -> sample_window   : parameter for accurate computation of feature indeces\n",
    "    <- full_path_id    : indeces of all features\n",
    "    <- norm_time_feats : indeces of time features\n",
    "    <- norm_freq_feats : indeces of frequency features\n",
    "    \"\"\"\n",
    "    # get the feat inds wrt their source : 3rd level\n",
    "    norm_time_phin = range(0,14)\n",
    "    norm_freq_phin = range(norm_time_phin[-1] + 1, norm_time_phin[-1] + 9 + sample_window + 1)\n",
    "    norm_time_golz = range(norm_freq_phin[-1] + 1, norm_freq_phin[-1] + 10 + sample_window + 1)\n",
    "    norm_freq_golz = range(norm_time_golz[-1] + 1, norm_time_golz[-1] + 2 + sample_window + 1)\n",
    "    # get the feat inds wrt their domain : 2nd level \n",
    "    norm_time_feats = norm_time_phin + norm_time_golz\n",
    "    norm_freq_feats = norm_freq_phin + norm_freq_golz\n",
    "    # get the feat inds wrt their prefeat: 1st level \n",
    "    norm_feats = norm_time_feats + norm_freq_feats\n",
    "\n",
    "    # get the feat inds wrt their source : 3rd level\n",
    "    disp = norm_feats[-1]+1\n",
    "    ftfn_time_phin = range(disp ,disp + 14)\n",
    "    ftfn_freq_phin = range(ftfn_time_phin[-1] + 1, ftfn_time_phin[-1] + 9 + sample_window + 1)\n",
    "    ftfn_time_golz = range(ftfn_freq_phin[-1] + 1, ftfn_freq_phin[-1] + 10 + sample_window + 1)\n",
    "    ftfn_freq_golz = range(ftfn_time_golz[-1] + 1, ftfn_time_golz[-1] + 2 + sample_window + 1)\n",
    "    # get the feat inds wrt their domain : 2nd level \n",
    "    ftfn_time_feats = ftfn_time_phin + ftfn_time_golz\n",
    "    ftfn_freq_feats = ftfn_freq_phin + ftfn_freq_golz\n",
    "    # get the feat inds wrt their prefeat: 1st level \n",
    "    ftfn_feats = ftfn_time_feats + ftfn_freq_feats\n",
    "\n",
    "    # create the final \"reference dictionary\"\n",
    "    # 3 np.arrays, id_list[0] = level 1 etc\n",
    "    id_list = [np.zeros((len(ftfn_feats + norm_feats),1)) for i in range(3)]\n",
    "    id_list[0][:norm_feats[-1]+1] = 0 # 0 signifies norm / 1 signifies ft/fn\n",
    "    id_list[0][norm_feats[-1]+1:] = 1\n",
    "\n",
    "    id_list[1][:norm_time_phin[-1]+1] = 0 # 0 signifies time / 1 signifies freq\n",
    "    id_list[1][norm_time_phin[-1]+1:norm_freq_phin[-1]+1] = 1\n",
    "    id_list[1][norm_freq_phin[-1]+1:norm_time_golz[-1]+1] = 0\n",
    "    id_list[1][norm_time_golz[-1]+1:norm_freq_golz[-1]+1] = 1\n",
    "    id_list[1][norm_freq_golz[-1]+1:ftfn_time_phin[-1]+1] = 0\n",
    "    id_list[1][ftfn_time_phin[-1]+1:ftfn_freq_phin[-1]+1] = 1\n",
    "    id_list[1][ftfn_freq_phin[-1]+1:ftfn_time_golz[-1]+1] = 0\n",
    "    id_list[1][ftfn_time_golz[-1]+1:] = 1\n",
    "\n",
    "    id_list[2][:norm_freq_phin[-1]+1] = 0 #0 signifies phinyomark / 1 signifies golz\n",
    "    id_list[2][norm_freq_phin[-1]+1:norm_freq_golz[-1]+1] = 1\n",
    "    id_list[2][norm_freq_golz[-1]+1:ftfn_freq_phin[-1]+1] = 0\n",
    "    id_list[2][ftfn_freq_phin[-1]+1:] = 1 \n",
    "    \n",
    "    full_path_id = [np.zeros((len(feat_ind),5)) for i in range(len(feat_ind))]\n",
    "   \n",
    "    for ind, val in enumerate(feat_ind):\n",
    "        full_path_id[ind] = [val, id_list[2][val], id_list[1][val], id_list[0][val]]\n",
    "        if (printit==1):\n",
    "            if(full_path_id[ind][1]==0):\n",
    "                lvl3 = 'Phin'\n",
    "            else:\n",
    "                lvl3 = 'Golz'\n",
    "            if(full_path_id[ind][2]==0):\n",
    "                lvl2 = 'Time'\n",
    "            else:\n",
    "                lvl2 = 'Freq'\n",
    "            if(full_path_id[ind][3]==0):\n",
    "                lvl1 = 'Norm'\n",
    "            else:\n",
    "                lvl1 = 'Ft/Fn'\n",
    "            print(feat_ind[ind],featnames[val%(norm_feats[-1]+1)],lvl3,lvl2,lvl1)\n",
    "    \n",
    "    return(full_path_id,norm_time_feats,norm_freq_feats)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############ Surface Splitting ############\n",
    "def surface_split(data_X, data_Y, n=6, k=2):\n",
    "    \"\"\"Split input data in k*n equal slices which represent n different surfaces sampled from k fingers.\n",
    "    Indexes 0:n:(k-1)*n, 1:n:(k-1)*n+1, 2:n:(k-1)*n+2, ... correspond to the same surface (finger1 upto fingerk)\n",
    "    Assuming k=2, namely 2 fingers case, unless stated differently\n",
    "    -> data_X, data_Y        : input data and labels, with the convention that data_X contains k*n almost \n",
    "                               equally sized data, where the n first are acquired from finger1 ... \n",
    "                               and the n last from fingerk. \n",
    "    -> n                     : number of different surfaces\n",
    "    -> k                     : number of fingers logging data\n",
    "    <- surfaces, surf_labels : corresponding output data and labels\n",
    "    \"\"\"\n",
    "    keep = data_X.shape[0]-np.mod(data_X.shape[0],k*n)\n",
    "    surfaces_pre = np.array(np.split(data_X[:keep,:],k*n))\n",
    "    surf_labels_pre = np.array(np.split(data_Y[:keep],k*n))\n",
    "    surfaces, surf_labels = {},{}\n",
    "    for i in range(n):\n",
    "        inds = range(i,k*n,n)\n",
    "        surfaces[inds[0]] = np.concatenate((surfaces_pre[inds[0]], surfaces_pre[inds[1]]), axis = 0)\n",
    "        surf_labels[inds[0]] = np.concatenate((surf_labels_pre[inds[0]], surf_labels_pre[inds[1]]), axis = 0)\n",
    "    surfaces = np.array([i for _,i in surfaces.items()])\n",
    "    surf_labels = np.array([i for _,i in surf_labels.items()])\n",
    "    return surfaces, surf_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############ Featureset Splitting ############\n",
    "subfeats = ['AFFT','FREQ','TIME','BOTH']\n",
    "def feat_subsets(data,fs_ind,ofs=len(featnames)):\n",
    "    \"\"\"returns a splitting per featureset of input features\n",
    "    -> data                                : input data X\n",
    "    -> fs_ind                              : prefeature id\n",
    "    -> ofs                                 : number of features in total\n",
    "    <- X_amfft, X_freq_all, X_time, X_both : split featuresets amplitude of FFT, all time only,\n",
    "                                                               all frequency only and all features\n",
    "    \"\"\"\n",
    "    _,tf,ff = get_feat_id(range(ofs))\n",
    "    amfft_inds = []\n",
    "    temp1 = deepcopy(data)\n",
    "    \n",
    "    for i in range(len(featnames)):\n",
    "        if (featnames[i].startswith('amFFT')):\n",
    "            amfft_inds.append(i)\n",
    "\n",
    "    if (fs_ind == 2):\n",
    "        ff2 = [ff[i]+ofs for i in range(len(ff))]\n",
    "        tf2 = [tf[i]+ofs for i in range(len(tf))]\n",
    "        amfft2 = [amfft_inds[i]+ofs for i in range(len(amfft_inds))]\n",
    "        freqf = ff2 + ff\n",
    "        timef = tf2 + tf\n",
    "        amfft = amfft_inds + amfft2\n",
    "    else:\n",
    "        freqf = ff\n",
    "        timef = tf\n",
    "        amfft = amfft_inds\n",
    "\n",
    "    X_amfft = temp1[:,amfft]\n",
    "    X_time = np.delete(temp1,freqf,axis=1)\n",
    "    X_freq_all = np.delete(temp1,timef,axis=1)\n",
    "    X_both = data\n",
    "    return X_amfft, X_freq_all, X_time, X_both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############ Prepare the dataset split for each surface ############\n",
    "def computeXY_persurf(Xsp,Ysp,surffile=surffile):\n",
    "    \"\"\"returns a split per surface data and label of inputs\n",
    "    -> Xsp, Ysp     : input data and labels, after having trimmed data around the label's change points\n",
    "    -> surffile     : desired output's filename for saving\n",
    "    <- surf, surfla : output data and label, split per surface\n",
    "    \"\"\"\n",
    "    print \"------------------------ COMPUTING X,Y per surface CLASSIFIERS' INPUT --------------------------------\"\n",
    "    if os.path.isfile(surffile):\n",
    "        surf = np.load(surffile)['surf']       # input array containing computed features for each surface\n",
    "        surfla = np.load(surffile)['surfla']   # corresponding label\n",
    "    else:\n",
    "        surf, surfla = [], []\n",
    "        for i in range(len(prefeatid)-1): # for each featureset (corresponding to each prefeature, here only |f|)\n",
    "            surf1, surfla1 = surface_split(Xsp[2], Ysp[2])\n",
    "            tmpsurf = deepcopy(surf1)\n",
    "            tmpsurfla = deepcopy(surfla1)\n",
    "            tmpsurfsubfeat = []\n",
    "            for j in range(tmpsurf.shape[0]+1): # for each surface\n",
    "                print i,j,surf1.shape\n",
    "                if j == tmpsurf.shape[0]:\n",
    "                    # ommit a sample for converting to array\n",
    "                    tmpsurfsubfeat.append(feat_subsets(tmpsurf[j-1,:-1,:],i))\n",
    "                else:\n",
    "                    # keep all subfeaturesets\n",
    "                    tmpsurfsubfeat.append(feat_subsets(tmpsurf[j],i))\n",
    "            surf.append(tmpsurfsubfeat)\n",
    "            surfla.append(surfla1)\n",
    "        # surf dims: (featuresets, surfaces, prefeaturesets) with each one enclosing (samples, features)\n",
    "        surf = np.array(surf).transpose()[:,:-1,:]\n",
    "        # surfla dims: (samples, surfaces, prefeaturesets)\n",
    "        surfla = np.array(surfla).transpose()\n",
    "        np.savez(surffile,surf=surf,surfla=surfla)\n",
    "    print surf.shape, surfla.shape\n",
    "    return surf, surfla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############ PIPELINE OF TRANSFORMATIONS ############\n",
    "def make_pipe_clf(scaler,feature_selection,decomp,clf):\n",
    "    \"\"\"returns a pipeline of inputs: \n",
    "    -> scaler            : first normalize\n",
    "    -> feature_selection : then perform feature selection\n",
    "    -> decomp            : followed by PCA \n",
    "    -> clf               : and finally the desired classifier\n",
    "    <- pipeline          : output pipeline\n",
    "    \"\"\"\n",
    "    pipeline = Pipeline([('scaler', scaler),\n",
    "                         ('feature_selection', feature_selection),\n",
    "                         ('decomp', decomp),         \n",
    "                         ('classifier', clf) ])\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def comb(n,r):\n",
    "    \"\"\"Combinations of n objects by r, namely picking r among n possible.\n",
    "    comb(n,r) = n!/(r!(n-r)!)\n",
    "    \"\"\"\n",
    "    return math.factorial(n)/(math.factorial(r)*math.factorial(n-r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############ TRAINING with 1 surface each time, out of 6 surfaces in total ##############\n",
    "def filename1(i,j,k,l,retpath=0):\n",
    "    \"\"\"function for the filename of the selected combination for training per 1 surface\n",
    "    -> i : prefeature id, among all computed prefeatures (0: |f|, ... see prefeatid)\n",
    "    -> j : subfeatureset among all features (0: AFFT, 1: FREQ, 2: TIME, 3: ALL)\n",
    "    -> k : surface id trained on\n",
    "    -> l : surface id tested on\n",
    "    <- filename\n",
    "    \"\"\"\n",
    "    filepath = respath+'1/'\n",
    "    ensure_dir(filepath)\n",
    "    if retpath:\n",
    "        return filepath\n",
    "    else:\n",
    "        return filepath+'fs_'+str(i)+'_subfs_'+str(j)+'_tr_'+str(k)+'_ts_'+str(l)+'.npz'\n",
    "\n",
    "def cross_fit1(i,j,k,kmax,l,data,labels,data2,labels2,pipe):\n",
    "    \"\"\"function for fitting model per 1 surface\n",
    "    -> i              : prefeature id, among all computed prefeatures (0: |f|, ... see prefeatid)\n",
    "    -> j              : subfeatureset among all features (0: AFFT, 1: FREQ, 2: TIME, 3: ALL)\n",
    "    -> k              : surface id trained on\n",
    "    -> kmax           : maximum surfaces\n",
    "    -> l              : surface id tested on\n",
    "    -> data, labels   : training data and labels\n",
    "    -> data2, labels2 : testing data and labels\n",
    "    -> pipe           : the desired pipeline configuration\n",
    "    <- no output, saved model and confusion matrix in corresponding filename.npz\n",
    "    \"\"\"\n",
    "    fileid = filename1(i,j,k,l)\n",
    "    if not os.path.isfile(fileid):\n",
    "        print i,j,k,l\n",
    "        if k==l: # perform K-fold cross-validation       \n",
    "            folds = cv.split(data, labels)\n",
    "            cm_all = np.zeros((2,2))\n",
    "            for fold, (train_ind, test_ind) in enumerate(folds):\n",
    "                x_train, x_test = data[train_ind], data[test_ind]\n",
    "                y_train, y_test = labels[train_ind], labels[test_ind]\n",
    "                model = pipe.fit(x_train,y_train)\n",
    "                y_pred = model.predict(x_test)\n",
    "                cm = confusion_matrix(y_pred=y_pred, y_true=y_test)\n",
    "                cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "                cm_all += cm/5.\n",
    "            np.savez(fileid,cm=cm_all,model=np.array([model]))\n",
    "        else: # perform cross-check\n",
    "            tr_data = data\n",
    "            tr_labels = labels\n",
    "            ts_data = data2\n",
    "            ts_labels = labels2\n",
    "            # Check if model already existent, but not the cross-validated one (on the same surface)\n",
    "            model = []\n",
    "            for m in range(kmax):\n",
    "                tmpcopyfileid = filepath+filename1(i,j,k,m)+'.npz'\n",
    "                if k!=m and os.path.isfile(tmpcopyfileid):\n",
    "                    print 'Found precomputed model of '+str(k)+', tested on '+str(m)+'. Testing on '+str(l)+'...'\n",
    "                    model = np.load(tmpcopyfileid)['model'][0]\n",
    "                    break\n",
    "            if model==[]: # model not found precomputed\n",
    "                print 'Fitting on '+str(k)+', testing on '+str(l)+'...'\n",
    "                model = pipe.fit(tr_data,tr_labels)\n",
    "            y_pred = model.predict(ts_data)\n",
    "            cm = confusion_matrix(y_pred=y_pred, y_true=ts_labels)\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            np.savez(fileid,cm=cm,model=np.array([model]))\n",
    "\n",
    "def init_steps1(i,j,jmax,surf,surfla):\n",
    "    \"\"\"function for helping parallelization of computations per 1 surface\n",
    "    -> i              : prefeature id, among all computed prefeatures (0: |f|, ... see prefeatid)\n",
    "    -> j              : subfeatureset among all features (0: AFFT, 1: FREQ, 2: TIME, 3: ALL)\n",
    "    -> jmax           : number of all subfeaturesets\n",
    "    -> surf, surfla   : surface data and labels\n",
    "    \"\"\"\n",
    "    if j==jmax:\n",
    "        featsel = SelectKBest(k=1000,score_func= mutual_info_classif)\n",
    "    else:\n",
    "        featsel = SelectKBest(k='all',score_func= mutual_info_classif)\n",
    "    pipe = make_pipe_clf(scaler, featsel, decomp, classifiers[2])\n",
    "    for k in range(surf.shape[0]): # for every training surface\n",
    "        for l in range(surf.shape[0]): # for every testing surface\n",
    "            cross_fit1(i,j,k,surf.shape[0],l,surf[k],surfla[:,k],surf[l],surfla[:,l],pipe)\n",
    "\n",
    "def train_1_surface(surf,surfla,n=-1):\n",
    "    \"\"\"Parallel training -on surface level- of all combinations on 1 surface\n",
    "    -> n              : number of cores to run in parallel, \n",
    "                        input of joblib's Parallel (n=-1 means all available cores)\n",
    "    -> surf, surfla   : surface data and labels\n",
    "    *** Cross surface validation, TRAINING with 1 surface each time, out of 6 surfaces in total\n",
    "    total= 4 (featuresets) * [comb(6,1)*6] (surface combinations: trained on 1, tested on 1) * 1 (prefeatureset)\n",
    "         = 4*6*6*1 = 144 different runs-files.\n",
    "    Note that comb(n,r) = n!/(r!(n-r)!)\n",
    "    \"\"\"\n",
    "    print \"-------------------------- TRAINING all combinations per 1 surface -----------------------------------\"\n",
    "    for i in range(len(prefeatid)-1):\n",
    "        _ = [Parallel(n_jobs=n)([delayed(init_steps1) (i,j,surf.shape[0]-1,surf[j,:,i],surfla[:,:,i]) \n",
    "                                  for j in range(surf.shape[0])])]\n",
    "        \n",
    "def bargraph_file_gen1(maxsurf):\n",
    "    prefeats = prefeatnames[prefeatid][:-1]\n",
    "    # prefeatures, subfeatures, trained, tested, (TP,TN,FN,FP)\n",
    "    acc = np.zeros((len(prefeats),len(subfeats),maxsurf,maxsurf,4))\n",
    "    # prefeatures, subfeatures, trained, cross_val_self_accuracy, (TP,TN,FN,FP)\n",
    "    self_acc = np.zeros((len(prefeats),len(subfeats),maxsurf,1,4))\n",
    "    # features, subfeatures, trained, (tested avg, tested std), (TP,TN,FN,FP)\n",
    "    cross_acc = np.zeros((len(prefeats),len(subfeats),maxsurf,2,4))    \n",
    "    initial_str = \"\"\"# clustered and stacked graph data\n",
    "=stackcluster;TP;TN;FN;FP\n",
    "colors=med_blue,dark_green,yellow,red\n",
    "=nogridy\n",
    "=noupperright\n",
    "fontsz=3\n",
    "legendx=right\n",
    "legendy=center\n",
    "datascale=50\n",
    "yformat=%g%%\n",
    "xlabel=TrainedON-TestedON\n",
    "ylabel=Metrics\n",
    "=table\"\"\"\n",
    "    respath = filename1(_,_,_,_,1)\n",
    "    for i in range(len(prefeats)):\n",
    "        outname = respath+prefeats[i]\n",
    "        outfile = outname+'.perf'\n",
    "        outfile1 = outname+'_selfaccuracy.perf'\n",
    "        outfile2 = outname+'_crossaccuracy.perf'\n",
    "        out = open(outfile,'w+')\n",
    "        out.write(initial_str+\"\\n\")\n",
    "        out1 = open(outfile1,'w+')\n",
    "        out1.write(initial_str+\"\\n\")\n",
    "        out2 = open(outfile2,'w+')\n",
    "        out2.write(initial_str+\"\\n\")\n",
    "        for k in range(maxsurf):\n",
    "            for k2 in range(maxsurf):\n",
    "                out.write(\"multimulti=\"+str(k)+\"-\"+str(k2)+\"\\n\")\n",
    "                for j in range(len(subfeats)):\n",
    "                    fileid = filename1(i,j,k,k2)\n",
    "                    tmp = np.load(fileid)['cm']\n",
    "                    # print to outfile\n",
    "                    acc[i,j,k,k2,0] = round(tmp[1,1],2) # TP\n",
    "                    acc[i,j,k,k2,1] = round(tmp[0,0],2) # TN\n",
    "                    acc[i,j,k,k2,2] = 1-round(tmp[1,1],2) # FN\n",
    "                    acc[i,j,k,k2,3] = 1-round(tmp[0,0],2) # FP\n",
    "                    out.write(\"%s %.2f %.2f %.2f %.2f\\n\" % (subfeats[j],acc[i,j,k,k2,0],acc[i,j,k,k2,1],\n",
    "                                                                        acc[i,j,k,k2,2],acc[i,j,k,k2,3]))\n",
    "                    # prepare and print to outfile1\n",
    "                    if k == k2:\n",
    "                        if j == 0:\n",
    "                            out1.write(\"multimulti=\"+str(k)+\"-\"+str(k2)+\"\\n\")\n",
    "                        self_acc[i,j,k,0,:] = acc[i,j,k,k2,:]\n",
    "                        out1.write(\"%s %.2f %.2f %.2f %.2f\\n\" % (subfeats[j],self_acc[i,j,k,0,0],\n",
    "                                                                 self_acc[i,j,k,0,1],self_acc[i,j,k,0,2],\n",
    "                                                                 self_acc[i,j,k,0,3]))\n",
    "                    # prepare and print to outfile2\n",
    "                    if k != k2:\n",
    "                        # all values of corresponding subfeatureset j have been filled to compute avg and std\n",
    "                        if (k < maxsurf-1 and k2 == maxsurf-1) or (k == maxsurf-1 and k2 == maxsurf-2):\n",
    "                            if j == 0:\n",
    "                                out2.write(\"multimulti=\"+str(k)+\"\\n\")\n",
    "                            t = range(maxsurf)\n",
    "                            t.remove(k)\n",
    "                            cross_acc[i,j,k,0,:] = np.mean(acc[i,j,k,t,:], axis=0) # avg\n",
    "                            # cross_acc[i,j,k,1,:] = np.std(acc[i,j,k,t,:], axis=0) # std\n",
    "                            out2.write(\"%s %.2f %.2f %.2f %.2f\\n\" % (subfeats[j], cross_acc[i,j,k,0,0],\n",
    "                                                                     cross_acc[i,j,k,0,1], cross_acc[i,j,k,0,2],\n",
    "                                                                     cross_acc[i,j,k,0,3]))\n",
    "        out.write(\"multimulti=AVG\\n\")\n",
    "        out1.write(\"multimulti=AVG\\n\")\n",
    "        out2.write(\"multimulti=AVG\\n\")\n",
    "        for j in range(4):\n",
    "            avgacc = np.mean(np.mean(acc[i,j,:,:,:], axis=0), axis=0)\n",
    "            out.write(\"%s %.2f %.2f %.2f %.2f\\n\" % (subfeats[j], avgacc[0], avgacc[1], avgacc[2], avgacc[3]))\n",
    "            avgselfacc = np.mean(self_acc[i,j,:,0,:], axis=0)\n",
    "            out1.write(\"%s %.2f %.2f %.2f %.2f\\n\" % (subfeats[j], avgselfacc[0], avgselfacc[1],\n",
    "                                                                  avgselfacc[2], avgselfacc[3]))\n",
    "            avgcrossacc0 = np.mean(cross_acc[i,j,:,0,:], axis=0)\n",
    "            # avgcrossacc1 = np.std(cross_acc[i,j,:,0,:], axis=0)\n",
    "            out2.write(\"%s %.2f %.2f %.2f %.2f\\n\" % (subfeats[j], avgcrossacc0[0], avgcrossacc0[1],\n",
    "                                                                  avgcrossacc0[2], avgcrossacc0[3]))\n",
    "        out.close()\n",
    "        out1.close()\n",
    "        out2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ TRAINING with 2 surfaces each time, out of 6 surfaces in total ##############\n",
    "def filename2(i,j,k1,k2,l,retpath=0):\n",
    "    \"\"\"function for the filename of the selected combination for training per 2 surfaces\n",
    "    -> i  : prefeature id, among all computed prefeatures (0: |f|, ... see prefeatid)\n",
    "    -> j  : subfeatureset among all features (0: AFFT, 1: FREQ, 2: TIME, 3: ALL)\n",
    "    -> ki : surface ids trained on\n",
    "    -> l  : surface id tested on\n",
    "    <- filename\n",
    "    \"\"\"\n",
    "    filepath = respath+'2/'\n",
    "    ensure_dir(filepath)\n",
    "    if retpath:\n",
    "        return filepath\n",
    "    else:\n",
    "        return filepath+'fs_'+str(i)+'_subfs_'+str(j)+'_tr1_'+str(k1)+'_tr2_'+str(k2)+'_ts_'+str(l)+'.npz'\n",
    "\n",
    "def cross_fit2(i,j,k1,k2,kmax,l,data,labels,data2,labels2,pipe):\n",
    "    \"\"\"function for fitting model per 2 surfaces\n",
    "    -> i              : prefeature id, among all computed prefeatures (0: |f|, ... see prefeatid)\n",
    "    -> j              : subfeatureset among all features (0: AFFT, 1: FREQ, 2: TIME, 3: ALL)\n",
    "    -> ki             : surface ids trained on\n",
    "    -> kmax           : maximum surfaces\n",
    "    -> l              : surface id tested on\n",
    "    -> data, labels   : training data and labels\n",
    "    -> data2, labels2 : testing data and labels\n",
    "    -> pipe           : the desired pipeline configuration\n",
    "    <- no output, saved model and confusion matrix in corresponding filename.npz\n",
    "    \"\"\"\n",
    "    fileid = filename2(i,j,k1,k2,l)\n",
    "    if not os.path.isfile(fileid):\n",
    "        print i,j,k1,k2,l\n",
    "        if k1==l or k2==l: # perform K-fold      \n",
    "            print 'Fitting on '+str(k1)+\"-\"+str(k2)+', cross-validating on '+str(l)+'...'\n",
    "            if l == k1: # copy if existent from the other sibling file\n",
    "                tmpcopyfileid = filepath+filename2(i,j,k1,k2,k2)+'.npz'\n",
    "            else:   # same as above\n",
    "                tmpcopyfileid = filepath+filename2(i,j,k1,k2,k1)+'.npz'                \n",
    "            if not os.path.isfile(tmpcopyfileid):\n",
    "                folds = cv.split(data, labels)\n",
    "                cm_all = np.zeros((2,2))\n",
    "                for fold, (train_ind, test_ind) in enumerate(folds):\n",
    "                    x_train, x_test = data[train_ind], data[test_ind]\n",
    "                    y_train, y_test = labels[train_ind], labels[test_ind]\n",
    "                    model = pipe.fit(x_train,y_train)\n",
    "                    y_pred = model.predict(x_test)\n",
    "                    cm = confusion_matrix(y_pred=y_pred, y_true=y_test)\n",
    "                    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "                    cm_all += cm/5.\n",
    "            else:\n",
    "                cm_all = np.load(tmpcopyfileid)['cm']\n",
    "                model = np.load(tmpcopyfileid)['model'][0]\n",
    "            np.savez(fileid,cm=cm_all,model=np.array([model]))\n",
    "        else: # perform cross-check\n",
    "            tr_data = data\n",
    "            tr_labels = labels\n",
    "            ts_data = data2\n",
    "            ts_labels = labels2\n",
    "            model = []\n",
    "            for m in range(kmax):\n",
    "                tmpcopyfileid = filepath+filename2(i,j,k1,k2,m)+'.npz'\n",
    "                if k1!=m and k2!=m and os.path.isfile(tmpcopyfileid):\n",
    "                    print 'Found precomputed model of '+str(k1)+str(k2)+', tested on '+str(m) \\\n",
    "                                                                       +'. Testing on '+str(l)+'...'\n",
    "                    model = np.load(tmpcopyfileid)['model'][0]\n",
    "                    break\n",
    "            if model==[]: # model not found precomputed\n",
    "                print 'Fitting on '+str(k1)+\"-\"+str(k2)+', testing on '+str(l)+'...'\n",
    "                model = pipe.fit(tr_data,tr_labels)\n",
    "            y_pred = model.predict(ts_data)\n",
    "            cm = confusion_matrix(y_pred=y_pred, y_true=ts_labels)\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            np.savez(fileid,cm=cm,model=np.array([model]))\n",
    "\n",
    "def init_steps2(i,j,jmax,surf,surfla):\n",
    "    \"\"\"function for helping parallelization of computations per 2 surfaces\n",
    "    -> i              : prefeature id, among all computed prefeatures (0: |f|, ... see prefeatid)\n",
    "    -> j              : subfeatureset among all features (0: AFFT, 1: FREQ, 2: TIME, 3: ALL)\n",
    "    -> jmax           : number of all subfeaturesets\n",
    "    -> surf, surfla   : surface data and labels\n",
    "    \"\"\"\n",
    "    if j==jmax:\n",
    "        featsel = SelectKBest(k=1000,score_func= mutual_info_classif)\n",
    "    else:\n",
    "        featsel = SelectKBest(k='all',score_func= mutual_info_classif)\n",
    "    pipe = make_pipe_clf(scaler, featsel, decomp, classifiers[2])\n",
    "    for k1 in range(surf.shape[0]): # for every training surface1\n",
    "        for k2 in range(surf.shape[0]): # for every training surface2\n",
    "            if k2 > k1:\n",
    "                for l in range(surf.shape[0]): # for every testing surface\n",
    "                    tr_surf = np.concatenate((surf[k1],surf[k2]),axis=0)\n",
    "                    tr_surfla = np.concatenate((surfla[:,k1],surfla[:,k2]),axis=0)\n",
    "                    ts_surf, ts_surfla = surf[l], surfla[:,l]\n",
    "                    cross_fit2(i,j,k1,k2,surf.shape[0],l,tr_surf,tr_surfla,ts_surf,ts_surfla,pipe)\n",
    "\n",
    "def train_2_surface(surf,surfla,n=-1):\n",
    "    \"\"\"Parallel training -on surface level- of all combinations on 2 surfaces\n",
    "    -> n              : number of cores to run in parallel, \n",
    "                        input of joblib's Parallel (n=-1 means all available cores)\n",
    "    -> surf, surfla   : surface data and labels\n",
    "    *** Cross surface validation, TRAINING with 2 surfaces each time, out of 6 surfaces in total\n",
    "    total= 4 (featuresets) * [comb(6,2)*6] (surface combinations: trained on 2, tested on 1) * 1 (prefeatureset)\n",
    "         = 4*15*6*1 = 360 different runs-files.\n",
    "    Note that comb(n,r) = n!/(r!(n-r)!)\n",
    "    \"\"\"\n",
    "    print \"-------------------------- TRAINING all combinations per 2 surfaces ----------------------------------\"    \n",
    "    for i in range(len(prefeatid)-1):\n",
    "        _ = [Parallel(n_jobs=n)([delayed(init_steps2) (i,j,surf.shape[0]-1,surf[j,:,i],surfla[:,:,i]) \n",
    "                                 for j in range(surf.shape[0])])]\n",
    "\n",
    "def bargraph_file_gen2(maxsurf):\n",
    "    prefeats = prefeatnames[prefeatid][:-1]\n",
    "    # prefeatures, subfeatures, trained, tested, (TP,TN,FN,FP)\n",
    "    acc = np.zeros((len(prefeats),len(subfeats),maxsurf,maxsurf,maxsurf,4))\n",
    "    # features, subfeatures, (TP,TN,FN,FP) -> avg over all tested surfaces\n",
    "    avg = np.zeros((len(prefeats),len(subfeats),4))\n",
    "    # prefeatures, subfeatures, trained, cross_val_self_accuracy, (TP,TN,FN,FP)\n",
    "    self_acc = np.zeros((len(prefeats),len(subfeats),maxsurf,maxsurf,1,4))\n",
    "    # features, subfeatures, (TP,TN,FN,FP) -> avg over all self tested surfaces\n",
    "    avgs = np.zeros((len(prefeats),len(subfeats),4))\n",
    "    # features, subfeatures, trained, (tested avg, tested std), (TP,TN,FN,FP)\n",
    "    cross_acc = np.zeros((len(prefeats),len(subfeats),maxsurf,maxsurf,2,4))\n",
    "     # features, subfeatures, (TP,TN,FN,FP) -> avg over all cross tested surfaces\n",
    "    avgc = np.zeros((len(prefeats),len(subfeats),4))\n",
    "    initial_str = \"\"\"# clustered and stacked graph bogus data\n",
    "=stackcluster;TP;TN;FN;FP\n",
    "colors=med_blue,dark_green,yellow,red\n",
    "=nogridy\n",
    "=noupperright\n",
    "fontsz=2\n",
    "legendx=right\n",
    "legendy=center\n",
    "datascale=50\n",
    "yformat=%g%%\n",
    "xlabel=TrainedON-TestedON\n",
    "ylabel=Metrics\n",
    "=table\"\"\"\n",
    "    respath = filename2(_,_,_,_,_,1)\n",
    "    for i in range(len(prefeats)):\n",
    "        outname = respath+prefeats[i]\n",
    "        outfile = outname+'.perf'\n",
    "        outfile1 = outname+'_selfaccuracy.perf'\n",
    "        outfile2 = outname+'_crossaccuracy.perf'\n",
    "        out = open(outfile,'w+')\n",
    "        out.write(initial_str+\"\\n\")\n",
    "        out1 = open(outfile1,'w+')\n",
    "        out1.write(initial_str+\"\\n\")\n",
    "        out2 = open(outfile2,'w+')\n",
    "        out2.write(initial_str+\"\\n\")\n",
    "        for k1 in range(maxsurf):\n",
    "            for k2 in range(maxsurf):\n",
    "                if k2 > k1:\n",
    "                    for l in range(maxsurf):\n",
    "                        out.write(\"multimulti=\"+str(k1)+str(k2)+\"-\"+str(l)+\"\\n\")\n",
    "                        for j in range(len(subfeats)):\n",
    "                            fileid = filename2(i,j,k1,k2,l)\n",
    "                            tmp = np.load(fileid)['cm']\n",
    "                            acc[i,j,k1,k2,l,0] = round(tmp[1,1],2) # TP\n",
    "                            acc[i,j,k1,k2,l,1] = round(tmp[0,0],2) # TN\n",
    "                            acc[i,j,k1,k2,l,2] = 1-round(tmp[1,1],2) # FN\n",
    "                            acc[i,j,k1,k2,l,3] = 1-round(tmp[0,0],2) # FP\n",
    "                            avg[i,j,:] += acc[i,j,k1,k2,l,:]\n",
    "                            out.write(\"%s %.2f %.2f %.2f %.2f\\n\" % (subfeats[j],acc[i,j,k1,k2,l,0],\n",
    "                                                                    acc[i,j,k1,k2,l,1],acc[i,j,k1,k2,l,2],\n",
    "                                                                    acc[i,j,k1,k2,l,3]))\n",
    "                            if l == k1 or l == k2: # selc accuracy\n",
    "                                if j == 0 and l == k2:\n",
    "                                    out1.write(\"multimulti=\"+str(k1)+str(k2)+\"-\"+str(l)+\"\\n\")\n",
    "                                self_acc[i,j,k1,k2,0,:] = acc[i,j,k1,k2,l]\n",
    "                                avgs[i,j,:] += self_acc[i,j,k1,k2,0,:]\n",
    "                                if l == k2:\n",
    "                                    out1.write(\"%s %.2f %.2f %.2f %.2f\\n\" % (subfeats[j],\n",
    "                                                                             self_acc[i,j,k1,k2,0,0],\n",
    "                                                                             self_acc[i,j,k1,k2,0,1],\n",
    "                                                                             self_acc[i,j,k1,k2,0,2],\n",
    "                                                                             self_acc[i,j,k1,k2,0,3]))\n",
    "                            if l != k1 and l != k2:\n",
    "                                t = range(maxsurf)\n",
    "                                t.remove(k1)\n",
    "                                t.remove(k2)\n",
    "                                if (l == t[-1]):\n",
    "                                    if j == 0:\n",
    "                                        out2.write(\"multimulti=\"+str(k1)+str(k2)+\"\\n\")\n",
    "                                    cross_acc[i,j,k1,k2,0,:] = np.mean(acc[i,j,k1,k2,t,:], axis=0) # avg\n",
    "                                    # cross_acc[i,j,k1,k2,1,:] = np.std(acc[i,j,k1,k2,t,:], axis=0) # std\n",
    "                                    avgc[i,j,:] += cross_acc[i,j,k1,k2,0,:]\n",
    "                                    out2.write(\"%s %.2f %.2f %.2f %.2f\\n\" % (subfeats[j],\n",
    "                                                                             cross_acc[i,j,k1,k2,0,0],\n",
    "                                                                             cross_acc[i,j,k1,k2,0,1],\n",
    "                                                                             cross_acc[i,j,k1,k2,0,2],\n",
    "                                                                             cross_acc[i,j,k1,k2,0,3]))\n",
    "        out.write(\"multimulti=AVG\\n\")\n",
    "        out1.write(\"multimulti=AVG\\n\")\n",
    "        out2.write(\"multimulti=AVG\\n\")\n",
    "        avg /= comb(maxsurf,2)*maxsurf*1.\n",
    "        avgs /= comb(maxsurf,2)*2.\n",
    "        avgc /= comb(maxsurf,2)*1.\n",
    "        for j in range(len(subfeats)):\n",
    "            out.write(\"%s %.2f %.2f %.2f %.2f\\n\" % (subfeats[j],avg[i,j,0],avg[i,j,1],avg[i,j,2],avg[i,j,3]))\n",
    "            out1.write(\"%s %.2f %.2f %.2f %.2f\\n\" % (subfeats[j],avgs[i,j,0],avgs[i,j,1],avgs[i,j,2],avgs[i,j,3]))\n",
    "            out2.write(\"%s %.2f %.2f %.2f %.2f\\n\" % (subfeats[j],avgc[i,j,0],avgc[i,j,1],avgc[i,j,2],avgc[i,j,3]))\n",
    "        out.close()\n",
    "        out1.close()\n",
    "        out2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############ TRAINING with 3 surfaces each time, out of 6 surfaces in total ##############\n",
    "def filename3(i,j,k1,k2,k3,l,retpath=0):\n",
    "    \"\"\"function for the filename of the selected combination for training per 3 surfaces\n",
    "    -> i  : prefeature id, among all computed prefeatures (0: |f|, ... see prefeatid)\n",
    "    -> j  : subfeatureset among all features (0: AFFT, 1: FREQ, 2: TIME, 3: ALL)\n",
    "    -> ki : surface ids trained on\n",
    "    -> l  : surface id tested on\n",
    "    <- filename\n",
    "    \"\"\"\n",
    "    filepath = respath+'3/'\n",
    "    ensure_dir(filepath)\n",
    "    if retpath:\n",
    "        return filepath\n",
    "    else:\n",
    "        return filepath+'fs_'+str(i)+'_subfs_'+str(j)+'_tr1_'+str(k1)+'_tr2_'+str(k2)+'_tr3_'+str(k3)\\\n",
    "                                                                  +'_ts_'+str(l)+'.npz'\n",
    "\n",
    "def cross_fit3(i,j,k1,k2,k3,kmax,l,data,labels,data2,labels2,pipe):\n",
    "    \"\"\"function for fitting model per 3 surfaces\n",
    "    -> i              : prefeature id, among all computed prefeatures (0: |f|, ... see prefeatid)\n",
    "    -> j              : subfeatureset among all features (0: AFFT, 1: FREQ, 2: TIME, 3: ALL)\n",
    "    -> ki             : surface ids trained on\n",
    "    -> kmax           : maximum surfaces\n",
    "    -> l              : surface id tested on\n",
    "    -> data, labels   : training data and labels\n",
    "    -> data2, labels2 : testing data and labels\n",
    "    -> pipe           : the desired pipeline configuration\n",
    "    <- no output, saved model and confusion matrix in corresponding filename.npz\n",
    "    \"\"\"\n",
    "    fileid = filename3(i,j,k1,k2,k3,l)\n",
    "    if not os.path.isfile(fileid):\n",
    "        print i,j,k1,k2,k3,l\n",
    "        if k1==l or k2==l or k3==l: # perform K-fold      \n",
    "            print 'Fitting on '+str(k1)+\"-\"+str(k2)+\"-\"+str(k3)+', cross-validating on '+str(l)+'...'\n",
    "            if l == k1: # copy if existent from the other sibling file\n",
    "                tmpcopyfileid1 = filepath+filename3(i,j,k1,k2,k3,k2)+'.npz'\n",
    "                tmpcopyfileid2 = filepath+filename3(i,j,k1,k2,k3,k3)+'.npz'\n",
    "            elif l == k2:   # same as above\n",
    "                tmpcopyfileid1 = filepath+filename3(i,j,k1,k2,k3,k1)+'.npz'\n",
    "                tmpcopyfileid2 = filepath+filename3(i,j,k1,k2,k3,k3)+'.npz'\n",
    "            else:\n",
    "                tmpcopyfileid1 = filepath+filename3(i,j,k1,k2,k3,k1)+'.npz'\n",
    "                tmpcopyfileid2 = filepath+filename3(i,j,k1,k2,k3,k2)+'.npz'\n",
    "            if not os.path.isfile(tmpcopyfileid1) and not os.path.isfile(tmpcopyfileid2):\n",
    "                folds = cv.split(data, labels)\n",
    "                cm_all = np.zeros((2,2))\n",
    "                for fold, (train_ind, test_ind) in enumerate(folds):\n",
    "                    x_train, x_test = data[train_ind], data[test_ind]\n",
    "                    y_train, y_test = labels[train_ind], labels[test_ind]\n",
    "                    model = pipe.fit(x_train,y_train)\n",
    "                    y_pred = model.predict(x_test)\n",
    "                    cm = confusion_matrix(y_pred=y_pred, y_true=y_test)\n",
    "                    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "                    cm_all += cm/5.\n",
    "            else:\n",
    "                if os.path.isfile(tmpcopyfileid1):\n",
    "                    cm_all = np.load(tmpcopyfileid1)['cm']\n",
    "                    model = np.load(tmpcopyfileid1)['model'][0]\n",
    "                else:\n",
    "                    cm_all = np.load(tmpcopyfileid2)['cm']\n",
    "                    model = np.load(tmpcopyfileid2)['model'][0]\n",
    "            np.savez(fileid,cm=cm_all,model=np.array([model]))\n",
    "        else: # perform cross-check\n",
    "            tr_data = data\n",
    "            tr_labels = labels\n",
    "            ts_data = data2\n",
    "            ts_labels = labels2\n",
    "            model = []\n",
    "            for m in range(kmax):\n",
    "                tmpcopyfileid = filepath+filename3(i,j,k1,k2,k3,m)+'.npz'\n",
    "                if k1!=m and k2!=m and k3!=m and os.path.isfile(tmpcopyfileid):\n",
    "                    print 'Found precomputed model of '+str(k1)+str(k2)+str(k3)+', tested on '+str(m) \\\n",
    "                                                                               +'. Testing on '+str(l)+'...'\n",
    "                    model = np.load(tmpcopyfileid)['model'][0]\n",
    "                    break\n",
    "            if model==[]: # model not found precomputed\n",
    "                print 'Fitting on '+str(k1)+\"-\"+str(k2)+\"-\"+str(k3)+', testing on '+str(l)+'...'\n",
    "                model = pipe.fit(tr_data,tr_labels)\n",
    "            y_pred = model.predict(ts_data)\n",
    "            cm = confusion_matrix(y_pred=y_pred, y_true=ts_labels)\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            np.savez(fileid,cm=cm,model=np.array([model]))\n",
    "\n",
    "def init_steps3(i,j,jmax,surf,surfla):\n",
    "    \"\"\"function for helping parallelization of computations per 3 surfaces\n",
    "    -> i              : prefeature id, among all computed prefeatures (0: |f|, ... see prefeatid)\n",
    "    -> j              : subfeatureset among all features (0: AFFT, 1: FREQ, 2: TIME, 3: ALL)\n",
    "    -> jmax           : number of all subfeaturesets\n",
    "    -> surf, surfla   : surface data and labels\n",
    "    \"\"\"\n",
    "    if j==jmax:\n",
    "        featsel = SelectKBest(k=1000,score_func= mutual_info_classif)\n",
    "    else:\n",
    "        featsel = SelectKBest(k='all',score_func= mutual_info_classif)\n",
    "    pipe = make_pipe_clf(scaler, featsel, decomp, classifiers[2])\n",
    "    for k1 in range(surf.shape[0]): # for every training surface1\n",
    "        for k2 in range(surf.shape[0]): # for every training surface2\n",
    "            if k2 > k1:\n",
    "                for k3 in range(surf.shape[0]):\n",
    "                    if k3 > k2:\n",
    "                        for l in range(surf.shape[0]): # for every testing surface\n",
    "                            tr_surf = np.concatenate((surf[k1],surf[k2],surf[k3]),axis=0)\n",
    "                            tr_surfla = np.concatenate((surfla[:,k1],surfla[:,k2],surfla[:,k3]),axis=0)\n",
    "                            ts_surf, ts_surfla = surf[l], surfla[:,l]\n",
    "                            cross_fit3(i,j,k1,k2,k3,surf.shape[0],l,tr_surf,tr_surfla,ts_surf,ts_surfla,pipe)\n",
    "\n",
    "def train_3_surface(surf,surfla,n=-1):\n",
    "    \"\"\"Parallel training -on surface level- of all combinations on 3 surfaces\n",
    "    -> n              : number of cores to run in parallel, \n",
    "                        input of joblib's Parallel (n=-1 means all available cores)\n",
    "    -> surf, surfla   : surface data and labels\n",
    "    *** Cross surface validation, TRAINING with 3 surfaces each time, out of 6 surfaces in total\n",
    "    total= 4 (featuresets) * [comb(6,3)*6] (surface combinations: trained on 3, tested on 1) * 1 (prefeatureset)\n",
    "         = 4*20*6*1 = 480 different runs-files.\n",
    "    Note that comb(n,r) = n!/(r!(n-r)!)\n",
    "    \"\"\"\n",
    "    print \"-------------------------- TRAINING all combinations per 3 surfaces ----------------------------------\"\n",
    "    for i in range(len(prefeatid)-1):\n",
    "        _ = [Parallel(n_jobs=n)([delayed(init_steps3) (i,j,surf.shape[0]-1,surf[j,:,i],surfla[:,:,i]) \n",
    "                                 for j in range(surf.shape[0])])]\n",
    "        \n",
    "def bargraph_file_gen3(maxsurf):\n",
    "    prefeats = prefeatnames[prefeatid][:-1]\n",
    "    # prefeatures, subfeatures, trained, tested, (TP,TN,FN,FP)\n",
    "    acc = np.zeros((len(prefeats),len(subfeats),maxsurf,maxsurf,maxsurf,maxsurf,4))\n",
    "    # features, subfeatures, (TP,TN,FN,FP) -> avg over all tested surfaces\n",
    "    avg = np.zeros((len(prefeats),len(subfeats),4))\n",
    "    # prefeatures, subfeatures, trained, cross_val_self_accuracy, (TP,TN,FN,FP)\n",
    "    self_acc = np.zeros((len(prefeats),len(subfeats),maxsurf,maxsurf,maxsurf,1,4))\n",
    "    # features, subfeatures, (TP,TN,FN,FP) -> avg over all self tested surfaces\n",
    "    avgs = np.zeros((len(prefeats),len(subfeats),4))\n",
    "    # features, subfeatures, trained, (tested avg, tested std), (TP,TN,FN,FP)\n",
    "    cross_acc = np.zeros((len(prefeats),len(subfeats),maxsurf,maxsurf,maxsurf,2,4))\n",
    "     # features, subfeatures, (TP,TN,FN,FP) -> avg over all cross tested surfaces\n",
    "    avgc = np.zeros((len(prefeats),len(subfeats),4))\n",
    "    initial_str = \"\"\"# clustered and stacked graph bogus data\n",
    "=stackcluster;TP;TN;FN;FP\n",
    "colors=med_blue,dark_green,yellow,red\n",
    "=nogridy\n",
    "=noupperright\n",
    "fontsz=2\n",
    "legendx=right\n",
    "legendy=center\n",
    "datascale=50\n",
    "yformat=%g%%\n",
    "xlabel=TrainedON-TestedON\n",
    "ylabel=Metrics\n",
    "=table\"\"\"\n",
    "    respath = filename3(_,_,_,_,_,_,1)\n",
    "    for i in range(len(prefeats)):\n",
    "        outname = respath+prefeats[i]\n",
    "        outfile = outname+'.perf'\n",
    "        outfile1 = outname+'_selfaccuracy.perf'\n",
    "        outfile2 = outname+'_crossaccuracy.perf'\n",
    "        out = open(outfile,'w+')\n",
    "        out.write(initial_str+\"\\n\")\n",
    "        out1 = open(outfile1,'w+')\n",
    "        out1.write(initial_str+\"\\n\")\n",
    "        out2 = open(outfile2,'w+')\n",
    "        out2.write(initial_str+\"\\n\")\n",
    "        for k1 in range(maxsurf):\n",
    "            for k2 in range(maxsurf):\n",
    "                if k2 > k1:\n",
    "                    for k3 in range(maxsurf):\n",
    "                        if k3 > k2:\n",
    "                            for l in range(maxsurf):\n",
    "                                out.write(\"multimulti=\"+str(k1)+str(k2)+str(k3)+\"-\"+str(l)+\"\\n\")\n",
    "                                for j in range(len(subfeats)):\n",
    "                                    fileid = filename3(i,j,k1,k2,k3,l)\n",
    "                                    tmp = np.load(fileid)['cm']\n",
    "                                    acc[i,j,k1,k2,k3,l,0] = round(tmp[1,1],2) # TP\n",
    "                                    acc[i,j,k1,k2,k3,l,1] = round(tmp[0,0],2) # TN\n",
    "                                    acc[i,j,k1,k2,k3,l,2] = 1-round(tmp[1,1],2) # FN\n",
    "                                    acc[i,j,k1,k2,k3,l,3] = 1-round(tmp[0,0],2) # FP\n",
    "                                    avg[i,j,:] += acc[i,j,k1,k2,k3,l,:]\n",
    "                                    out.write(\"%s %.2f %.2f %.2f %.2f\\n\" % (subfeats[j],acc[i,j,k1,k2,k3,l,0],\n",
    "                                                                            acc[i,j,k1,k2,k3,l,1],\n",
    "                                                                            acc[i,j,k1,k2,k3,l,2],\n",
    "                                                                            acc[i,j,k1,k2,k3,l,3]))\n",
    "                                    if l == k1 or l == k2 or l == k3: # selc accuracy\n",
    "                                        if j == 0 and l == k3:\n",
    "                                            out1.write(\"multimulti=\"+str(k1)+str(k2)+str(k3)+\"-\"+str(l)+\"\\n\")\n",
    "                                        self_acc[i,j,k1,k2,k3,0,:] = acc[i,j,k1,k2,k3,l]\n",
    "                                        avgs[i,j,:] += self_acc[i,j,k1,k2,k3,0,:]\n",
    "                                        if l == k3:\n",
    "                                            out1.write(\"%s %.2f %.2f %.2f %.2f\\n\" % (subfeats[j],\n",
    "                                                                                     self_acc[i,j,k1,k2,k3,0,0],\n",
    "                                                                                     self_acc[i,j,k1,k2,k3,0,1],\n",
    "                                                                                     self_acc[i,j,k1,k2,k3,0,2],\n",
    "                                                                                     self_acc[i,j,k1,k2,k3,0,3]))\n",
    "                                    if l != k1 and l != k2 and l != k3:\n",
    "                                        t = range(maxsurf)\n",
    "                                        t.remove(k1)\n",
    "                                        t.remove(k2)\n",
    "                                        t.remove(k3)\n",
    "                                        if (l == t[-1]):\n",
    "                                            if j == 0:\n",
    "                                                out2.write(\"multimulti=\"+str(k1)+str(k2)+str(k3)+\"\\n\")\n",
    "                                            # avg\n",
    "                                            cross_acc[i,j,k1,k2,k3,0,:] = np.mean(acc[i,j,k1,k2,k3,t,:], axis=0)\n",
    "                                            # std\n",
    "                                            # cross_acc[i,j,k1,k2,k3,1,:] = np.std(acc[i,j,k1,k2,k3,t,:], axis=0)\n",
    "                                            avgc[i,j,:] += cross_acc[i,j,k1,k2,k3,0,:]\n",
    "                                            out2.write(\"%s %.2f %.2f %.2f %.2f\\n\" % (subfeats[j],\n",
    "                                                                                     cross_acc[i,j,k1,k2,k3,0,0],\n",
    "                                                                                     cross_acc[i,j,k1,k2,k3,0,1],\n",
    "                                                                                     cross_acc[i,j,k1,k2,k3,0,2],\n",
    "                                                                                     cross_acc[i,j,k1,k2,k3,0,3]))\n",
    "        out.write(\"multimulti=AVG\\n\")\n",
    "        out1.write(\"multimulti=AVG\\n\")\n",
    "        out2.write(\"multimulti=AVG\\n\")\n",
    "        avg /= comb(maxsurf,3)*maxsurf*1.\n",
    "        avgs /= comb(maxsurf,3)*3.\n",
    "        avgc /= comb(maxsurf,3)*1.\n",
    "        for j in range(len(subfeats)):\n",
    "            out.write(\"%s %.2f %.2f %.2f %.2f\\n\" % (subfeats[j],avg[i,j,0],avg[i,j,1],avg[i,j,2],avg[i,j,3]))\n",
    "            out1.write(\"%s %.2f %.2f %.2f %.2f\\n\" % (subfeats[j],avgs[i,j,0],avgs[i,j,1],avgs[i,j,2],avgs[i,j,3]))\n",
    "            out2.write(\"%s %.2f %.2f %.2f %.2f\\n\" % (subfeats[j],avgc[i,j,0],avgc[i,j,1],avgc[i,j,2],avgc[i,j,3]))\n",
    "        out.close()\n",
    "        out1.close()\n",
    "        out2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############ TRAINING with 4 surfaces each time, out of 6 surfaces in total ##############\n",
    "def filename4(i,j,k1,k2,k3,k4,l,retpath=0):\n",
    "    \"\"\"function for the filename of the selected combination for training per 4 surfaces\n",
    "    -> i  : prefeature id, among all computed prefeatures (0: |f|, ... see prefeatid)\n",
    "    -> j  : subfeatureset among all features (0: AFFT, 1: FREQ, 2: TIME, 3: ALL)\n",
    "    -> ki : surface ids trained on\n",
    "    -> l  : surface id tested on\n",
    "    <- filename\n",
    "    \"\"\"\n",
    "    filepath = respath+'4/'\n",
    "    ensure_dir(filepath)\n",
    "    if retpath:\n",
    "        return filepath\n",
    "    else:\n",
    "        return filepath+'fs_'+str(i)+'_subfs_'+str(j)+'_tr1_'+str(k1)+'_tr2_'+str(k2)+'_tr3_' \\\n",
    "                                 +str(k3)+'_tr4_'+str(k4)+'_ts_'+str(l)+'.npz'\n",
    "\n",
    "def cross_fit4(i,j,k1,k2,k3,k4,kmax,l,data,labels,data2,labels2,pipe):\n",
    "    \"\"\"function for fitting model per 4 surfaces\n",
    "    -> i              : prefeature id, among all computed prefeatures (0: |f|, ... see prefeatid)\n",
    "    -> j              : subfeatureset among all features (0: AFFT, 1: FREQ, 2: TIME, 3: ALL)\n",
    "    -> ki             : surface ids trained on\n",
    "    -> kmax           : maximum surfaces\n",
    "    -> l              : surface id tested on\n",
    "    -> data, labels   : training data and labels\n",
    "    -> data2, labels2 : testing data and labels\n",
    "    -> pipe           : the desired pipeline configuration\n",
    "    <- no output, saved model and confusion matrix in corresponding filename.npz\n",
    "    \"\"\"\n",
    "    fileid = filename4(i,j,k1,k2,k3,k4,l)\n",
    "    if not os.path.isfile(fileid):\n",
    "        print i,j,k1,k2,k3,k4,l\n",
    "        if k1==l or k2==l or k3==l or k4==l: # perform K-fold      \n",
    "            print 'Fitting on '+str(k1)+\"-\"+str(k2)+\"-\"+str(k3)+\"-\"+str(k4)+', cross-validating on '+str(l)+'...'\n",
    "            if l == k1: # copy if existent from the other sibling file\n",
    "                tmpcopyfileid1 = filepath+filename4(i,j,k1,k2,k3,k4,k2)+'.npz'\n",
    "                tmpcopyfileid2 = filepath+filename4(i,j,k1,k2,k3,k4,k3)+'.npz'\n",
    "                tmpcopyfileid3 = filepath+filename4(i,j,k1,k2,k3,k4,k4)+'.npz'\n",
    "            elif l == k2:   # same as above\n",
    "                tmpcopyfileid1 = filepath+filename4(i,j,k1,k2,k3,k4,k1)+'.npz'\n",
    "                tmpcopyfileid2 = filepath+filename4(i,j,k1,k2,k3,k4,k3)+'.npz'\n",
    "                tmpcopyfileid3 = filepath+filename4(i,j,k1,k2,k3,k4,k4)+'.npz'\n",
    "            elif l == k3:   # same as above\n",
    "                tmpcopyfileid1 = filepath+filename4(i,j,k1,k2,k3,k4,k1)+'.npz'\n",
    "                tmpcopyfileid2 = filepath+filename4(i,j,k1,k2,k3,k4,k2)+'.npz'\n",
    "                tmpcopyfileid3 = filepath+filename4(i,j,k1,k2,k3,k4,k4)+'.npz'\n",
    "            else:\n",
    "                tmpcopyfileid1 = filepath+filename4(i,j,k1,k2,k3,k4,k1)+'.npz'\n",
    "                tmpcopyfileid2 = filepath+filename4(i,j,k1,k2,k3,k4,k2)+'.npz'\n",
    "                tmpcopyfileid3 = filepath+filename4(i,j,k1,k2,k3,k4,k3)+'.npz'\n",
    "            if not os.path.isfile(tmpcopyfileid1) and not os.path.isfile(tmpcopyfileid2) \\\n",
    "                                                  and not os.path.isfile(tmpcopyfileid3):\n",
    "                folds = cv.split(data, labels)\n",
    "                cm_all = np.zeros((2,2))\n",
    "                for fold, (train_ind, test_ind) in enumerate(folds):\n",
    "                    x_train, x_test = data[train_ind], data[test_ind]\n",
    "                    y_train, y_test = labels[train_ind], labels[test_ind]\n",
    "                    model = pipe.fit(x_train,y_train)\n",
    "                    y_pred = model.predict(x_test)\n",
    "                    cm = confusion_matrix(y_pred=y_pred, y_true=y_test)\n",
    "                    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "                    cm_all += cm/5.\n",
    "            else:\n",
    "                if os.path.isfile(tmpcopyfileid1):\n",
    "                    cm_all = np.load(tmpcopyfileid1)['cm']\n",
    "                    model = np.load(tmpcopyfileid1)['model'][0]\n",
    "                elif os.path.isfile(tmpcopyfileid2):\n",
    "                    cm_all = np.load(tmpcopyfileid2)['cm']\n",
    "                    model = np.load(tmpcopyfileid2)['model'][0]\n",
    "                elif os.path.isfile(tmpcopyfileid3):\n",
    "                    cm_all = np.load(tmpcopyfileid3)['cm']\n",
    "                    model = np.load(tmpcopyfileid3)['model'][0]\n",
    "            np.savez(fileid,cm=cm_all,model=np.array([model]))\n",
    "        else: # perform cross-check\n",
    "            tr_data = data\n",
    "            tr_labels = labels\n",
    "            ts_data = data2\n",
    "            ts_labels = labels2\n",
    "            model = []\n",
    "            for m in range(kmax):\n",
    "                tmpcopyfileid = filepath+filename4(i,j,k1,k2,k3,k4,m)+'.npz'\n",
    "                if k1!=m and k2!=m and k3!=m and k4!=m and os.path.isfile(tmpcopyfileid):\n",
    "                    print 'Found precomputed model of '+str(k1)+str(k2)+str(k3)+str(k4)\\\n",
    "                                                       +', tested on '+str(m)+'. Testing on '+str(l)+'...'\n",
    "                    model = np.load(tmpcopyfileid)['model'][0]\n",
    "                    break\n",
    "            if model==[]: # model not found precomputed\n",
    "                print 'Fitting on '+str(k1)+\"-\"+str(k2)+\"-\"+str(k3)+\"-\"+str(k4)+', testing on '+str(l)+'...'\n",
    "                model = pipe.fit(tr_data,tr_labels)\n",
    "            y_pred = model.predict(ts_data)\n",
    "            cm = confusion_matrix(y_pred=y_pred, y_true=ts_labels)\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            np.savez(fileid,cm=cm,model=np.array([model]))\n",
    "\n",
    "def init_steps4(i,j,jmax,surf,surfla):\n",
    "    \"\"\"function for helping parallelization of computations per 4 surfaces\n",
    "    -> i              : prefeature id, among all computed prefeatures (0: |f|, ... see prefeatid)\n",
    "    -> j              : subfeatureset among all features (0: AFFT, 1: FREQ, 2: TIME, 3: ALL)\n",
    "    -> jmax           : number of all subfeaturesets\n",
    "    -> surf, surfla   : surface data and labels\n",
    "    \"\"\"\n",
    "    if j==jmax:\n",
    "        featsel = SelectKBest(k=1000,score_func= mutual_info_classif)\n",
    "    else:\n",
    "        featsel = SelectKBest(k='all',score_func= mutual_info_classif)\n",
    "    pipe = make_pipe_clf(scaler, featsel, decomp, classifiers[2])\n",
    "    for k1 in range(surf.shape[0]): # for every training surface1\n",
    "        for k2 in range(surf.shape[0]): # for every training surface2\n",
    "            if k2 > k1:\n",
    "                for k3 in range(surf.shape[0]):\n",
    "                    if k3 > k2:\n",
    "                        for k4 in range(surf.shape[0]):\n",
    "                            if k4 > k3:\n",
    "                                for l in range(surf.shape[0]): # for every testing surface\n",
    "                                    tr_surf = np.concatenate((surf[k1],surf[k2],surf[k3]),axis=0)\n",
    "                                    tr_surfla = np.concatenate((surfla[:,k1],surfla[:,k2],surfla[:,k3]),axis=0)\n",
    "                                    ts_surf, ts_surfla = surf[l], surfla[:,l]\n",
    "                                    cross_fit4(i,j,k1,k2,k3,k4,surf.shape[0],l,\n",
    "                                               tr_surf,tr_surfla,ts_surf,ts_surfla,pipe)\n",
    "\n",
    "def train_4_surface(surf,surfla,n=-1):\n",
    "    \"\"\"Parallel training -on surface level- of all combinations on 4 surfaces\n",
    "    -> n              : number of cores to run in parallel, \n",
    "                        input of joblib's Parallel (n=-1 means all available cores)\n",
    "    -> surf, surfla   : surface data and labels\n",
    "    *** Cross surface validation, TRAINING with 2 surfaces each time, out of 6 surfaces in total\n",
    "    total= 4 (featuresets) * [comb(6,4)*6] (surface combinations: trained on 4, tested on 1) * 1 (prefeatureset)\n",
    "         = 4*15*6*1 = 360 different runs-files.\n",
    "    Note that comb(n,r) = n!/(r!(n-r)!)\n",
    "    \"\"\"\n",
    "    print \"-------------------------- TRAINING all combinations per 4 surfaces ----------------------------------\"        \n",
    "    for i in range(len(prefeatid)-1):\n",
    "        _ = [Parallel(n_jobs=n)([delayed(init_steps4) (i,j,surf.shape[0]-1,surf[j,:,i],surfla[:,:,i]) \n",
    "                                 for j in range(surf.shape[0])])]\n",
    "        \n",
    "def bargraph_file_gen4(maxsurf):\n",
    "    prefeats = prefeatnames[prefeatid][:-1]\n",
    "    # prefeatures, subfeatures, trained, tested, (TP,TN,FN,FP)\n",
    "    acc = np.zeros((len(prefeats),len(subfeats),maxsurf,maxsurf,maxsurf,maxsurf,maxsurf,4))\n",
    "    # features, subfeatures, (TP,TN,FN,FP) -> avg over all tested surfaces\n",
    "    avg = np.zeros((len(prefeats),len(subfeats),4))\n",
    "    # prefeatures, subfeatures, trained, cross_val_self_accuracy, (TP,TN,FN,FP)\n",
    "    self_acc = np.zeros((len(prefeats),len(subfeats),maxsurf,maxsurf,maxsurf,maxsurf,1,4))\n",
    "    # features, subfeatures, (TP,TN,FN,FP) -> avg over all self tested surfaces\n",
    "    avgs = np.zeros((len(prefeats),len(subfeats),4))\n",
    "    # features, subfeatures, trained, (tested avg, tested std), (TP,TN,FN,FP)\n",
    "    cross_acc = np.zeros((len(prefeats),len(subfeats),maxsurf,maxsurf,maxsurf,maxsurf,2,4))\n",
    "     # features, subfeatures, (TP,TN,FN,FP) -> avg over all cross tested surfaces\n",
    "    avgc = np.zeros((len(prefeats),len(subfeats),4))\n",
    "    initial_str = \"\"\"# clustered and stacked graph bogus data\n",
    "=stackcluster;TP;TN;FN;FP\n",
    "colors=med_blue,dark_green,yellow,red\n",
    "=nogridy\n",
    "=noupperright\n",
    "fontsz=2\n",
    "legendx=right\n",
    "legendy=center\n",
    "datascale=50\n",
    "yformat=%g%%\n",
    "xlabel=TrainedON-TestedON\n",
    "ylabel=Metrics\n",
    "=table\"\"\"\n",
    "    respath = filename4(_,_,_,_,_,_,_,1)\n",
    "    for i in range(len(prefeats)):\n",
    "        outname = respath+prefeats[i]\n",
    "        outfile = outname+'.perf'\n",
    "        outfile1 = outname+'_selfaccuracy.perf'\n",
    "        outfile2 = outname+'_crossaccuracy.perf'\n",
    "        out = open(outfile,'w+')\n",
    "        out.write(initial_str+\"\\n\")\n",
    "        out1 = open(outfile1,'w+')\n",
    "        out1.write(initial_str+\"\\n\")\n",
    "        out2 = open(outfile2,'w+')\n",
    "        out2.write(initial_str+\"\\n\")\n",
    "        for k1 in range(maxsurf):\n",
    "            for k2 in range(maxsurf):\n",
    "                if k2 > k1:\n",
    "                    for k3 in range(maxsurf):\n",
    "                        if k3 > k2:\n",
    "                            for k4 in range(maxsurf):\n",
    "                                if k4 > k3:\n",
    "                                    for l in range(maxsurf):\n",
    "                                        out.write(\"multimulti=\"+str(k1)+str(k2)+str(k3)+str(k4)+\"-\"+str(l)+\"\\n\")\n",
    "                                        for j in range(len(subfeats)):\n",
    "                                            fileid = filename4(i,j,k1,k2,k3,k4,l)\n",
    "                                            tmp = np.load(fileid)['cm']\n",
    "                                            acc[i,j,k1,k2,k3,k4,l,0] = round(tmp[1,1],2) # TP\n",
    "                                            acc[i,j,k1,k2,k3,k4,l,1] = round(tmp[0,0],2) # TN\n",
    "                                            acc[i,j,k1,k2,k3,k4,l,2] = 1-round(tmp[1,1],2) # FN\n",
    "                                            acc[i,j,k1,k2,k3,k4,l,3] = 1-round(tmp[0,0],2) # FP\n",
    "                                            avg[i,j,:] += acc[i,j,k1,k2,k3,k4,l,:]\n",
    "                                            out.write(\"%s %.2f %.2f %.2f %.2f\\n\" % (subfeats[j],\n",
    "                                                                                    acc[i,j,k1,k2,k3,k4,l,0],\n",
    "                                                                                    acc[i,j,k1,k2,k3,k4,l,1],\n",
    "                                                                                    acc[i,j,k1,k2,k3,k4,l,2],\n",
    "                                                                                    acc[i,j,k1,k2,k3,k4,l,3]))\n",
    "                                            if l == k1 or l == k2 or l == k3 or l == k4: # selc accuracy\n",
    "                                                if j == 0 and l == k4:\n",
    "                                                    out1.write(\"multimulti=\"+str(k1)+str(k2)+str(k3)\\\n",
    "                                                                            +str(k4)+\"-\"+str(l)+\"\\n\")\n",
    "                                                self_acc[i,j,k1,k2,k3,k4,0,:] = acc[i,j,k1,k2,k3,k4,l]\n",
    "                                                avgs[i,j,:] += self_acc[i,j,k1,k2,k3,k4,0,:]\n",
    "                                                if l == k4:\n",
    "                                                    out1.write(\"%s %.2f %.2f %.2f %.2f\\n\" \\\n",
    "                                                               % (subfeats[j],\n",
    "                                                                  self_acc[i,j,k1,k2,k3,k4,0,0],\n",
    "                                                                  self_acc[i,j,k1,k2,k3,k4,0,1],\n",
    "                                                                  self_acc[i,j,k1,k2,k3,k4,0,2],\n",
    "                                                                  self_acc[i,j,k1,k2,k3,k4,0,3]))\n",
    "                                            if l != k1 and l != k2 and l != k3 and l!= k4:\n",
    "                                                t = range(maxsurf)\n",
    "                                                t.remove(k1)\n",
    "                                                t.remove(k2)\n",
    "                                                t.remove(k3)\n",
    "                                                t.remove(k4)\n",
    "                                                if (l == t[-1]):\n",
    "                                                    if j == 0:\n",
    "                                                        out2.write(\"multimulti=\"+str(k1)+str(k2)\\\n",
    "                                                                   +str(k3)+str(k4)+\"\\n\")\n",
    "                                                    cross_acc[i,j,k1,k2,k3,k4,0,:] = \\\n",
    "                                                        np.mean(acc[i,j,k1,k2,k3,k4,t,:], axis=0)\n",
    "                                                    avgc[i,j,:] += cross_acc[i,j,k1,k2,k3,k4,0,:]\n",
    "                                                    out2.write(\"%s %.2f %.2f %.2f %.2f\\n\" \\\n",
    "                                                               % (subfeats[j],\n",
    "                                                                  cross_acc[i,j,k1,k2,k3,k4,0,0],\n",
    "                                                                  cross_acc[i,j,k1,k2,k3,k4,0,1],\n",
    "                                                                  cross_acc[i,j,k1,k2,k3,k4,0,2],\n",
    "                                                                  cross_acc[i,j,k1,k2,k3,k4,0,3]))\n",
    "        out.write(\"multimulti=AVG\\n\")\n",
    "        out1.write(\"multimulti=AVG\\n\")\n",
    "        out2.write(\"multimulti=AVG\\n\")\n",
    "        avg /= comb(maxsurf,4)*maxsurf*1.\n",
    "        avgs /= comb(maxsurf,4)*4.\n",
    "        avgc /= comb(maxsurf,4)*1.\n",
    "        for j in range(len(subfeats)):\n",
    "            out.write(\"%s %.2f %.2f %.2f %.2f\\n\" % (subfeats[j],avg[i,j,0],avg[i,j,1],avg[i,j,2],avg[i,j,3]))\n",
    "            out1.write(\"%s %.2f %.2f %.2f %.2f\\n\" % (subfeats[j],avgs[i,j,0],avgs[i,j,1],avgs[i,j,2],avgs[i,j,3]))\n",
    "            out2.write(\"%s %.2f %.2f %.2f %.2f\\n\" % (subfeats[j],avgc[i,j,0],avgc[i,j,1],avgc[i,j,2],avgc[i,j,3]))\n",
    "        out.close()\n",
    "        out1.close()\n",
    "        out2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ TRAINING with 5 surfaces each time, out of 6 surfaces in total ##############\n",
    "def filename5(i,j,k1,k2,k3,k4,k5,l,retpath=0):\n",
    "    \"\"\"function for the filename of the selected combination for training per 5 surfaces\n",
    "    -> i  : prefeature id, among all computed prefeatures (0: |f|, ... see prefeatid)\n",
    "    -> j  : subfeatureset among all features (0: AFFT, 1: FREQ, 2: TIME, 3: ALL)\n",
    "    -> ki : surface ids trained on\n",
    "    -> l  : surface id tested on\n",
    "    <- filename\n",
    "    \"\"\"\n",
    "    filepath = respath+'5/'\n",
    "    ensure_dir(filepath)\n",
    "    if retpath:\n",
    "        return filepath\n",
    "    else:\n",
    "        return filepath+'fs_'+str(i)+'_subfs_'+str(j)+'_tr1_'+str(k1)+'_tr2_'+str(k2)+'_tr3_' \\\n",
    "                                 +str(k3)+'_tr4_'+str(k4)+'_tr5_'+str(k5)+'_ts_'+str(l)+'.npz'\n",
    "\n",
    "def cross_fit5(i,j,k1,k2,k3,k4,k5,kmax,l,data,labels,data2,labels2,pipe):\n",
    "    \"\"\"function for fitting model per 5 surfaces\n",
    "    -> i              : prefeature id, among all computed prefeatures (0: |f|, ... see prefeatid)\n",
    "    -> j              : subfeatureset among all features (0: AFFT, 1: FREQ, 2: TIME, 3: ALL)\n",
    "    -> ki             : surface ids trained on\n",
    "    -> kmax           : maximum surfaces\n",
    "    -> l              : surface id tested on\n",
    "    -> data, labels   : training data and labels\n",
    "    -> data2, labels2 : testing data and labels\n",
    "    -> pipe           : the desired pipeline configuration\n",
    "    <- no output, saved model and confusion matrix in corresponding filename.npz\n",
    "    \"\"\"\n",
    "    fileid = filename5(i,j,k1,k2,k3,k4,k5,l)\n",
    "    if not os.path.isfile(fileid):\n",
    "        print i,j,k1,k2,k3,k4,k5,l\n",
    "        if k1==l or k2==l or k3==l or k4==l or k5==l: # perform K-fold      \n",
    "            print 'Fitting on '+str(k1)+\"-\"+str(k2)+\"-\"+str(k3)+\"-\"+str(k4)+\"-\" \\\n",
    "                                           +str(k5)+', cross-validating on '+str(l)+'...'\n",
    "            if l == k1: # copy if existent from the other sibling file\n",
    "                tmpcopyfileid1 = filepath+filename5(i,j,k1,k2,k3,k4,k5,k2)+'.npz'\n",
    "                tmpcopyfileid2 = filepath+filename5(i,j,k1,k2,k3,k4,k5,k3)+'.npz'\n",
    "                tmpcopyfileid3 = filepath+filename5(i,j,k1,k2,k3,k4,k5,k4)+'.npz'\n",
    "                tmpcopyfileid4 = filepath+filename5(i,j,k1,k2,k3,k4,k5,k5)+'.npz'\n",
    "            elif l == k2:   # same as above\n",
    "                tmpcopyfileid1 = filepath+filename5(i,j,k1,k2,k3,k4,k5,k1)+'.npz'\n",
    "                tmpcopyfileid2 = filepath+filename5(i,j,k1,k2,k3,k4,k5,k3)+'.npz'\n",
    "                tmpcopyfileid3 = filepath+filename5(i,j,k1,k2,k3,k4,k5,k4)+'.npz'\n",
    "                tmpcopyfileid4 = filepath+filename5(i,j,k1,k2,k3,k4,k5,k5)+'.npz'\n",
    "            elif l == k3:   # same as above\n",
    "                tmpcopyfileid1 = filepath+filename5(i,j,k1,k2,k3,k4,k5,k1)+'.npz'\n",
    "                tmpcopyfileid2 = filepath+filename5(i,j,k1,k2,k3,k4,k5,k2)+'.npz'\n",
    "                tmpcopyfileid3 = filepath+filename5(i,j,k1,k2,k3,k4,k5,k4)+'.npz'\n",
    "                tmpcopyfileid4 = filepath+filename5(i,j,k1,k2,k3,k4,k5,k5)+'.npz'\n",
    "            elif l == k4:   # same as above\n",
    "                tmpcopyfileid1 = filepath+filename5(i,j,k1,k2,k3,k4,k5,k1)+'.npz'\n",
    "                tmpcopyfileid2 = filepath+filename5(i,j,k1,k2,k3,k4,k5,k2)+'.npz'\n",
    "                tmpcopyfileid3 = filepath+filename5(i,j,k1,k2,k3,k4,k5,k3)+'.npz'\n",
    "                tmpcopyfileid4 = filepath+filename5(i,j,k1,k2,k3,k4,k5,k5)+'.npz'\n",
    "            else:\n",
    "                tmpcopyfileid1 = filepath+filename5(i,j,k1,k2,k3,k4,k5,k1)+'.npz'\n",
    "                tmpcopyfileid2 = filepath+filename5(i,j,k1,k2,k3,k4,k5,k2)+'.npz'\n",
    "                tmpcopyfileid3 = filepath+filename5(i,j,k1,k2,k3,k4,k5,k3)+'.npz'\n",
    "                tmpcopyfileid4 = filepath+filename5(i,j,k1,k2,k3,k4,k5,k4)+'.npz'\n",
    "            if not os.path.isfile(tmpcopyfileid1) and not os.path.isfile(tmpcopyfileid2) \\\n",
    "               and not os.path.isfile(tmpcopyfileid3) and not os.path.isfile(tmpcopyfileid4):\n",
    "                folds = cv.split(data, labels)\n",
    "                cm_all = np.zeros((2,2))\n",
    "                for fold, (train_ind, test_ind) in enumerate(folds):\n",
    "                    x_train, x_test = data[train_ind], data[test_ind]\n",
    "                    y_train, y_test = labels[train_ind], labels[test_ind]\n",
    "                    model = pipe.fit(x_train,y_train)\n",
    "                    y_pred = model.predict(x_test)\n",
    "                    cm = confusion_matrix(y_pred=y_pred, y_true=y_test)\n",
    "                    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "                    cm_all += cm/5.\n",
    "            else:\n",
    "                if os.path.isfile(tmpcopyfileid1):\n",
    "                    cm_all = np.load(tmpcopyfileid1)['cm']\n",
    "                    model = np.load(tmpcopyfileid1)['model'][0]\n",
    "                elif os.path.isfile(tmpcopyfileid2):\n",
    "                    cm_all = np.load(tmpcopyfileid2)['cm']\n",
    "                    model = np.load(tmpcopyfileid2)['model'][0]\n",
    "                elif os.path.isfile(tmpcopyfileid3):\n",
    "                    cm_all = np.load(tmpcopyfileid3)['cm']\n",
    "                    model = np.load(tmpcopyfileid3)['model'][0]\n",
    "                elif os.path.isfile(tmpcopyfileid4):\n",
    "                    cm_all = np.load(tmpcopyfileid4)['cm']\n",
    "                    model = np.load(tmpcopyfileid4)['model'][0]\n",
    "            np.savez(fileid,cm=cm_all,model=np.array([model]))\n",
    "        else: # perform cross-check\n",
    "            tr_data = data\n",
    "            tr_labels = labels\n",
    "            ts_data = data2\n",
    "            ts_labels = labels2\n",
    "            model = []\n",
    "            for m in range(kmax):\n",
    "                tmpcopyfileid = filepath+filename5(i,j,k1,k2,k3,k4,k5,m)+'.npz'\n",
    "                if k1!=m and k2!=m and k3!=m and k4!=m and k5!=m and os.path.isfile(tmpcopyfileid):\n",
    "                    print 'Found precomputed model of '+str(k1)+str(k2)+str(k3)+str(k4)+str(k5) \\\n",
    "                                                       +', tested on '+str(m)+'. Testing on '+str(l)+'...'\n",
    "                    model = np.load(tmpcopyfileid)['model'][0]\n",
    "                    break\n",
    "            if model==[]: # model not found precomputed\n",
    "                print 'Fitting on '+str(k1)+\"-\"+str(k2)+\"-\"+str(k3)+\"-\"+str(k4)+\"-\" \\\n",
    "                                               +str(k5)+', testing on '+str(l)+'...'\n",
    "                model = pipe.fit(tr_data,tr_labels)\n",
    "            y_pred = model.predict(ts_data)\n",
    "            cm = confusion_matrix(y_pred=y_pred, y_true=ts_labels)\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "            np.savez(fileid,cm=cm,model=np.array([model]))\n",
    "\n",
    "def init_steps5(i,j,jmax,surf,surfla):\n",
    "    \"\"\"function for helping parallelization of computations per 5 surfaces\n",
    "    -> i              : prefeature id, among all computed prefeatures (0: |f|, ... see prefeatid)\n",
    "    -> j              : subfeatureset among all features (0: AFFT, 1: FREQ, 2: TIME, 3: ALL)\n",
    "    -> jmax           : number of all subfeaturesets\n",
    "    -> surf, surfla   : surface data and labels\n",
    "    \"\"\"\n",
    "    if j==jmax:\n",
    "        featsel = SelectKBest(k=1000,score_func= mutual_info_classif)\n",
    "    else:\n",
    "        featsel = SelectKBest(k='all',score_func= mutual_info_classif)\n",
    "    pipe = make_pipe_clf(scaler, featsel, decomp, classifiers[2])\n",
    "    for k1 in range(surf.shape[0]): # for every training surface1\n",
    "        for k2 in range(surf.shape[0]): # for every training surface2\n",
    "            if k2 > k1:\n",
    "                for k3 in range(surf.shape[0]):\n",
    "                    if k3 > k2:\n",
    "                        for k4 in range(surf.shape[0]):\n",
    "                            if k4 > k3:\n",
    "                                for k5 in range(surf.shape[0]):\n",
    "                                    if k5 > k4:\n",
    "                                        for l in range(surf.shape[0]): # for every testing surface\n",
    "                                            tr_surf = np.concatenate((surf[k1],surf[k2],surf[k3]),axis=0)\n",
    "                                            tr_surfla = np.concatenate((surfla[:,k1],surfla[:,k2],\n",
    "                                                                        surfla[:,k3]),axis=0)\n",
    "                                            ts_surf, ts_surfla = surf[l], surfla[:,l]\n",
    "                                            cross_fit5(i,j,k1,k2,k3,k4,k5,surf.shape[0],l,\n",
    "                                                       tr_surf,tr_surfla,ts_surf,ts_surfla,pipe)\n",
    "\n",
    "def train_5_surface(surf,surfla,n=-1):\n",
    "    \"\"\"Parallel training -on surface level- of all combinations on 5 surfaces\n",
    "    -> n              : number of cores to run in parallel, \n",
    "                        input of joblib's Parallel (n=-1 means all available cores)\n",
    "    -> surf, surfla   : surface data and labels\n",
    "    *** Cross surface validation, TRAINING with 5 surfaces each time, out of 6 surfaces in total\n",
    "    total= 4 (featuresets) * [comb(6,5)*6] (surface combinations: trained on 5, tested on 1) * 1 (prefeatureset)\n",
    "         = 4*6*6*1 = 144 different runs-files.\n",
    "    Note that comb(n,r) = n!/(r!(n-r)!)\n",
    "    \"\"\"\n",
    "    print \"-------------------------- TRAINING all combinations per 5 surfaces ----------------------------------\"        \n",
    "    for i in range(len(prefeatid)-1):\n",
    "        _ = [Parallel(n_jobs=n)([delayed(init_steps5) (i,j,surf.shape[0]-1,surf[j,:,i],surfla[:,:,i]) \n",
    "                                 for j in range(surf.shape[0])])]\n",
    "        \n",
    "def bargraph_file_gen5(maxsurf):\n",
    "    prefeats = prefeatnames[prefeatid][:-1]\n",
    "    # prefeatures, subfeatures, trained, tested, (TP,TN,FN,FP)\n",
    "    acc = np.zeros((len(prefeats),len(subfeats),maxsurf,maxsurf,maxsurf,maxsurf,maxsurf,maxsurf,4))\n",
    "    # features, subfeatures, (TP,TN,FN,FP) -> avg over all tested surfaces\n",
    "    avg = np.zeros((len(prefeats),len(subfeats),4))\n",
    "    # prefeatures, subfeatures, trained, cross_val_self_accuracy, (TP,TN,FN,FP)\n",
    "    self_acc = np.zeros((len(prefeats),len(subfeats),maxsurf,maxsurf,maxsurf,maxsurf,maxsurf,1,4))\n",
    "    # features, subfeatures, (TP,TN,FN,FP) -> avg over all self tested surfaces\n",
    "    avgs = np.zeros((len(prefeats),len(subfeats),4))\n",
    "    # features, subfeatures, trained, (tested avg, tested std), (TP,TN,FN,FP)\n",
    "    cross_acc = np.zeros((len(prefeats),len(subfeats),maxsurf,maxsurf,maxsurf,maxsurf,maxsurf,2,4))\n",
    "     # features, subfeatures, (TP,TN,FN,FP) -> avg over all cross tested surfaces\n",
    "    avgc = np.zeros((len(prefeats),len(subfeats),4))\n",
    "    initial_str = \"\"\"# clustered and stacked graph bogus data\n",
    "=stackcluster;TP;TN;FN;FP\n",
    "colors=med_blue,dark_green,yellow,red\n",
    "=nogridy\n",
    "=noupperright\n",
    "fontsz=2\n",
    "legendx=right\n",
    "legendy=center\n",
    "datascale=50\n",
    "yformat=%g%%\n",
    "xlabel=TrainedON-TestedON\n",
    "ylabel=Metrics\n",
    "=table\"\"\"\n",
    "    respath = filename5(_,_,_,_,_,_,_,_,1)\n",
    "    for i in range(len(prefeats)):\n",
    "        outname = respath+prefeats[i]\n",
    "        outfile = outname+'.perf'\n",
    "        outfile1 = outname+'_selfaccuracy.perf'\n",
    "        outfile2 = outname+'_crossaccuracy.perf'\n",
    "        out = open(outfile,'w+')\n",
    "        out.write(initial_str+\"\\n\")\n",
    "        out1 = open(outfile1,'w+')\n",
    "        out1.write(initial_str+\"\\n\")\n",
    "        out2 = open(outfile2,'w+')\n",
    "        out2.write(initial_str+\"\\n\")\n",
    "        for k1 in range(maxsurf):\n",
    "            for k2 in range(maxsurf):\n",
    "                if k2 > k1:\n",
    "                    for k3 in range(maxsurf):\n",
    "                        if k3 > k2:\n",
    "                            for k4 in range(maxsurf):\n",
    "                                if k4 > k3:\n",
    "                                    for k5 in range(maxsurf):\n",
    "                                        if k5 > k4:\n",
    "                                            for l in range(maxsurf):\n",
    "                                                out.write(\"multimulti=\"+str(k1)+str(k2)+str(k3)+str(k4)\\\n",
    "                                                          +str(k5)+\"-\"+str(l)+\"\\n\")\n",
    "                                                for j in range(len(subfeats)):\n",
    "                                                    fileid = filename5(i,j,k1,k2,k3,k4,k5,l)\n",
    "                                                    tmp = np.load(fileid)['cm']\n",
    "                                                    acc[i,j,k1,k2,k3,k4,k5,l,0] = round(tmp[1,1],2) # TP\n",
    "                                                    acc[i,j,k1,k2,k3,k4,k5,l,1] = round(tmp[0,0],2) # TN\n",
    "                                                    acc[i,j,k1,k2,k3,k4,k5,l,2] = 1-round(tmp[1,1],2) # FN\n",
    "                                                    acc[i,j,k1,k2,k3,k4,k5,l,3] = 1-round(tmp[0,0],2) # FP\n",
    "                                                    avg[i,j,:] += acc[i,j,k1,k2,k3,k4,k5,l,:]\n",
    "                                                    out.write(\"%s %.2f %.2f %.2f %.2f\\n\" \\\n",
    "                                                              % (subfeats[j],\n",
    "                                                                 acc[i,j,k1,k2,k3,k4,k5,l,0],\n",
    "                                                                 acc[i,j,k1,k2,k3,k4,k5,l,1],\n",
    "                                                                 acc[i,j,k1,k2,k3,k4,k5,l,2],\n",
    "                                                                 acc[i,j,k1,k2,k3,k4,k5,l,3]))\n",
    "                                                    # selc accuracy\n",
    "                                                    if l == k1 or l == k2 or l == k3 or l == k4 or l == k5:\n",
    "                                                        if j == 0 and l == k5:\n",
    "                                                            out1.write(\"multimulti=\"+str(k1)+str(k2)\n",
    "                                                                       +str(k3)+str(k4)+str(k5)+\"-\"+str(l)+\"\\n\")\n",
    "                                                        self_acc[i,j,k1,k2,k3,k4,k5,0,:] = \\\n",
    "                                                            acc[i,j,k1,k2,k3,k4,k5,l]\n",
    "                                                        avgs[i,j,:] += self_acc[i,j,k1,k2,k3,k4,k5,0,:]\n",
    "                                                        if l == k5:\n",
    "                                                            out1.write(\"%s %.2f %.2f %.2f %.2f\\n\" \\\n",
    "                                                                       % (subfeats[j],\n",
    "                                                                          self_acc[i,j,k1,k2,k3,k4,k5,0,0],\n",
    "                                                                          self_acc[i,j,k1,k2,k3,k4,k5,0,1],\n",
    "                                                                          self_acc[i,j,k1,k2,k3,k4,k5,0,2],\n",
    "                                                                          self_acc[i,j,k1,k2,k3,k4,k5,0,3]))\n",
    "                                                    if l != k1 and l != k2 and l != k3 and l!= k4 and l!= k5:\n",
    "                                                        t = range(maxsurf)\n",
    "                                                        t.remove(k1)\n",
    "                                                        t.remove(k2)\n",
    "                                                        t.remove(k3)\n",
    "                                                        t.remove(k4)\n",
    "                                                        t.remove(k5)\n",
    "                                                        if (l == t[-1]):\n",
    "                                                            if j == 0:\n",
    "                                                                out2.write(\"multimulti=\"+str(k1)+str(k2)\\\n",
    "                                                                           +str(k3)+str(k4)+str(k5)+\"\\n\")\n",
    "                                                            cross_acc[i,j,k1,k2,k3,k4,k5,0,:] = \\\n",
    "                                                                np.mean(acc[i,j,k1,k2,k3,k4,k5,t,:], axis=0)\n",
    "                                                            avgc[i,j,:] += cross_acc[i,j,k1,k2,k3,k4,k5,0,:]\n",
    "                                                            out2.write(\"%s %.2f %.2f %.2f %.2f\\n\" \\\n",
    "                                                                       % (subfeats[j],\n",
    "                                                                          cross_acc[i,j,k1,k2,k3,k4,k5,0,0],\n",
    "                                                                          cross_acc[i,j,k1,k2,k3,k4,k5,0,1],\n",
    "                                                                          cross_acc[i,j,k1,k2,k3,k4,k5,0,2],\n",
    "                                                                          cross_acc[i,j,k1,k2,k3,k4,k5,0,3]))\n",
    "        out.write(\"multimulti=AVG\\n\")\n",
    "        out1.write(\"multimulti=AVG\\n\")\n",
    "        out2.write(\"multimulti=AVG\\n\")\n",
    "        avg /= comb(maxsurf,5)*maxsurf*1.\n",
    "        avgs /= comb(maxsurf,5)*5.\n",
    "        avgc /= comb(maxsurf,5)*1.\n",
    "        for j in range(len(subfeats)):\n",
    "            out.write(\"%s %.2f %.2f %.2f %.2f\\n\" % (subfeats[j],avg[i,j,0],avg[i,j,1],avg[i,j,2],avg[i,j,3]))\n",
    "            out1.write(\"%s %.2f %.2f %.2f %.2f\\n\" % (subfeats[j],avgs[i,j,0],avgs[i,j,1],avgs[i,j,2],avgs[i,j,3]))\n",
    "            out2.write(\"%s %.2f %.2f %.2f %.2f\\n\" % (subfeats[j],avgc[i,j,0],avgc[i,j,1],avgc[i,j,2],avgc[i,j,3]))\n",
    "        out.close()\n",
    "        out1.close()\n",
    "        out2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------- LOADING DATA and COMPUTING NECESSARY STRUCTS ----------------------------\n",
      "1 -> f1: (36,) (36,) (36, 4)\n",
      "2 -> f2: (36,) (36,) (36, 4)\n",
      "3 -> f: (72,) (72,) (72, 4)\n",
      "4 -> m1,m2: 36 36 1.0 1.0\n",
      "5 -> f=f+l: (72,) : [(345002, 4), (105001, 4), (210001, 4), (225002, 4), (130001, 4), (65001, 4), (195001, 4), (65001, 4), (130001, 4), (195001, 4), (65001, 4), (130001, 4), (225002, 4), (65001, 4), (130001, 4), (195001, 4), (65001, 4), (130001, 4), (75001, 4), (130001, 4), (195001, 4), (195001, 4), (130001, 4), (65001, 4), (65001, 4), (130001, 4), (195001, 4), (195001, 4), (130001, 4), (65001, 4), (65001, 4), (130001, 4), (195001, 4), (130001, 4), (195001, 4), (65001, 4), (345002, 4), (105001, 4), (210001, 4), (225002, 4), (130001, 4), (65001, 4), (195001, 4), (65001, 4), (130001, 4), (195001, 4), (65001, 4), (130001, 4), (225002, 4), (65001, 4), (130001, 4), (195001, 4), (65001, 4), (130001, 4), (75001, 4), (130001, 4), (195001, 4), (195001, 4), (130001, 4), (65001, 4), (65001, 4), (130001, 4), (195001, 4), (195001, 4), (130001, 4), (65001, 4), (65001, 4), (130001, 4), (195001, 4), (130001, 4), (195001, 4), (65001, 4)]\n",
      "--------------------------------------- COMPUTING PREFEATURES ----------------------------------------\n",
      "(72,) : [(345002, 2), (105001, 2), (210001, 2), (225002, 2), (130001, 2), (65001, 2), (195001, 2), (65001, 2), (130001, 2), (195001, 2), (65001, 2), (130001, 2), (225002, 2), (65001, 2), (130001, 2), (195001, 2), (65001, 2), (130001, 2), (75001, 2), (130001, 2), (195001, 2), (195001, 2), (130001, 2), (65001, 2), (65001, 2), (130001, 2), (195001, 2), (195001, 2), (130001, 2), (65001, 2), (65001, 2), (130001, 2), (195001, 2), (130001, 2), (195001, 2), (65001, 2), (345002, 2), (105001, 2), (210001, 2), (225002, 2), (130001, 2), (65001, 2), (195001, 2), (65001, 2), (130001, 2), (195001, 2), (65001, 2), (130001, 2), (225002, 2), (65001, 2), (130001, 2), (195001, 2), (65001, 2), (130001, 2), (75001, 2), (130001, 2), (195001, 2), (195001, 2), (130001, 2), (65001, 2), (65001, 2), (130001, 2), (195001, 2), (195001, 2), (130001, 2), (65001, 2), (65001, 2), (130001, 2), (195001, 2), (130001, 2), (195001, 2), (65001, 2)]\n",
      "---------------------------------------- FEATURE EXTRACTION ------------------------------------------\n",
      "Features FOUND PRECOMPUTED! Feature Loading DONE in: 0.577996015549 seconds \n",
      "features:  (72,) , labels:  (72,)\n",
      "------------------------------------ AVG FEATURE COMPUTATION TIME ------------------------------------\n",
      "Avg feature computation time (millisec):  2.27145147324\n",
      "----------- KEEPING LABEL's PURE (STABLE, SLIP) PHASE PARTS (TRIMMING AROUND CHANGE POINTS)-----------\n",
      "new_labels:  (72,)\n",
      "----------------------------- COMPUTING X,Y for CLASSIFIERS' INPUT -----------------------------------\n",
      "XY files FOUND PRECOMPUTED!\n",
      "X,Y [0,1,2]:  (9935, 3107) (9935,) (9935, 3107) (9935,) (9935, 3107) (9935,)\n",
      "Xsp,Ysp [0,1,2]:  (8826, 3107) (8826,) (8826, 3107) (8826,) (8826, 3107) (8826,)\n",
      "------------------------ COMPUTING X,Y per surface CLASSIFIERS' INPUT --------------------------------\n",
      "(4, 6, 1) (1470, 6, 1)\n",
      "-------------------------- TRAINING all combinations per 1 surface -----------------------------------\n",
      "-------------------------- TRAINING all combinations per 2 surfaces ----------------------------------\n",
      "-------------------------- TRAINING all combinations per 3 surfaces ----------------------------------\n",
      "-------------------------- TRAINING all combinations per 4 surfaces ----------------------------------\n",
      "-------------------------- TRAINING all combinations per 5 surfaces ----------------------------------\n"
     ]
    }
   ],
   "source": [
    "############ TRAINING PROCEDURE ##############\n",
    "# necessary steps before training\n",
    "f,l,fd,member,m1,m2 = data_prep(datafile)                      # read input force and labels\n",
    "prefeat = compute_prefeat(f)                                   # compute corresponding prefeatures\n",
    "features, labels = feature_extraction(prefeat, member)         # feature extraction from prefeatures\n",
    "avg_feat_comp_time(prefeat)                                    # average feature extraction time\n",
    "new_labels = label_cleaning(prefeat,labels,member)             # trim labels, around change points\n",
    "X,Y,Yn,Xsp,Ysp = computeXY(features,labels,new_labels,m1,m2)   # compute data and labels, trimmed and untrimmed\n",
    "surf, surfla = computeXY_persurf(Xsp,Ysp)                      # compute per surface data and labels\n",
    "# training\n",
    "train_1_surface(surf,surfla)                                   # training of all combinations per 1 surface\n",
    "train_2_surface(surf,surfla)                                   # training of all combinations per 2 surfaces\n",
    "train_3_surface(surf,surfla)                                   # training of all combinations per 3 surfaces\n",
    "train_4_surface(surf,surfla)                                   # training of all combinations per 4 surfaces\n",
    "train_5_surface(surf,surfla)                                   # training of all combinations per 5 surfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ OFFLINE TESTING PROCEDURE ##############\n",
    "# generate files with stats\n",
    "bargraph_file_gen1(6)\n",
    "bargraph_file_gen2(6)\n",
    "bargraph_file_gen3(6)\n",
    "bargraph_file_gen4(6)\n",
    "bargraph_file_gen5(6)\n",
    "# use the bargraph tool to plot graphs from generated files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------- LOADING DATA and COMPUTING NECESSARY STRUCTS ----------------------------\n",
      "1 -> f1: (1, 1) (1, 1) (1, 1)\n",
      "2 -> f2: (1, 1) (1, 1) (1, 1)\n",
      "3 -> f: (2, 1) (2, 1) (2, 1)\n",
      "4 -> m1,m2: 1 1 1.0 1.0\n",
      "5 -> f=f+l: (2, 65000, 4) : [(65000, 4), (65000, 4)]\n",
      "--------------------------------------- COMPUTING PREFEATURES ----------------------------------------\n",
      "(2,) : [(65000, 2), (65000, 2)]\n",
      "---------------------------------------- FEATURE EXTRACTION ------------------------------------------\n",
      "Features FOUND PRECOMPUTED! Feature Loading DONE in: 0.616533041 seconds \n",
      "features:  (2,) , labels:  (2,)\n",
      "----------- KEEPING LABEL's PURE (STABLE, SLIP) PHASE PARTS (TRIMMING AROUND CHANGE POINTS)-----------\n",
      "new_labels:  (2,)\n",
      "----------------------------- COMPUTING X,Y for CLASSIFIERS' INPUT -----------------------------------\n",
      "XY files FOUND PRECOMPUTED!\n",
      "X,Y [0,1,2]:  (3199, 3107) (3199,) (3199, 3107) (3199,) (6398, 3107) (6398,)\n",
      "Xsp,Ysp [0,1,2]:  (2769, 3107) (2769,) (2769, 3107) (2769,) (5538, 3107) (5538,)\n",
      "------------------------ COMPUTING X,Y per surface CLASSIFIERS' INPUT --------------------------------\n",
      "(4, 6, 1) (922, 6, 1)\n"
     ]
    }
   ],
   "source": [
    "############ ONLINE TESTING PROCEDURE ##############\n",
    "# same necessary steps as in training\n",
    "f,l,fd,member,m1,m2 = data_prep(validfile)\n",
    "prefeat = compute_prefeat(f)\n",
    "features, labels = feature_extraction(prefeat, member, validfeatfile, 'validfeat_')\n",
    "new_labels = label_cleaning(prefeat,labels,member)\n",
    "X,Y,Yn,Xsp,Ysp = computeXY(features,labels,new_labels,m1,m2,validXYfile,validXYsplitfile)\n",
    "surf, surfla = computeXY_persurf(Xsp,Ysp,validsurffile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'data/results1/data/results1/fs_0_subfs_3_tr_0_ts_5.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-2d33ab5591c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mfileid5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatapath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'results5/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfilename5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mfileid5b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatapath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'results5/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfilename5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mmodelb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileidb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmodel5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileid5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jagrio/.local/lib/python2.7/site-packages/numpy/lib/npyio.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_pathlib_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'data/results1/data/results1/fs_0_subfs_3_tr_0_ts_5.npz'"
     ]
    }
   ],
   "source": [
    "############ VISUALIZING ONLINE TESTING PROCEDURE ##############\n",
    "\n",
    "subfeats = ['AFFT','FREQ','TIME','BOTH']\n",
    "feats = ['fnorm','ftfn','fnormftfn']\n",
    "matplotlib.rcParams['text.usetex'] = True\n",
    "\n",
    "fileid = datapath+'results1/'+filename1(0,3,0,5)\n",
    "fileidb = datapath+'results1/'+filename1(0,0,0,5)\n",
    "fileid5 = datapath+'results5/'+filename5(0,3,0,1,2,3,4,5)\n",
    "fileid5b = datapath+'results5/'+filename5(0,0,0,1,2,3,4,5)\n",
    "model = np.load(fileid)['model'][0]\n",
    "modelb = np.load(fileidb)['model'][0]\n",
    "model5 = np.load(fileid5)['model'][0]\n",
    "model5b = np.load(fileid5b)['model'][0]\n",
    "Yout = model.predict(X[0])\n",
    "Youtb = modelb.predict(Xsp[0][:,-window-2:-window/2-1])\n",
    "Yout5 = model5.predict(Xsp[0])\n",
    "Yout5b = model5b.predict(Xsp[0][:,-window-2:-window/2-1])\n",
    "print Yout.shape, Yout5.shape, Yout5b.shape\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('axes', linewidth=2)\n",
    "plt.rc('font', weight='bold')\n",
    "plt.rcParams['text.latex.preamble'] = [r'\\usepackage{sfmath} \\boldmath']\n",
    "offset = 2000-window\n",
    "endset = 2650\n",
    "skipf = 20\n",
    "skipy = 15\n",
    "ax = plt.figure(figsize=(20,10))\n",
    "tf = np.linalg.norm(f[0][offset+window::skipf,:3][:endset],axis=1)\n",
    "p1, = plt.plot(tf/max(tf),linewidth=5)\n",
    "ty = Yout[offset/skipf:][:endset]+0.02\n",
    "print tf.shape, ty.shape\n",
    "p = plt.scatter(range(len(tf))[::skipy],ty[::skipy],color='red',s=30)\n",
    "plt.hold\n",
    "plt.text(100, 0.15, r'\\textbf{Stable}', ha=\"center\", va=\"center\", rotation=0,\n",
    "            size=25)\n",
    "plt.text(1000, 0.85, r'\\textbf{Slip}', ha=\"center\", va=\"center\", rotation=0,\n",
    "            size=25)\n",
    "plt.annotate('', fontsize=10, xy=(100, 0.05), xytext=(100, 0.12),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "plt.annotate('', xy=(1000, 0.98), xytext=(1000, 0.9),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "plt.text(400, 0.55, r'\\textbf{P1}', ha=\"center\", va=\"center\", rotation=0,\n",
    "            size=25)\n",
    "plt.axvline(x=810,linestyle='dashed',color='black',linewidth=5)\n",
    "plt.text(1000, 0.55, r'\\textbf{P2}', ha=\"center\", va=\"center\", rotation=0,\n",
    "            size=25)\n",
    "plt.axvline(x=1200,linestyle='dashed',color='black',linewidth=5)\n",
    "plt.text(1250, 0.55, r'\\textbf{P3}', ha=\"center\", va=\"center\", rotation=0,\n",
    "            size=25)\n",
    "plt.axvline(x=1335,linestyle='dashed',color='black',linewidth=5)\n",
    "plt.text(1385, 0.25, r'\\textbf{P4}', ha=\"center\", va=\"center\", rotation=0,\n",
    "            size=25)\n",
    "plt.axvline(x=1445,linestyle='dashed',color='black',linewidth=5)\n",
    "plt.text(1650, 0.55, r'\\textbf{P1}', ha=\"center\", va=\"center\", rotation=0,\n",
    "            size=25)\n",
    "plt.axvline(x=1830,linestyle='dashed',color='black',linewidth=5)\n",
    "plt.text(2000, 0.55, r'\\textbf{P2}', ha=\"center\", va=\"center\", rotation=0,\n",
    "            size=25)\n",
    "plt.axvline(x=2200,linestyle='dashed',color='black',linewidth=5)\n",
    "plt.text(2250, 0.55, r'\\textbf{P3}', ha=\"center\", va=\"center\", rotation=0,\n",
    "            size=25)\n",
    "plt.axvline(x=2330,linestyle='dashed',color='black',linewidth=5)\n",
    "plt.text(2385, 0.25, r'\\textbf{P4}', ha=\"center\", va=\"center\", rotation=0,\n",
    "            size=25)\n",
    "plt.axvline(x=2440,linestyle='dashed',color='black',linewidth=5)\n",
    "plt.text(2540, 0.55, r'\\textbf{P1}', ha=\"center\", va=\"center\", rotation=0,\n",
    "            size=25)\n",
    "plt.xlabel(r't ($1e^{-2} sec$)',fontsize=35)\n",
    "plt.yticks([])\n",
    "plt.legend([p1,p],[r'$|\\textbf{f}|$',r'\\textbf{out1}'],loc=2, prop={'size': 35})\n",
    "plt.tick_params(labelsize=20)\n",
    "plt.tight_layout()\n",
    "savefig('validation.pdf', bbox_inches='tight')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
